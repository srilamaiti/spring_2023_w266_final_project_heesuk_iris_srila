{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilamaiti/spring_2023_w266_final_project_heesuk_iris_srila/blob/main/%20iris/IL_data_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ9w2FlGlNDV"
      },
      "source": [
        "https://www.kaggle.com/competitions/feedback-prize-english-language-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oUehh9Tfk8rA"
      },
      "outputs": [],
      "source": [
        "# # install\n",
        "# !pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNmUd_vinst-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import copy\n",
        "import tensorflow as tf\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXFJZt1KlRIJ"
      },
      "outputs": [],
      "source": [
        "# data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "%cd \"gdrive/MyDrive/Colab Notebooks/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdaPcHmjmlFS"
      },
      "outputs": [],
      "source": [
        "## only needed to run it once to download\n",
        "\n",
        "# os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/Colab Notebooks/\"\n",
        "\n",
        "# !kaggle competitions download -c feedback-prize-english-language-learning \n",
        "# !unzip -q feedback-prize-english-language-learning.zip -d ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3sZG8BKnUTx"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(\"train.csv\")\n",
        "print(f\"train shape: {train.shape}\")\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ncsa5RK4n2EC"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv(\"test.csv\")\n",
        "print(f\"test shape: {test.shape}\")\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYT9I1iToJpr"
      },
      "outputs": [],
      "source": [
        "train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgG43xnX8wkn"
      },
      "outputs": [],
      "source": [
        "# length of full_text\n",
        "pprint.pprint(train[\"full_text\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mm-WKueoUYxB"
      },
      "outputs": [],
      "source": [
        "len(train[\"full_text\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX_R3R7U8xQ4"
      },
      "outputs": [],
      "source": [
        "# histogram with cohesion, syntax, vocabulary, phraseology, grammar, conventions \n",
        "for y in [\"cohesion\",\"syntax\",\"vocabulary\", \"phraseology\",\"grammar\", \"conventions\"]:\n",
        "  print(train[y].value_counts().sort_index())\n",
        "  print()\n",
        "  train[y].hist()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYjjR6n086-c"
      },
      "outputs": [],
      "source": [
        "# any collinearity?\n",
        "cm = train.corr()\n",
        "print(cm)\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "sns.heatmap(cm, cmap=cmap)\n",
        "plt.show()\n",
        "# high correlation for all of them, especially phraseology+vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rhf00V0E9AV8"
      },
      "outputs": [],
      "source": [
        "# Any patterns of text? Head & tail has same topics? # range of topics with multiple paragraphs. pprint?\n",
        "[print(train[\"full_text\"][i]) for i in range(5)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_OBhpdB9D4L"
      },
      "outputs": [],
      "source": [
        "# [print(train[\"full_text\"][i]) for i in range(-5,-1,-1)]\n",
        "train[\"full_text\"][train.shape[1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfitgYXHib4D"
      },
      "outputs": [],
      "source": [
        "# any pattern for those who are high scorers in all of them?\n",
        "# [\"cohesion\",\"syntax\",\"vocabulary\", \"phraseology\",\"grammar\", \"conventions\"]\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "# high scorer for one is high scorer for the rest.\n",
        "train[(train[\"cohesion\"]==5) & (train[\"syntax\"]==5) & (train[\"vocabulary\"]==5) & (train[\"phraseology\"]==5) & (train[\"grammar\"]==5) & (train[\"conventions\"]==5)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEfXT6NfkUSH"
      },
      "outputs": [],
      "source": [
        "# any pattern for those who are low scorers in all of them?\n",
        "# [\"cohesion\",\"syntax\",\"vocabulary\", \"phraseology\",\"grammar\", \"conventions\"]\n",
        "\n",
        "# low scorer for one is low scorer for the rest.\n",
        "train[(train[\"cohesion\"]==1) & (train[\"syntax\"]==1) & (train[\"vocabulary\"]==1) & (train[\"phraseology\"]==1) & (train[\"grammar\"]==1) & (train[\"conventions\"]==1)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxgzZ5_yorIg"
      },
      "source": [
        "Notes:\n",
        "\n",
        "* test.csv only had 3 items. just discard and split by train.csv\n",
        "* high correlation, so high scorer in one is high scorer in rest and vice versa\n",
        "* hard to be any extreme\n",
        "* high scorers also tend to write more\n",
        "* low scorers have a lot of misspellings, suggesting a model with more casual language might be more appropriate\n",
        "* there are paragraphs so we need to take note of line separations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzB82PhPxEqX"
      },
      "source": [
        "# RetriBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLBV7TnUxCwv"
      },
      "outputs": [],
      "source": [
        "# RetriBERT tokenizer\n",
        "# class transformers.RetriBertTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hr_JUf8J7jyj"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install ktrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LC-BH85qy3lg"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Optional\n",
        "import transformers\n",
        "from ktrain import text\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from torch import nn\n",
        "# from transformers import TFAutoModel, AutoTokenizer\n",
        "\n",
        "# from ...modeling_utils import PreTrainedModel\n",
        "# from ...utils import add_start_docstrings, logging\n",
        "# from ..bert.modeling_bert import BertModel\n",
        "# from .configuration_retribert import RetriBertConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUeVSs2z7Fhp"
      },
      "outputs": [],
      "source": [
        "# model_checkpoint = 'yjernite/retribert-base-uncased'\n",
        "# model = TFAutoModel.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiF9WJjLxtfa"
      },
      "outputs": [],
      "source": [
        "# # RetriBERT model\n",
        "# # class transformers.RetriBertModel\n",
        "# # coding=utf-8\n",
        "# # Copyright 2019-present, the HuggingFace Inc. team, The Google AI Language Team and Facebook, Inc.\n",
        "# #\n",
        "# # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# # you may not use this file except in compliance with the License.\n",
        "# # You may obtain a copy of the License at\n",
        "# #\n",
        "# #     http://www.apache.org/licenses/LICENSE-2.0\n",
        "# #\n",
        "# # Unless required by applicable law or agreed to in writing, software\n",
        "# # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# # See the License for the specific language governing permissions and\n",
        "# # limitations under the License.\n",
        "# \"\"\"\n",
        "# RetriBERT model\n",
        "# \"\"\"\n",
        "\n",
        "# logger = transformers.logging.get_logger(__name__)\n",
        "\n",
        "# RETRIBERT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "#     \"yjernite/retribert-base-uncased\",\n",
        "#     # See all RetriBert models at https://huggingface.co/models?filter=retribert\n",
        "# ]\n",
        "\n",
        "\n",
        "# # INTERFACE FOR ENCODER AND TASK SPECIFIC MODEL #\n",
        "# class RetriBertPreTrainedModel(PreTrainedModel):\n",
        "#     \"\"\"\n",
        "#     An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
        "#     models.\n",
        "#     \"\"\"\n",
        "\n",
        "#     config_class = RetriBertConfig\n",
        "#     load_tf_weights = None\n",
        "#     base_model_prefix = \"retribert\"\n",
        "\n",
        "#     def _init_weights(self, module):\n",
        "#         \"\"\"Initialize the weights\"\"\"\n",
        "#         if isinstance(module, nn.Linear):\n",
        "#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "#             if module.bias is not None:\n",
        "#                 module.bias.data.zero_()\n",
        "#         elif isinstance(module, nn.Embedding):\n",
        "#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "#             if module.padding_idx is not None:\n",
        "#                 module.weight.data[module.padding_idx].zero_()\n",
        "#         elif isinstance(module, nn.LayerNorm):\n",
        "#             module.bias.data.zero_()\n",
        "#             module.weight.data.fill_(1.0)\n",
        "\n",
        "\n",
        "# RETRIBERT_START_DOCSTRING = r\"\"\"\n",
        "#     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
        "#     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
        "#     etc.)\n",
        "#     This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
        "#     Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
        "#     and behavior.\n",
        "#     Parameters:\n",
        "#         config ([`RetriBertConfig`]): Model configuration class with all the parameters of the model.\n",
        "#             Initializing with a config file does not load the weights associated with the model, only the\n",
        "#             configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "# @add_start_docstrings(\n",
        "#     \"\"\"Bert Based model to embed queries or document for document retrieval.\"\"\",\n",
        "#     RETRIBERT_START_DOCSTRING,\n",
        "# )\n",
        "# class RetriBertModel(RetriBertPreTrainedModel):\n",
        "#     def __init__(self, config: RetriBertConfig) -> None:\n",
        "#         super().__init__(config)\n",
        "#         self.projection_dim = config.projection_dim\n",
        "\n",
        "#         self.bert_query = BertModel(config)\n",
        "#         self.bert_doc = None if config.share_encoders else BertModel(config)\n",
        "#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "#         self.project_query = nn.Linear(config.hidden_size, config.projection_dim, bias=False)\n",
        "#         self.project_doc = nn.Linear(config.hidden_size, config.projection_dim, bias=False)\n",
        "\n",
        "#         self.ce_loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "#         # Initialize weights and apply final processing\n",
        "#         self.post_init()\n",
        "\n",
        "#     def embed_sentences_checkpointed(\n",
        "#         self,\n",
        "#         input_ids,\n",
        "#         attention_mask,\n",
        "#         sent_encoder,\n",
        "#         checkpoint_batch_size=-1,\n",
        "#     ):\n",
        "#         # reproduces BERT forward pass with checkpointing\n",
        "#         if checkpoint_batch_size < 0 or input_ids.shape[0] < checkpoint_batch_size:\n",
        "#             return sent_encoder(input_ids, attention_mask=attention_mask)[1]\n",
        "#         else:\n",
        "#             # prepare implicit variables\n",
        "#             device = input_ids.device\n",
        "#             input_shape = input_ids.size()\n",
        "#             token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "#             head_mask = [None] * sent_encoder.config.num_hidden_layers\n",
        "#             extended_attention_mask: torch.Tensor = sent_encoder.get_extended_attention_mask(\n",
        "#                 attention_mask, input_shape\n",
        "#             )\n",
        "\n",
        "#             # define function for checkpointing\n",
        "#             def partial_encode(*inputs):\n",
        "#                 encoder_outputs = sent_encoder.encoder(\n",
        "#                     inputs[0],\n",
        "#                     attention_mask=inputs[1],\n",
        "#                     head_mask=head_mask,\n",
        "#                 )\n",
        "#                 sequence_output = encoder_outputs[0]\n",
        "#                 pooled_output = sent_encoder.pooler(sequence_output)\n",
        "#                 return pooled_output\n",
        "\n",
        "#             # run embedding layer on everything at once\n",
        "#             embedding_output = sent_encoder.embeddings(\n",
        "#                 input_ids=input_ids, position_ids=None, token_type_ids=token_type_ids, inputs_embeds=None\n",
        "#             )\n",
        "#             # run encoding and pooling on one mini-batch at a time\n",
        "#             pooled_output_list = []\n",
        "#             for b in range(math.ceil(input_ids.shape[0] / checkpoint_batch_size)):\n",
        "#                 b_embedding_output = embedding_output[b * checkpoint_batch_size : (b + 1) * checkpoint_batch_size]\n",
        "#                 b_attention_mask = extended_attention_mask[b * checkpoint_batch_size : (b + 1) * checkpoint_batch_size]\n",
        "#                 pooled_output = checkpoint.checkpoint(partial_encode, b_embedding_output, b_attention_mask)\n",
        "#                 pooled_output_list.append(pooled_output)\n",
        "#             return torch.cat(pooled_output_list, dim=0)\n",
        "\n",
        "#     def embed_questions(\n",
        "#         self,\n",
        "#         input_ids,\n",
        "#         attention_mask=None,\n",
        "#         checkpoint_batch_size=-1,\n",
        "#     ):\n",
        "#         q_reps = self.embed_sentences_checkpointed(\n",
        "#             input_ids,\n",
        "#             attention_mask,\n",
        "#             self.bert_query,\n",
        "#             checkpoint_batch_size,\n",
        "#         )\n",
        "#         return self.project_query(q_reps)\n",
        "\n",
        "#     def embed_answers(\n",
        "#         self,\n",
        "#         input_ids,\n",
        "#         attention_mask=None,\n",
        "#         checkpoint_batch_size=-1,\n",
        "#     ):\n",
        "#         a_reps = self.embed_sentences_checkpointed(\n",
        "#             input_ids,\n",
        "#             attention_mask,\n",
        "#             self.bert_query if self.bert_doc is None else self.bert_doc,\n",
        "#             checkpoint_batch_size,\n",
        "#         )\n",
        "#         return self.project_doc(a_reps)\n",
        "\n",
        "#     def forward(\n",
        "#         self,\n",
        "#         input_ids_query: torch.LongTensor,\n",
        "#         attention_mask_query: Optional[torch.FloatTensor],\n",
        "#         input_ids_doc: torch.LongTensor,\n",
        "#         attention_mask_doc: Optional[torch.FloatTensor],\n",
        "#         checkpoint_batch_size: int = -1,\n",
        "#     ) -> torch.FloatTensor:\n",
        "#         r\"\"\"\n",
        "#         Args:\n",
        "#             input_ids_query (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "#                 Indices of input sequence tokens in the vocabulary for the queries in a batch.\n",
        "#                 Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "#                 [`PreTrainedTokenizer.__call__`] for details.\n",
        "#                 [What are input IDs?](../glossary#input-ids)\n",
        "#             attention_mask_query (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "#                 Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "#                 - 1 for tokens that are **not masked**,\n",
        "#                 - 0 for tokens that are **masked**.\n",
        "#                 [What are attention masks?](../glossary#attention-mask)\n",
        "#             input_ids_doc (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "#                 Indices of input sequence tokens in the vocabulary for the documents in a batch.\n",
        "#             attention_mask_doc (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "#                 Mask to avoid performing attention on documents padding token indices.\n",
        "#             checkpoint_batch_size (`int`, *optional*, defaults to `-1`):\n",
        "#                 If greater than 0, uses gradient checkpointing to only compute sequence representation on\n",
        "#                 `checkpoint_batch_size` examples at a time on the GPU. All query representations are still compared to\n",
        "#                 all document representations in the batch.\n",
        "#         Return:\n",
        "#             `torch.FloatTensor``: The bidirectional cross-entropy loss obtained while trying to match each query to its\n",
        "#             corresponding document and each document to its corresponding query in the batch\n",
        "#         \"\"\"\n",
        "#         device = input_ids_query.device\n",
        "#         q_reps = self.embed_questions(input_ids_query, attention_mask_query, checkpoint_batch_size)\n",
        "#         a_reps = self.embed_answers(input_ids_doc, attention_mask_doc, checkpoint_batch_size)\n",
        "#         compare_scores = torch.mm(q_reps, a_reps.t())\n",
        "#         loss_qa = self.ce_loss(compare_scores, torch.arange(compare_scores.shape[1]).to(device))\n",
        "#         loss_aq = self.ce_loss(compare_scores.t(), torch.arange(compare_scores.shape[0]).to(device))\n",
        "#         loss = (loss_qa + loss_aq) / 2\n",
        "#         return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saSRFRT_Vvqy"
      },
      "outputs": [],
      "source": [
        "# retribert_model = transformers.TFAutoModel.from_pretrained(\"yjernite/retribert-base-uncased\")\n",
        "# text.Transformer(\"yjernite/retribert-base-uncased/\")\n",
        "# transformers.AutoConfig.from_pretrained(\"yjernite/retribert-base-uncased/\")\n",
        "\n",
        "# retriconfig = transformers.RetriBertConfig(vocab_size=30522, \n",
        "#                              hidden_size=768, \n",
        "#                              num_hidden_layers=8, \n",
        "#                              num_attention_heads=12, \n",
        "#                              intermediate_size=3072, \n",
        "#                              hidden_act='gelu', \n",
        "#                              hidden_dropout_prob=0.1, \n",
        "#                              attention_probs_dropout_prob=0.1, \n",
        "#                              max_position_embeddings=512, \n",
        "#                              type_vocab_size=2, \n",
        "#                              initializer_range=0.02, \n",
        "#                              layer_norm_eps=1e-12, \n",
        "#                              share_encoders=True, \n",
        "#                              projection_dim=128, \n",
        "#                              pad_token_id=0)\n",
        "# # retribert_model = transformers.TFAutoModel.from_pretrained(retriconfig)\n",
        "# transformers.AutoConfig.from_pretrained(retriconfig) # tried using the config function\n",
        "\n",
        "retribert_model_checkpoints = \"yjernite/retribert-base-uncased\"\n",
        "retribert_model = transformers.BertModel.from_pretrained(retribert_model_checkpoints)\n",
        "# retribert_model\n",
        "\n",
        "# Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n",
        "for param in retribert_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "retribert_tokenizer = transformers.BertTokenizer.from_pretrained(retribert_model_checkpoints)\n",
        "# retribert_tokenizer = transformers.AutoTokenizer.from_pretrained(retribert_model_checkpoints, use_fast=False, normalization = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osrK4zpiryVZ"
      },
      "outputs": [],
      "source": [
        "retri_cuda = retribert_model.cuda()\n",
        "retri_cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG6hNNysrmyU"
      },
      "outputs": [],
      "source": [
        "input_train_df = train\n",
        "input_test_df = test\n",
        "label_cols = input_train_df.columns[2:]\n",
        "orig_train_df = copy.deepcopy(input_train_df)\n",
        "orig_train_df.head()\n",
        "     \n",
        "shuffle = np.random.permutation(np.arange(orig_train_df.shape[0]))\n",
        "orig_train_df = orig_train_df.iloc[shuffle]\n",
        "split=(0.8,0.1,0.1)\n",
        "splits = np.multiply(len(orig_train_df), split).astype(int)\n",
        "df_train, df_val, df_test = np.split(orig_train_df, [splits[0], splits[0] + splits[1]])\n",
        "\n",
        "X_train, X_val, X_test = df_train['full_text'], df_val['full_text'], df_test['full_text']\n",
        "y_train, y_val, y_test = np.array(df_train[label_cols]), np.array(df_val[label_cols]), np.array(df_test[label_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j20twVVktlZY"
      },
      "outputs": [],
      "source": [
        "def set_config_param(seed = 99):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    tf.keras.backend.clear_session()\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/Kaggle\"\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    \n",
        "    \n",
        "# set_config_param(20230214)\n",
        "set_config_param()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6dKVou5uMo5"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 512\n",
        "epochs = 5\n",
        "batch_size = 8\n",
        "dropout = .1\n",
        "learning_rate = .00005\n",
        "number_of_hidden_layer = 1\n",
        "hidden_layer_node_count = 256\n",
        "trainable_flag = False\n",
        "retrain_layer_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLosOxRss8B2"
      },
      "outputs": [],
      "source": [
        "def MCRMSE(y_true, y_pred):\n",
        "    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n",
        "    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=-1, keepdims=True)\n",
        "\n",
        "def plot_loss_accuracy(history, col_list):\n",
        "    fig, ax = plt.subplots(2, 6, figsize=(16, 6), sharex='col', sharey='row')\n",
        "    fig.tight_layout(pad=5.0)\n",
        "    for idx, col in enumerate(col_list):\n",
        "\n",
        "        ax[0, idx].plot(history[col + '_loss'], lw=2, color='darkgoldenrod')\n",
        "        ax[0, idx].plot(history['val_' + col + '_loss'], lw=2, color='indianred')\n",
        "        #ax[0, idx].legend(loc='center left')\n",
        "        ax[0, idx].legend(['Train', 'Validation'], fontsize=5)\n",
        "        ax[0, idx].set_xlabel('Epochs', size=10)\n",
        "        ax[0, idx].set_title('Loss: ' + col)\n",
        "\n",
        "        ax[1, idx].plot(history[col + '_accuracy'], lw=2, color='darkgoldenrod')\n",
        "        ax[1, idx].plot(history['val_' + col + '_accuracy'], lw=2, color='indianred')\n",
        "        #ax[0, idx].legend(loc='center left')\n",
        "        ax[1, idx].legend(['Train', 'Validation'], fontsize=5)\n",
        "        ax[1, idx].set_xlabel('Epochs', size=10)\n",
        "        ax[1, idx].set_title('Accuracy: ' + col)\n",
        "\n",
        "def encode_text(text, tokenizer):\n",
        "    \n",
        "    encoded = tokenizer.batch_encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors=\"tf\",\n",
        "    )\n",
        "\n",
        "    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_masks\": attention_masks\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKTyGEb7wRwo"
      },
      "outputs": [],
      "source": [
        "train_encodings = encode_text(df_train['full_text'].tolist(), retribert_tokenizer)\n",
        "val_encodings = encode_text(df_val['full_text'].tolist(), retribert_tokenizer)\n",
        "test_encodings = encode_text(df_test['full_text'].tolist(), retribert_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwu_HuGqzvCG"
      },
      "outputs": [],
      "source": [
        "# double check encodings and length=512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Am8IcOsF0Y4s"
      },
      "outputs": [],
      "source": [
        "#### from BERT TF\n",
        "# def create_retribert_classification_model(retribert_model,\n",
        "#                                           num_train_layers=0,\n",
        "#                                           hidden_size = 200,\n",
        "#                                           dropout=0.3,\n",
        "#                                           learning_rate=0.00005):\n",
        "#     \"\"\"\n",
        "#     Build a simple classification model with BERT. Use the CLS Output for classification purposes\n",
        "#     \"\"\"\n",
        "#     if num_train_layers == 0:\n",
        "#         # Freeze all layers of pre-trained BERT model\n",
        "#         retribert_model.trainable = False\n",
        "\n",
        "#     elif num_train_layers == 12: ##### NUMBER OF RETRIBERT MODELS. NEED TO UPDATE.\n",
        "#         # Train all layers of the BERT model\n",
        "#         retribert_model.trainable = True\n",
        "\n",
        "#     else:\n",
        "#         # Restrict training to the num_train_layers outer transformer layers\n",
        "#         retrain_layers = []\n",
        "\n",
        "#         for retrain_layer_number in range(num_train_layers):\n",
        "\n",
        "#             layer_code = '_' + str(11 - retrain_layer_number)\n",
        "#             retrain_layers.append(layer_code)\n",
        "          \n",
        "        \n",
        "#         print('retrain layers: ', retrain_layers)\n",
        "\n",
        "#         for w in retribert_model.weights:\n",
        "#             if not any([x in w.name for x in retrain_layers]):\n",
        "#                 #print('freezing: ', w)\n",
        "#                 w._trainable = False\n",
        "\n",
        "#     input_ids = tf.keras.layers.Input(shape=(MAX_LENGTH,), dtype=tf.int64, name='input_ids_layer')\n",
        "#     token_type_ids = tf.keras.layers.Input(shape=(MAX_LENGTH,), dtype=tf.int64, name='token_type_ids_layer')\n",
        "#     attention_mask = tf.keras.layers.Input(shape=(MAX_LENGTH,), dtype=tf.int64, name='attention_mask_layer')\n",
        "\n",
        "#     retribert_inputs = {'input_ids': input_ids,\n",
        "#                         'token_type_ids': token_type_ids,\n",
        "#                         'attention_mask': attention_mask}      \n",
        "\n",
        "#     retribert_out = retribert_model(retribert_inputs)\n",
        "\n",
        "#     cls_token = retribert_out[0][:, 0, :]\n",
        "\n",
        "#     hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(cls_token)\n",
        "\n",
        "\n",
        "#     hidden = tf.keras.layers.Dropout(dropout)(hidden)\n",
        "\n",
        "\n",
        "#     classification = tf.keras.layers.Dense(1, activation='sigmoid',name='classification_layer')(hidden)\n",
        "    \n",
        "#     classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[classification])\n",
        "    \n",
        "#     classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "#                                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \n",
        "#                                  metrics='accuracy')\n",
        "    \n",
        "#     return classification_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kiw3dT5U0rjI"
      },
      "outputs": [],
      "source": [
        "# retribert_classification_model = create_retribert_classification_model(retribert_model, num_train_layers=0)\n",
        "# retribert_classification_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXEi1T5WBmpv"
      },
      "outputs": [],
      "source": [
        "# Import required libraries/code\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "     \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_CpwOxtEIQ0"
      },
      "outputs": [],
      "source": [
        "# from torch.utils.data import Dataset\n",
        "\n",
        "# class PandasDataset(Dataset):\n",
        "#     def __init__(self, dataframe):\n",
        "#         self.dataframe = dataframe\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.dataframe)\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         return self.dataframe.iloc[index]\n",
        "\n",
        "# from torch.utils.data import DataLoader\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# train, test = train_test_split(orig_train_df, test_size=0.2, random_state=0)\n",
        "# train_dataset = PandasDataset(train)\n",
        "# test_dataset = PandasDataset(test)\n",
        "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16)\n",
        "# test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# train_dataloader = torch.utils.data.DataLoader(X_train, batch_size=64, shuffle=True)\n",
        "# test_dataloader = torch.utils.data.DataLoader(X_test, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um1mFaPz7shm"
      },
      "outputs": [],
      "source": [
        "# # Set the manual seeds\n",
        "# torch.manual_seed(42)\n",
        "# torch.cuda.manual_seed(42)\n",
        "# class_names = [1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0]\n",
        "# # Get the length of class_names (one output unit for each class)\n",
        "# output_shape = len(class_names)\n",
        "\n",
        "\n",
        "# # Recreate the classifier layer and seed it to the target device\n",
        "# retribert_model.classifier = torch.nn.Sequential(\n",
        "#     torch.nn.Dropout(p=0.2, inplace=True), \n",
        "#     torch.nn.Linear(in_features=1280, \n",
        "#                     out_features=output_shape, # same number of output units as our number of classes\n",
        "#                     bias=True)).to(device)\n",
        "\n",
        "\n",
        "# # Define loss and optimizer\n",
        "# loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(retribert_model.parameters(), lr=0.001)\n",
        "\n",
        "# # retribert_model_results = engine.train(model=retribert_model,\n",
        "# #                                        train_dataloader=train_dataloader,\n",
        "# #                                        test_dataloader=test_dataloader,\n",
        "# #                                        optimizer=optimizer,\n",
        "# #                                        loss_fn=loss_fn,\n",
        "# #                                        epochs=5,\n",
        "# #                                        device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsQiinBXmpPV"
      },
      "outputs": [],
      "source": [
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9TCblE5n5Ht"
      },
      "outputs": [],
      "source": [
        "train_encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O710Kp2ClQpP"
      },
      "outputs": [],
      "source": [
        "## copy of code above\n",
        "X_train, X_val, X_test = df_train['full_text'], df_val['full_text'], df_test['full_text']\n",
        "y_train, y_val, y_test = np.array(df_train[label_cols]), np.array(df_val[label_cols]), np.array(df_test[label_cols])\n",
        "\n",
        "#### from\n",
        "# https://stackoverflow.com/questions/55369821/how-to-train-a-neural-network-model-with-bert-embeddings-instead-of-static-embed\n",
        "retribert_tokenizer = transformers.BertTokenizer.from_pretrained(retribert_model_checkpoints)\n",
        "# retribert_tokenizer = transformers.AutoTokenizer.from_pretrained(retribert_model_checkpoints, use_fast=False, normalization = True)\n",
        "\n",
        "X_train = [retribert_tokenizer.tokenize('[CLS] ' + sent + ' [SEP]') for sent in X_train] \n",
        "X_train_tokens = [retribert_tokenizer.convert_tokens_to_ids(sent) for sent in X_train]\n",
        "# # X_train[0]\n",
        "# # X_train_tokens[0]\n",
        "X_test = [retribert_tokenizer.tokenize('[CLS] ' + sent + ' [SEP]') for sent in X_test] \n",
        "X_test_tokens = [retribert_tokenizer.convert_tokens_to_ids(sent) for sent in X_test]\n",
        "# y_train_tensor = torch.from_numpy(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lIKdSf6rkyQ"
      },
      "outputs": [],
      "source": [
        "### train individual tensors\n",
        "y_train_cohesion = [y_train[i][0] for i in range(len(y_train))]\n",
        "y_train_syntax = [y_train[i][1] for i in range(len(y_train))]\n",
        "y_train_vocabulary = [y_train[i][2] for i in range(len(y_train))]\n",
        "y_train_phraseology = [y_train[i][3] for i in range(len(y_train))]\n",
        "y_train_grammar = [y_train[i][4] for i in range(len(y_train))]\n",
        "y_train_convention = [y_train[i][5] for i in range(len(y_train))]\n",
        "\n",
        "y_train_cohesion_tensor = torch.Tensor(y_train_cohesion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftdF-3pK1xgu"
      },
      "outputs": [],
      "source": [
        "# ## trying with dataloader\n",
        "# # len(X_train_tokens) #3128\n",
        "# # len(y_train_cohesion) #33128\n",
        "# cohesion_dict = {\"X_train_tokens\":X_train_tokens,\n",
        "#                  \"y_train_cohesion\": y_train_cohesion}\n",
        "# cohesion_df = pd.DataFrame(cohesion_dict)\n",
        "\n",
        "# class customDataset:\n",
        "#   def __init__(self):\n",
        "#     xy = cohesion_df\n",
        "#     self.len = xy.shape[0]\n",
        "#     self.x_data = torch.from_numpy(xy[:,0:-1])\n",
        "#     self.y_data = torch.from_numpy(xy.iloc[:,[-1]])\n",
        "#   def __getitem__(self,index):\n",
        "#     return self.x_data[index], self.y_data[index]\n",
        "#   def __len__(self):\n",
        "#     return self.len\n",
        "\n",
        "# cohesion_ds = customDataset()\n",
        "# training_data = torchvision.datasets.cohesion_ds(\n",
        "#     root=\"data\",\n",
        "#     train=True,\n",
        "#     download=True,\n",
        "#     transform=torchvision.transformsToTensor()\n",
        "# )\n",
        "\n",
        "# # train_dataloader = torch.utils.data.DataLoader(cohesion_df, batch_size=8, shuffle=True)\n",
        "# # train_dataloader\n",
        "\n",
        "# # train_features, train_labels = iter(train_dataloader)\n",
        "# # print(f\"Feature batch shape: {train_features.size()}\")\n",
        "# # print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "# # img = train_features[0].squeeze()\n",
        "# # label = train_labels[0]\n",
        "# # plt.imshow(img, cmap=\"gray\")\n",
        "# # plt.show()\n",
        "# # print(f\"Label: {label}\")\n",
        "# # test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True)\n",
        "\n",
        "class pyDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, X,y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "  def __getitem__(self,idx):\n",
        "    return self.X[idx], self.y[idx]\n",
        "train_loader = torch.utils.data.DataLoader(dataset=pyDataset(X_train, y_train), batch_size=8, shuffle=T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4k7Cqm3CFLw"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/64156202/add-dense-layer-on-top-of-huggingface-bert-model\n",
        "### this seems to be working  but out of memory\n",
        "class CustomBERTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "          super(CustomBERTModel, self).__init__()\n",
        "          self.retribert = transformers.BertModel.from_pretrained(retribert_model_checkpoints)\n",
        "          # for param in self.retribert.parameters():\n",
        "            # param.requires_grad = False\n",
        "          self.linear1 = nn.Linear(768, 9)\n",
        "          self.dropout1 = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, ids, mask):\n",
        "          outputs = self.retribert(ids,attention_mask=mask)\n",
        "          last_hidden_state = outputs[0]\n",
        "\n",
        "          # sequence_output has the following shape: (batch_size, sequence_length, 768)\n",
        "          linear1_output = self.linear1(last_hidden_state[:,0,:].view(-1,768)) ## extract the 1st token's embeddings\n",
        "          # ### New layers:\n",
        "          linear2_dropout_output = self.dropout1(linear1_output)\n",
        "\n",
        "          return linear2_dropout_output\n",
        "\n",
        "retribert_model = CustomBERTModel() # You can pass the parameters if required to have more flexible model\n",
        "retribert_model.to(torch.device(\"cuda\")) ## can be gpu\n",
        "criterion = nn.CrossEntropyLoss() ## If required define your own criterion\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, retribert_model.parameters()))\n",
        "\n",
        "for epoch in range(5):\n",
        "  optimizer.zero_grad()\n",
        "  predicted = []\n",
        "  for sent in X_train:\n",
        "    encoded_sent = retribert_tokenizer.encode_plus(sent, \n",
        "                                                   add_special_tokens=True, \n",
        "                                                   truncation=True, \n",
        "                                                   padding=True, \n",
        "                                                   return_attention_mask=True, \n",
        "                                                   return_tensors='pt', \n",
        "                                                   max_length=510)\n",
        "    outputs_1 = retribert_model(encoded_sent[\"input_ids\"].to(\"cuda\"), encoded_sent[\"attention_mask\"].to(\"cuda\"))\n",
        "    outputs_1\n",
        "  #   final_outputs = torch.nn.functional.log_softmax(outputs_1, dim=1)\n",
        "  # # print(final_outputs)\n",
        "  #   final_outputs_max = torch.max(final_outputs)\n",
        "  #   predicted.append(final_outputs_max)\n",
        "  # print(predicted)\n",
        "  # print(final_outputs_2)\n",
        "        # input_ids = encoding['input_ids']\n",
        "        # attention_mask = encoding['attention_mask']\n",
        "  # loss = criterion(final_outputs, targets)\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "\n",
        "\n",
        "# for epoch in epochs:\n",
        "#     for batch in data_loader: ## If you have a DataLoader()  object to get the data.\n",
        "\n",
        "#         data = batch[0]\n",
        "#         targets = batch[1] ## assuming that data loader returns a tuple of data and its targets\n",
        "        \n",
        "#         optimizer.zero_grad()   \n",
        "#         encoding = tokenizer.batch_encode_plus(data, return_tensors='pt', padding=True, truncation=True,max_length=510, add_special_tokens = True)\n",
        "        # outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        # outputs = F.log_softmax(outputs, dim=1)\n",
        "        # input_ids = encoding['input_ids']\n",
        "        # attention_mask = encoding['attention_mask']\n",
        "        # loss = criterion(outputs, targets)\n",
        "        # loss.backward()\n",
        "        # optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2t3c-HtCffw"
      },
      "outputs": [],
      "source": [
        "# use MCRMSE as loss metric\n",
        "\n",
        "# def MCRMSE(y_true, y_pred):\n",
        "#     colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis = 1)\n",
        "#     return tf.reduce_mean(tf.sqrt(colwise_mse), axis = -1, keepdims = True)\n",
        "     \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlP4F2WSXDLd"
      },
      "outputs": [],
      "source": [
        "### anothing thing\n",
        "### https://stackoverflow.com/questions/68115993/input-ids-torch-tensorinput-ids-valueerror-expected-sequence-of-length-133\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "# print(flaubert)\n",
        "for sent in X_train:\n",
        "    encoded_sent = retribert_tokenizer.encode_plus(sent, add_special_tokens=True, truncation=True, padding=True, return_attention_mask=True, return_tensors='pt', max_length=510)\n",
        "\n",
        "    # Add the outputs to the lists\n",
        "    # input_ids.append(encoded_sent.get('input_ids'))\n",
        "    # attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "\n",
        "# input_ids = torch.as_tensor(input_ids)\n",
        "# attention_masks = torch.as_tensor(attention_masks)\n",
        "\n",
        "hidden_state = retribert_model(input_ids=encoded_sent[\"input_ids\"].to(\"cuda\"), attention_mask=encoded_sent[\"attention_mask\"].to(\"cuda\"))\n",
        "hidden_state\n",
        "# # # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "last_hidden_state_cls = hidden_state[0][:, 0, :]\n",
        "\n",
        "retribert_model.linear1 = nn.Linear(768, 256)\n",
        "retribert_model.dropout1 = nn.Dropout(0.1)\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vImVYBNL56NB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd7c8a96-162c-426f-fed1-34e87698b0b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0186, -0.0166, -0.0154,  ...,  0.0220,  0.0171,  0.0076],\n",
              "        [ 0.0118,  0.0026,  0.0049,  ..., -0.0287,  0.0192,  0.0038],\n",
              "        ...,\n",
              "        [-0.0003, -0.0193, -0.0029,  ..., -0.0182, -0.0184, -0.0021],\n",
              "        [ 0.0355, -0.0214, -0.0174,  ..., -0.0083,  0.0015,  0.0047],\n",
              "        [-0.0179,  0.0016,  0.0094,  ..., -0.0528,  0.0078,  0.0255]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "list(retribert_model.parameters())[0] # model weights?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAY8GJHXZtKi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "a24c7e28-b4fc-4fce-c006-a0481836ae36"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-35ae434d271b>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretribert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_hidden_state_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    984\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m                 \u001b[0mbuffered_token_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m                 \u001b[0mbuffered_token_type_ids_expanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffered_token_type_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m                 \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffered_token_type_ids_expanded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (768) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [1, 768].  Tensor sizes: [1, 512]"
          ]
        }
      ],
      "source": [
        "# Set the manual seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# # Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n",
        "# for param in retribert_model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# retribert_model(last_hidden_state_cls)\n",
        "\n",
        "# ## from https://pypi.org/project/pytorch-pretrained-bert/\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     predictions = retribert_model(last_hidden_state_cls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thhTWuFiV1VN"
      },
      "outputs": [],
      "source": [
        "# results = torch.zeros((len(X_test_tokens), retri_cuda.config.hidden_size)).long()\n",
        "# results[0]\n",
        "# with torch.no_grad():\n",
        "#     for stidx in range(0, len(X_test_tokens), batch_size):\n",
        "#         X = X_test_tokens[stidx:stidx + batch_size]\n",
        "#         X = torch.LongTensor(X).cuda()\n",
        "#         embed, pooled_output = retri_cuda(X)\n",
        "#         results[stidx:stidx + batch_size,:] = embed.cpu()\n",
        "# torch.LongTensor(X)\n",
        "\n",
        "#### from https://machinelearningmastery.com/building-a-regression-model-in-pytorch/\n",
        "# X_train_torch = torch.tensor(X_train_tokens, dtype=torch.float32)\n",
        "# y_train_torch = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
        "# X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n",
        "# y_test_torch = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
        " \n",
        "# # training parameters\n",
        "# n_epochs = 5   # number of epochs to run\n",
        "# batch_size = 8  # size of each batch\n",
        "# batch_start = torch.arange(0, len(X_train), batch_size)\n",
        " \n",
        "# # Hold the best model\n",
        "# best_mse = np.inf   # init to infinity\n",
        "# best_weights = None\n",
        "# history = []\n",
        " \n",
        "# # training loop\n",
        "# for epoch in range(n_epochs):\n",
        "#     retribert_model.train()\n",
        "#     with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
        "#         bar.set_description(f\"Epoch {epoch}\")\n",
        "#         for start in bar:\n",
        "#             # take a batch\n",
        "#             X_batch = X_train[start:start+batch_size]\n",
        "#             y_batch = y_train[start:start+batch_size]\n",
        "#             # forward pass\n",
        "#             y_pred = retribert_model(X_batch)\n",
        "#             loss = loss_fn(y_pred, y_batch)\n",
        "#             # backward pass\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             # update weights\n",
        "#             optimizer.step()\n",
        "#             # print progress\n",
        "#             bar.set_postfix(mse=float(loss))\n",
        "#     # evaluate accuracy at end of each epoch\n",
        "#     retribert_model.eval()\n",
        "#     y_pred = retribert_model(X_test)\n",
        "#     mse = loss_fn(y_pred, y_test)\n",
        "#     mse = float(mse)\n",
        "#     history.append(mse)\n",
        "#     if mse < best_mse:\n",
        "#         best_mse = mse\n",
        "#         best_weights = copy.deepcopy(retribert_model.state_dict())\n",
        " \n",
        "# # restore model and return best accuracy\n",
        "# retribert_model.load_state_dict(best_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hzAHU3_8oUl"
      },
      "outputs": [],
      "source": [
        "### evaluating the model\n",
        "# from https://discuss.pytorch.org/t/multi-class-bert-model-class-weights-and-where-to-use-them/176264\n",
        "# def evaluate():  \n",
        "#     model.eval()\n",
        "#     total_loss, total_accuracy = 0, 0\n",
        "#     total_preds = []\n",
        "#     for step,batch in enumerate(val_dataloader):\n",
        "#         if step % 50 == 0 and not step == 0:\n",
        "#             elapsed = format_time(time.time() - t0)\n",
        "#             print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "#         batch = [t.to(device) for t in batch]\n",
        "#         sent_id, mask, labels = batch\n",
        "#         with torch.no_grad():\n",
        "#             preds = model(sent_id, mask)\n",
        "#             loss = cross_entropy(preds,labels)  #<------- Here\n",
        "#             total_loss = total_loss + loss.item()\n",
        "#             preds = preds.detach().cpu().numpy()\n",
        "#             total_preds.append(preds)\n",
        "#     avg_loss = total_loss / len(val_dataloader) \n",
        "#     total_preds  = np.concatenate(total_preds, axis=0)\n",
        "#     return avg_loss, total_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting to TensorFlow?"
      ],
      "metadata": {
        "id": "28DHlMfOymrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow\n",
        "# !pip install tensorflow-addons\n",
        "# !pip install onnx\n",
        "# !git clone https://github.com/onnx/onnx-tensorflow.git && cd onnx-tensorflow && pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYiC0VJBtznL",
        "outputId": "88b97afd-e8f9-4ef9-b947-c1f201c3d97c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.53.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (0.0.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.9/dist-packages (0.19.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.9/dist-packages (from tensorflow-addons) (3.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow-addons) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.7->tensorflow-addons) (4.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.7->tensorflow-addons) (6.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.7->tensorflow-addons) (3.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.9/dist-packages (1.13.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.20.2 in /usr/local/lib/python3.9/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.9/dist-packages (from onnx) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from onnx) (1.22.4)\n",
            "fatal: destination path 'onnx-tensorflow' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "# from torchvision import datasets, transforms\n",
        "# from torch.autograd import Variable\n",
        "# import onnx\n",
        "# from onnx_tf.backend import prepare"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtVoQdW3t7g0",
        "outputId": "6a719768-50e9-425f-8d75-8d98985300c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.12.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# retribert_model = transformers.BertModel.from_pretrained(retribert_model_checkpoints)\n",
        "# torch.save(retribert_model.state_dict(), './retribert.pt')"
      ],
      "metadata": {
        "id": "NBDgFT8LuHxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_pytorch = retribert_model()\n",
        "# model_pytorch.load_state_dict('./retribert.pt')\n",
        "\n",
        "# dummy_input = torch.from_numpy(X_train_tokens.to(\"cuda\"))\n",
        "# dummy_output = model_pytorch(dummy_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "bMCLMKCqxrjz",
        "outputId": "e53819a9-3fa2-4a14-b5df-61d26b27b870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-8f55201d1509>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_pytorch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretribert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_pytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./retribert.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdummy_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdummy_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_pytorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify either input_ids or inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You have to specify either input_ids or inputs_embeds"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWSHeAu98m40XFSDWAHaZq",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}