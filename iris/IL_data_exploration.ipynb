{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNB5EQGl92bCxCGTfZiwIf7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilamaiti/spring_2023_w266_final_project_heesuk_iris_srila/blob/main/iris/IL_data_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/competitions/feedback-prize-english-language-learning"
      ],
      "metadata": {
        "id": "MZ9w2FlGlNDV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oUehh9Tfk8rA"
      },
      "outputs": [],
      "source": [
        "# # install\n",
        "# !pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import copy\n",
        "import tensorflow as tf\n",
        "import os"
      ],
      "metadata": {
        "id": "qNmUd_vinst-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "%cd \"gdrive/MyDrive/Colab Notebooks/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXFJZt1KlRIJ",
        "outputId": "30cb58e7-0cf6-4440-d428-fec6c68330ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## only needed to run it once to download\n",
        "\n",
        "# os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/Colab Notebooks/\"\n",
        "\n",
        "# !kaggle competitions download -c feedback-prize-english-language-learning \n",
        "# !unzip -q feedback-prize-english-language-learning.zip -d ."
      ],
      "metadata": {
        "id": "sdaPcHmjmlFS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"train.csv\")\n",
        "print(f\"train shape: {train.shape}\")\n",
        "train.head()"
      ],
      "metadata": {
        "id": "x3sZG8BKnUTx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "c8a58777-598e-4b4f-bc12-1874b5717712"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train shape: (3911, 8)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        text_id                                          full_text  cohesion  \\\n",
              "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
              "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
              "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n",
              "3  003885A45F42  The best time in life is when you become yours...       4.5   \n",
              "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n",
              "\n",
              "   syntax  vocabulary  phraseology  grammar  conventions  \n",
              "0     3.5         3.0          3.0      4.0          3.0  \n",
              "1     2.5         3.0          2.0      2.0          2.5  \n",
              "2     3.5         3.0          3.0      3.0          2.5  \n",
              "3     4.5         4.5          4.5      4.0          5.0  \n",
              "4     3.0         3.0          3.0      2.5          2.5  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c435db01-a62b-41e2-9b49-9f71033c2e3d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>full_text</th>\n",
              "      <th>cohesion</th>\n",
              "      <th>syntax</th>\n",
              "      <th>vocabulary</th>\n",
              "      <th>phraseology</th>\n",
              "      <th>grammar</th>\n",
              "      <th>conventions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0016926B079C</td>\n",
              "      <td>I think that students would benefit from learn...</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0022683E9EA5</td>\n",
              "      <td>When a problem is a change you have to let it ...</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00299B378633</td>\n",
              "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>003885A45F42</td>\n",
              "      <td>The best time in life is when you become yours...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0049B1DF5CCC</td>\n",
              "      <td>Small act of kindness can impact in other peop...</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c435db01-a62b-41e2-9b49-9f71033c2e3d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c435db01-a62b-41e2-9b49-9f71033c2e3d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c435db01-a62b-41e2-9b49-9f71033c2e3d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(\"test.csv\")\n",
        "print(f\"test shape: {test.shape}\")\n",
        "test.head()"
      ],
      "metadata": {
        "id": "Ncsa5RK4n2EC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "ba09f571-9c93-41c8-fd1d-77253938f1e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test shape: (3, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        text_id                                          full_text\n",
              "0  0000C359D63E  when a person has no experience on a job their...\n",
              "1  000BAD50D026  Do you think students would benefit from being...\n",
              "2  00367BB2546B  Thomas Jefferson once states that \"it is wonde..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-da1ff8fa-e882-45fe-8f04-03f3295402bf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000C359D63E</td>\n",
              "      <td>when a person has no experience on a job their...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000BAD50D026</td>\n",
              "      <td>Do you think students would benefit from being...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00367BB2546B</td>\n",
              "      <td>Thomas Jefferson once states that \"it is wonde...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da1ff8fa-e882-45fe-8f04-03f3295402bf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-da1ff8fa-e882-45fe-8f04-03f3295402bf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-da1ff8fa-e882-45fe-8f04-03f3295402bf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYT9I1iToJpr",
        "outputId": "83393459-fe38-4311-dd13-d36c5e4c217d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3911 entries, 0 to 3910\n",
            "Data columns (total 8 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   text_id      3911 non-null   object \n",
            " 1   full_text    3911 non-null   object \n",
            " 2   cohesion     3911 non-null   float64\n",
            " 3   syntax       3911 non-null   float64\n",
            " 4   vocabulary   3911 non-null   float64\n",
            " 5   phraseology  3911 non-null   float64\n",
            " 6   grammar      3911 non-null   float64\n",
            " 7   conventions  3911 non-null   float64\n",
            "dtypes: float64(6), object(2)\n",
            "memory usage: 244.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# length of full_text\n",
        "pprint.pprint(train[\"full_text\"][0])"
      ],
      "metadata": {
        "id": "dgG43xnX8wkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa6b6e09-cf6e-4f68-8d93-b2c01cf61922"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('I think that students would benefit from learning at home,because they wont '\n",
            " 'have to change and get up early in the morning to shower and do there hair. '\n",
            " \"taking only classes helps them because at there house they'll be pay more \"\n",
            " 'attention. they will be comfortable at home.\\n'\n",
            " '\\n'\n",
            " 'The hardest part of school is getting ready. you wake up go brush your teeth '\n",
            " 'and go to your closet and look at your cloths. after you think you picked a '\n",
            " 'outfit u go look in the mirror and youll either not like it or you look and '\n",
            " \"see a stain. Then you'll have to change. with the online classes you can \"\n",
            " 'wear anything and stay home and you wont need to stress about what to wear.\\n'\n",
            " '\\n'\n",
            " 'most students usually take showers before school. they either take it before '\n",
            " 'they sleep or when they wake up. some students do both to smell good. that '\n",
            " 'causes them do miss the bus and effects on there lesson time cause they come '\n",
            " 'late to school. when u have online classes u wont need to miss lessons cause '\n",
            " 'you can get everything set up and go take a shower and when u get out your '\n",
            " 'ready to go.\\n'\n",
            " '\\n'\n",
            " 'when your home your comfortable and you pay attention. it gives then an '\n",
            " 'advantage to be smarter and even pass there classmates on class work. public '\n",
            " 'schools are difficult even if you try. some teacher dont know how to teach '\n",
            " 'it in then way that students understand it. that causes students to fail and '\n",
            " 'they may repeat the class.              ')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train[\"full_text\"][0])"
      ],
      "metadata": {
        "id": "mm-WKueoUYxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "706f8783-6638-416d-9f12-55a1d966d0de"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1387"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# histogram with cohesion, syntax, vocabulary, phraseology, grammar, conventions \n",
        "for y in [\"cohesion\",\"syntax\",\"vocabulary\", \"phraseology\",\"grammar\", \"conventions\"]:\n",
        "  print(train[y].value_counts().sort_index())\n",
        "  print()\n",
        "  train[y].hist()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "TX_R3R7U8xQ4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "391bc113-747f-4794-954e-9dee4f4e38ff"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0      10\n",
            "1.5      27\n",
            "2.0     315\n",
            "2.5     790\n",
            "3.0    1096\n",
            "3.5     988\n",
            "4.0     534\n",
            "4.5     125\n",
            "5.0      26\n",
            "Name: cohesion, dtype: int64\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAASy0lEQVR4nO3df6zddX3H8edbioJc1yKYO9J2q4nExdHp6A1gWMyt3Rw/DCUZOhYmLcE021BxdBnVZGNzS1YTkQlbNI0Qy4ZeGOraFdCR0htjMjopMsoPHZVV7U3XCi1Xr+DM3d7743w67q63995zzj0/6Of5SG7u9/v9fL7n876fc87rfM/3nHtOZCaSpDq8qtcFSJK6x9CXpIoY+pJUEUNfkipi6EtSRRb1uoDZnHnmmblixYqW9//xj3/MaaedtnAFLRDrao51Nce6mnMi1rVnz57nMvMNMzZmZt/+rFq1Ktuxa9eutvbvFOtqjnU1x7qacyLWBTySx8lVT+9IUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JF+vpjGKR+tndsnPWb7uv6uPs3X9r1MXXi8Ehfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFVkztCPiDsi4nBEPDFl2+sj4sGIeKb8Pr1sj4i4NSL2RcTjEXHulH3Wlf7PRMS6zvw5kqTZzOdI/3PARdO2bQJ2ZubZwM6yDnAxcHb52QB8GhoPEsBNwPnAecBNxx4oJEndM2foZ+bXgCPTNq8FtpblrcDlU7bfmQ0PA0si4izgN4EHM/NIZh4FHuRnH0gkSR0WmTl3p4gVwI7MPKesv5CZS8pyAEczc0lE7AA2Z+bXS9tO4EZgGDglM/+ybP8T4KXM/MQMY22g8SyBwcHBVSMjIy3/cRMTEwwMDLS8f6dYV3P6ta7DR8Y59FL3x125dPGs7f06X9bVnHbqWr169Z7MHJqpre2vS8zMjIi5Hznmf3lbgC0AQ0NDOTw83PJljY6O0s7+nWJdzenXum67axs37+3+N47uv2p41vZ+nS/rak6n6mr1FnsoIs7KzIPl9M3hsn0MWD6l37KybYzG0f7U7aMtji1VbcUc38u7ceVkx7671+/nfeVr9S2b24Fj78BZB2ybsv3q8i6eC4DxzDwIfBV4V0ScXl7AfVfZJknqojmP9CPiCzSO0s+MiAM03oWzGbgnIq4Fvgu8t3S/H7gE2Ae8CFwDkJlHIuIvgG+Ufh/LzOkvDkuSOmzO0M/M3zlO05oZ+iZw3XEu5w7gjqaqkyQtKP8jV5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRVpK/Qj4g8j4smIeCIivhARp0TEGyNid0Tsi4i7I+LVpe9ryvq+0r5iQf4CSdK8tRz6EbEU+BAwlJnnACcBVwIfB27JzDcBR4Fryy7XAkfL9ltKP0lSF7V7emcRcGpELAJeCxwE3gncW9q3ApeX5bVlndK+JiKizfElSU1oOfQzcwz4BPA9GmE/DuwBXsjMydLtALC0LC8Fvl/2nSz9z2h1fElS8yIzW9sx4nTgi8BvAy8A/0DjCP7PyikcImI58EBmnhMRTwAXZeaB0vYd4PzMfG7a5W4ANgAMDg6uGhkZaak+gImJCQYGBlrev1Osqzn9WtfhI+MceqnXVfyswVPpWF0rly5ued9+vR5PxLpWr169JzOHZmpb1EZNvw78R2b+ACAivgRcCCyJiEXlaH4ZMFb6jwHLgQPldNBi4PnpF5qZW4AtAENDQzk8PNxygaOjo7Szf6dYV3P6ta7b7trGzXvbuQt1xsaVkx2ra/9Vwy3v26/XY211tXPL+B5wQUS8FngJWAM8AuwCrgBGgHXAttJ/e1n/l9L+ULb6NEN9Z8Wm+zp22RtXTrL+OJe/f/OlHRtXOhG1c05/N43TOY8Ce8tlbQFuBG6IiH00ztnfXna5HTijbL8B2NRG3ZKkFrT1HDAzbwJumrb5WeC8Gfr+BHhPO+NJktrjf+RKUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFWkrdCPiCURcW9EfCsino6It0fE6yPiwYh4pvw+vfSNiLg1IvZFxOMRce7C/AmSpPlq90j/U8BXMvOXgLcCTwObgJ2ZeTaws6wDXAycXX42AJ9uc2xJUpNaDv2IWAy8A7gdIDN/mpkvAGuBraXbVuDysrwWuDMbHgaWRMRZrY4vSWpeZGZrO0a8DdgCPEXjKH8PcD0wlplLSp8AjmbmkojYAWzOzK+Xtp3AjZn5yLTL3UDjmQCDg4OrRkZGWqoPYGJigoGBgZb375QTsa69Y+MLXM3LBk+FQy/N3LZy6eKOjTuXw0fGj1tXL802X+1qZ75PxNt9J7VT1+rVq/dk5tBMbYvaqGkRcC7wwczcHRGf4uVTOQBkZkZEU48qmbmFxoMJQ0NDOTw83HKBo6OjtLN/p5yIda3fdN/CFjPFxpWT3Lx35pvq/quGOzbuXG67a9tx6+ql2earXe3M94l4u++kTtXVzjn9A8CBzNxd1u+l8SBw6Nhpm/L7cGkfA5ZP2X9Z2SZJ6pKWQz8z/xP4fkS8uWxaQ+NUz3ZgXdm2DthWlrcDV5d38VwAjGfmwVbHlyQ1r93ngB8E7oqIVwPPAtfQeCC5JyKuBb4LvLf0vR+4BNgHvFj6SpK6qK3Qz8zHgJleLFgzQ98ErmtnPElSe/yPXEmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKtJ/HxwiqW+taOMzljaunGz5M5r2b7605XH1/3mkL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klSRtkM/Ik6KiG9GxI6y/saI2B0R+yLi7oh4ddn+mrK+r7SvaHdsSVJzFuJI/3rg6SnrHwduycw3AUeBa8v2a4GjZfstpZ8kqYvaCv2IWAZcCny2rAfwTuDe0mUrcHlZXlvWKe1rSn9JUpdEZra+c8S9wF8BrwP+CFgPPFyO5omI5cADmXlORDwBXJSZB0rbd4DzM/O5aZe5AdgAMDg4uGpkZKTl+iYmJhgYGGh5/045EevaOza+wNW8bPBUOPTSzG0rly7u2LhzOXxk/Lh19dJs89VL7dTVyev5RLw/rl69ek9mDs3UtqjVgiLi3cDhzNwTEcOtXs50mbkF2AIwNDSUw8OtX/To6Cjt7N8pJ2Jd6zfdt7DFTLFx5SQ37535prr/quGOjTuX2+7adty6emm2+eqldurq5PV8It4fZ9POLeNC4LKIuAQ4Bfg54FPAkohYlJmTwDJgrPQfA5YDByJiEbAYeL6N8SVJTWr5nH5mfiQzl2XmCuBK4KHMvArYBVxRuq0DtpXl7WWd0v5QtnNuSZLUtE68T/9G4IaI2AecAdxett8OnFG23wBs6sDYkqRZLMiJv8wcBUbL8rPAeTP0+QnwnoUYT5LUGv8jV5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JF+u/zV9WyFW1+vPHGlZMd/YhkSb3nkb4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIq0HPoRsTwidkXEUxHxZERcX7a/PiIejIhnyu/Ty/aIiFsjYl9EPB4R5y7UHyFJmp92jvQngY2Z+RbgAuC6iHgLsAnYmZlnAzvLOsDFwNnlZwPw6TbGliS1oOXQz8yDmfloWf4R8DSwFFgLbC3dtgKXl+W1wJ3Z8DCwJCLOanV8SVLzIjPbv5CIFcDXgHOA72XmkrI9gKOZuSQidgCbM/PrpW0ncGNmPjLtsjbQeCbA4ODgqpGRkZbrmpiYYGBgoOX9O6VTde0dG29r/8FT4dBLC1TMApqtrpVLF3e3mCkOHxl/xc1XL7VTVyev5xMxJ1avXr0nM4dmalvUVlVARAwAXwQ+nJk/bOR8Q2ZmRDT1qJKZW4AtAENDQzk8PNxybaOjo7Szf6d0qq71m+5ra/+NKye5eW/bN4kFN1td+68a7m4xU9x217ZX3Hz1Ujt1dfJ6ri0n2nr3TkScTCPw78rML5XNh46dtim/D5ftY8DyKbsvK9skSV3Szrt3ArgdeDozPzmlaTuwriyvA7ZN2X51eRfPBcB4Zh5sdXxJUvPaeQ54IfA+YG9EPFa2fRTYDNwTEdcC3wXeW9ruBy4B9gEvAte0MbYkqQUth355QTaO07xmhv4JXNfqeJKk9vkfuZJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SK9N+nMknSNCva/DDB2WxcOXncDyvcv/nSjo3bKx7pS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiF+X2AFzfbXbbF/PJql/dPJrGufyuYtO68jleqQvSRXpeuhHxEUR8e2I2BcRm7o9viTVrKundyLiJOBvgd8ADgDfiIjtmflUJ8bbOzbuaRRJmqLbR/rnAfsy89nM/CkwAqztcg2SVK3IzO4NFnEFcFFmvr+svw84PzM/MKXPBmBDWX0z8O02hjwTeK6N/TvFuppjXc2xruaciHX9Yma+YaaGvnv3TmZuAbYsxGVFxCOZObQQl7WQrKs51tUc62pObXV1+/TOGLB8yvqysk2S1AXdDv1vAGdHxBsj4tXAlcD2LtcgSdXq6umdzJyMiA8AXwVOAu7IzCc7OOSCnCbqAOtqjnU1x7qaU1VdXX0hV5LUW/5HriRVxNCXpIq84kM/Iu6IiMMR8cRx2iMibi0f+/B4RJzbJ3UNR8R4RDxWfv60CzUtj4hdEfFURDwZEdfP0Kfr8zXPuro+X2XcUyLiXyPi30ptfz5Dn9dExN1lznZHxIo+qWt9RPxgypy9v9N1lXFPiohvRsSOGdq6PlfzrKsnc1XG3h8Re8u4j8zQvrD3ycx8Rf8A7wDOBZ44TvslwANAABcAu/ukrmFgR5fn6izg3LL8OuDfgbf0er7mWVfX56uMG8BAWT4Z2A1cMK3PHwCfKctXAnf3SV3rgb/pwZzdAHx+puurF3M1z7p6Mldl7P3AmbO0L+h98hV/pJ+ZXwOOzNJlLXBnNjwMLImIs/qgrq7LzIOZ+WhZ/hHwNLB0Wreuz9c86+qJMg8TZfXk8jP93Q9rga1l+V5gTUREH9TVdRGxDLgU+OxxunR9ruZZVz9b0PvkKz7052Ep8P0p6wfok0AB3l6enj8QEb/czYHL0+pfpXGEOFVP52uWuqBH81VOCzwGHAYezMzjzllmTgLjwBl9UBfAb5VTAvdGxPIZ2hfaXwN/DPzPcdp7MlfzqAu6P1fHJPDPEbEnGh9DM92C3idrCP1+9SiNz8d4K3Ab8I/dGjgiBoAvAh/OzB92a9y5zFFXz+YrM/87M99G4z/Iz4uIc7o19mzmUdc/ASsy81eAB3n5CLsjIuLdwOHM3NPJcZo1z7q6OlfT/FpmngtcDFwXEe/o5GA1hH5ffvRDZv7w2NPzzLwfODkizuz0uBFxMo1gvSszvzRDl57M11x19Wq+ptXwArALuGha0//NWUQsAhYDz/e6rsx8PjP/q6x+FljV4VIuBC6LiP00PkH3nRHx99P69GKu5qyrB3M1deyx8vsw8GUan0Y81YLeJ2sI/e3A1eUV8AuA8cw82OuiIuLnj53LjIjzaFwXHb3xl/FuB57OzE8ep1vX52s+dfVivspYb4iIJWX5VBrfBfGtad22A+vK8hXAQ1legetlXdPO+15G47WSjsnMj2TmssxcQeNF2ocy83endev6XM2nrm7P1ZRxT4uI1x1bBt4FTH/H34LeJ/vuUzabFRFfoPHOjjMj4gBwE40XtcjMzwD303j1ex/wInBNn9R1BfD7ETEJvARc2ekbP40jnvcBe8u5YICPAr8wpa5ezNd86urFfEHjnUVbo/EFQK8C7snMHRHxMeCRzNxO4wHr7yJiH40X76/sk7o+FBGXAZOlrvVdqOtn9MFczaeuXs3VIPDlcjyzCPh8Zn4lIn4POnOf9GMYJKkiNZzekSQVhr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqyP8CSc/3KjD0BA0AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0      11\n",
            "1.5      29\n",
            "2.0     410\n",
            "2.5     839\n",
            "3.0    1250\n",
            "3.5     867\n",
            "4.0     388\n",
            "4.5     100\n",
            "5.0      17\n",
            "Name: syntax, dtype: int64\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0       2\n",
            "1.5      14\n",
            "2.0     124\n",
            "2.5     528\n",
            "3.0    1503\n",
            "3.5    1007\n",
            "4.0     577\n",
            "4.5     115\n",
            "5.0      41\n",
            "Name: vocabulary, dtype: int64\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0      10\n",
            "1.5      11\n",
            "2.0     350\n",
            "2.5     772\n",
            "3.0    1153\n",
            "3.5     929\n",
            "4.0     553\n",
            "4.5     108\n",
            "5.0      25\n",
            "Name: phraseology, dtype: int64\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0      8\n",
            "1.5     20\n",
            "2.0    544\n",
            "2.5    855\n",
            "3.0    994\n",
            "3.5    880\n",
            "4.0    447\n",
            "4.5    134\n",
            "5.0     29\n",
            "Name: grammar, dtype: int64\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0      15\n",
            "1.5      20\n",
            "2.0     402\n",
            "2.5     784\n",
            "3.0    1151\n",
            "3.5     908\n",
            "4.0     484\n",
            "4.5     122\n",
            "5.0      25\n",
            "Name: conventions, dtype: int64\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD7CAYAAACG50QgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUWklEQVR4nO3de4xcZ3nH8e9DnIQQ0zhgtLVstxsJiyrEUJJVEpQKrUlLnQTFkRpoUAo2DbIo4dLiCpxWanpDctWmXFIKspIUQw1OGqB2k3CxkmwRUmOwgca5cFmCadYyMcHJwpIA2vbpH/OmHZa9zczOzNrv9yON9pz3vOe8z76789uzZ26RmUiS6vCsfhcgSeodQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSJzhn5E3BIRRyPigaa2v42Ir0fE/RHx6YhY1rTtuogYjYhvRMRvN7WvL22jEbF1wb8TSdKc5nOm/xFg/ZS2vcA5mfkS4JvAdQARcTZwFfDiss8/RsRJEXES8EHgEuBs4HWlrySph5bM1SEzvxARg1PaPt+0eh9wZVneAOzKzJ8C34mIUeD8sm00Mx8BiIhdpe9Ds429fPnyHBwcnK3LrH784x9z+umnt71/t1hXa6yrNdbVmhOxrgMHDjyemS+YbtucoT8Pvw/cWpZX0vgj8Iyx0gbw6JT2C+Y68ODgIPv372+7sJGREYaHh9vev1usqzXW1Rrras2JWFdEfHembR2FfkT8KTAJ7OzkOFOOuRnYDDAwMMDIyEjbx5qYmOho/26xrtZYV2usqzXV1ZWZc96AQeCBKW2bgP8AntPUdh1wXdP654CXl9vnZuo30+28887LTtx7770d7d8t1tUa62qNdbXmRKwL2J8z5GpbT9mMiPXAu4DLM/Oppk17gKsi4tSIOAtYA3wJ+DKwJiLOiohTaDzYu6edsSVJ7Zvz8k5EfAIYBpZHxBhwPY0z9VOBvREBcF9mvjkzH4yI22g8QDsJXJuZ/12O81YaZ/4nAbdk5oNd+H4kSbOYz7N3XjdN882z9H8P8J5p2u8C7mqpOknSgvIVuZJUEUNfkipi6EtSRQx9SarIQrwiV6rSwcPjbNp6Z8/HPbTtsp6PqROHZ/qSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakic4Z+RNwSEUcj4oGmtudFxN6I+Fb5emZpj4j4QESMRsT9EXFu0z4bS/9vRcTG7nw7kqTZzOdM/yPA+iltW4G7M3MNcHdZB7gEWFNum4EPQeOPBHA9cAFwPnD9M38oJEm9M2foZ+YXgGNTmjcAO8ryDuCKpvaPZsN9wLKIWAH8NrA3M49l5hPAXn7xD4kkqcvavaY/kJlHyvL3gIGyvBJ4tKnfWGmbqV2S1ENLOj1AZmZE5EIUAxARm2lcGmJgYICRkZG2jzUxMdHR/t1iXa1ZrHUNnAZb1k72fNy55mKxzpd1taZbdbUb+o9FxIrMPFIu3xwt7YeB1U39VpW2w8DwlPaR6Q6cmduB7QBDQ0M5PDw8Xbd5GRkZoZP9u8W6WrNY67px525uONjxeVPLDl09POv2xTpf1tWabtXV7uWdPcAzz8DZCOxuan9DeRbPhcB4uQz0OeBVEXFmeQD3VaVNktRDc56mRMQnaJylL4+IMRrPwtkG3BYR1wDfBV5but8FXAqMAk8BbwTIzGMR8VfAl0u/v8zMqQ8OS5K6bM7Qz8zXzbDp4mn6JnDtDMe5BbilpeokSQvKV+RKUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kV6f37wkrqyODWO2fdvmXtJJvm6NOuQ9su68px1Tue6UtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klSRjkI/Iv4oIh6MiAci4hMR8eyIOCsi9kXEaETcGhGnlL6nlvXRsn1wQb4DSdK8tR36EbESeDswlJnnACcBVwF/A7w3M18IPAFcU3a5BniitL+39JMk9VCnl3eWAKdFxBLgOcAR4JXA7WX7DuCKsryhrFO2XxwR0eH4kqQWtB36mXkY+Dvgv2iE/ThwAHgyMydLtzFgZVleCTxa9p0s/Z/f7viSpNZFZra3Y8SZwCeB3wWeBP6Fxhn8n5dLOETEauAzmXlORDwArM/MsbLt28AFmfn4lONuBjYDDAwMnLdr16626gOYmJhg6dKlbe/fLdbVmsVa19Fj4zz2dL+r+EUDp9G1utauPKPtfRfrz/FErGvdunUHMnNoum2dfDD6bwLfyczvA0TEp4CLgGURsaScza8CDpf+h4HVwFi5HHQG8IOpB83M7cB2gKGhoRweHm67wJGRETrZv1usqzWLta4bd+7mhoOd3IW6Y8vaya7Vdejq4bb3Xaw/x9rq6uSa/n8BF0bEc8q1+YuBh4B7gStLn43A7rK8p6xTtt+T7f6bIUlqS9unA5m5LyJuB74CTAJfpXGGfiewKyL+urTdXHa5GfhYRIwCx2g800cniMGtd3bt2FvWTrJphuMf2nZZ18aVTkQd/Q+YmdcD109pfgQ4f5q+PwFe08l4kqTO+IpcSaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0JakiHYV+RCyLiNsj4usR8XBEvDwinhcReyPiW+XrmaVvRMQHImI0Iu6PiHMX5luQJM1Xp2f67wc+m5m/BrwUeBjYCtydmWuAu8s6wCXAmnLbDHyow7ElSS1qO/Qj4gzgFcDNAJn5s8x8EtgA7CjddgBXlOUNwEez4T5gWUSsaHd8SVLrOjnTPwv4PvBPEfHViLgpIk4HBjLzSOnzPWCgLK8EHm3af6y0SZJ6JDKzvR0jhoD7gIsyc19EvB/4IfC2zFzW1O+JzDwzIu4AtmXmF0v73cC7M3P/lONupnH5h4GBgfN27drVVn0AExMTLF26tO39u+VErOvg4fEFrub/DZwGjz09/ba1K8/o2rhzOXpsfMa6+mm2+epUJ/N9Iv7ed1Mnda1bt+5AZg5Nt21JBzWNAWOZua+s307j+v1jEbEiM4+UyzdHy/bDwOqm/VeVtp+TmduB7QBDQ0M5PDzcdoEjIyN0sn+3nIh1bdp658IW02TL2kluODj9r+qhq4e7Nu5cbty5e8a6+mm2+epUJ/N9Iv7ed1O36mr78k5mfg94NCJeVJouBh4C9gAbS9tGYHdZ3gO8oTyL50JgvOkykCSpBzo9HXgbsDMiTgEeAd5I4w/JbRFxDfBd4LWl713ApcAo8FTpK0nqoY5CPzO/Bkx33ejiafomcG0n40mSOuMrciWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFVl8H/ApadEa7OCzkLesnWz7s5QPbbus7XH18zzTl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klSRjkM/Ik6KiK9GxB1l/ayI2BcRoxFxa0ScUtpPLeujZftgp2NLklqzEGf67wAeblr/G+C9mflC4AngmtJ+DfBEaX9v6SdJ6qGOQj8iVgGXATeV9QBeCdxeuuwArijLG8o6ZfvFpb8kqUc6fZfN9wHvAp5b1p8PPJmZk2V9DFhZllcCjwJk5mREjJf+j3dYg5r0610QJR0fIjPb2zHi1cClmfmWiBgG/hjYBNxXLuEQEauBz2TmORHxALA+M8fKtm8DF2Tm41OOuxnYDDAwMHDerl272qoPYGJigqVLl7a9f7d0s66Dh8fb3nfgNHjs6QUsZoHMVtfalWf0tpgmR4+NH3fz1U+d1NXNn/OJmBPr1q07kJlD023r5Ez/IuDyiLgUeDbwS8D7gWURsaSc7a8CDpf+h4HVwFhELAHOAH4w9aCZuR3YDjA0NJTDw8NtFzgyMkIn+3dLN+vq5Ex9y9pJbji4+D5iYba6Dl093Ntimty4c/dxN1/91Eld3fw515YTbV/Tz8zrMnNVZg4CVwH3ZObVwL3AlaXbRmB3Wd5T1inb78l2/82QJLWlG8/TfzfwzogYpXHN/ubSfjPw/NL+TmBrF8aWJM1iQf4HzMwRYKQsPwKcP02fnwCvWYjxJEnt8RW5klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JF2g79iFgdEfdGxEMR8WBEvKO0Py8i9kbEt8rXM0t7RMQHImI0Iu6PiHMX6puQJM1PJ2f6k8CWzDwbuBC4NiLOBrYCd2fmGuDusg5wCbCm3DYDH+pgbElSG9oO/cw8kplfKcs/Ah4GVgIbgB2l2w7girK8AfhoNtwHLIuIFe2OL0lq3YJc04+IQeBlwD5gIDOPlE3fAwbK8krg0abdxkqbJKlHIjM7O0DEUuDfgfdk5qci4snMXNa0/YnMPDMi7gC2ZeYXS/vdwLszc/+U422mcfmHgYGB83bt2tV2bRMTEyxdurTt/bulm3UdPDze9r4Dp8FjTy9gMQtktrrWrjyjt8U0OXps/Libr37qpK5u/pxPxJxYt27dgcwcmm7bkk6KioiTgU8COzPzU6X5sYhYkZlHyuWbo6X9MLC6afdVpe3nZOZ2YDvA0NBQDg8Pt13fyMgInezfLd2sa9PWO9ved8vaSW442NGvRFfMVtehq4d7W0yTG3fuPu7mq586qaubP+facqKTZ+8EcDPwcGb+fdOmPcDGsrwR2N3U/obyLJ4LgfGmy0CSpB7o5HTgIuD1wMGI+Fpp+xNgG3BbRFwDfBd4bdl2F3ApMAo8Bbyxg7ElSW1oO/TLtfmYYfPF0/RP4Np2x5Mkdc5X5EpSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUW37sySdIUgx28keBctqydnPWNCg9tu6xrY/eDZ/qSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIr4ISqSNItufoDLbD6y/vSuHLfnoR8R64H3AycBN2Xmtl7X0G1z/ZLM9Uk9ktQtPQ39iDgJ+CDwW8AY8OWI2JOZD3VjvIOHxw1XSWrS62v65wOjmflIZv4M2AVs6HENklStXof+SuDRpvWx0iZJ6oHIzN4NFnElsD4z31TWXw9ckJlvbeqzGdhcVl8EfKODIZcDj3ewf7dYV2usqzXW1ZoTsa5fzcwXTLeh1w/kHgZWN62vKm3/JzO3A9sXYrCI2J+ZQwtxrIVkXa2xrtZYV2tqq6vXl3e+DKyJiLMi4hTgKmBPj2uQpGr19Ew/Mycj4q3A52g8ZfOWzHywlzVIUs16/jz9zLwLuKtHwy3IZaIusK7WWFdrrKs1VdXV0wdyJUn95XvvSFJFjvvQj4hbIuJoRDwww/aIiA9ExGhE3B8R5y6SuoYjYjwivlZuf9ajulZHxL0R8VBEPBgR75imT8/nbJ519XzOIuLZEfGliPjPUtdfTNPn1Ii4tczXvogYXCR1bYqI7zfN15u6XVfT2CdFxFcj4o5ptvV8vuZRUz/n6lBEHCzj7p9m+8LeHzPzuL4BrwDOBR6YYfulwGeAAC4E9i2SuoaBO/owXyuAc8vyc4FvAmf3e87mWVfP56zMwdKyfDKwD7hwSp+3AB8uy1cBty6SujYB/9Dr37Ey9juBj0/38+rHfM2jpn7O1SFg+SzbF/T+eNyf6WfmF4Bjs3TZAHw0G+4DlkXEikVQV19k5pHM/EpZ/hHwML/4quiez9k86+q5MgcTZfXkcpv6QNgGYEdZvh24OCJiEdTVFxGxCrgMuGmGLj2fr3nUtJgt6P3xuA/9eVjMb/3w8vLv+Wci4sW9Hrz8W/0yGmeJzfo6Z7PUBX2Ys3JZ4GvAUWBvZs44X5k5CYwDz18EdQH8TrkkcHtErJ5meze8D3gX8D8zbO/HfM1VE/RnrqDxx/rzEXEgGu9IMNWC3h9rCP3F6is0Xir9UuBG4F97OXhELAU+CfxhZv6wl2PPZo66+jJnmfnfmfnrNF5Bfn5EnNOLcecyj7r+DRjMzJcAe/n/s+uuiYhXA0cz80C3x5qvedbU87lq8huZeS5wCXBtRLyim4PVEPpzvvVDP2TmD5/59zwbr104OSKW92LsiDiZRrDuzMxPTdOlL3M2V139nLMy5pPAvcD6KZv+b74iYglwBvCDfteVmT/IzJ+W1ZuA83pQzkXA5RFxiMa76L4yIv55Sp9ez9ecNfVprp4Z+3D5ehT4NI13I262oPfHGkJ/D/CG8gj4hcB4Zh7pd1ER8cvPXMeMiPNp/Cy6HhRlzJuBhzPz72fo1vM5m09d/ZiziHhBRCwry6fR+CyIr0/ptgfYWJavBO7J8ghcP+uact33chqPk3RVZl6Xmasyc5DGg7T3ZObvTenW0/maT039mKsy7ukR8dxnloFXAVOf8beg98fj/uMSI+ITNJ7VsTwixoDraTyoRWZ+mMarfy8FRoGngDcukrquBP4gIiaBp4Gruh0UxUXA64GD5XowwJ8Av9JUWz/mbD519WPOVgA7ovEBQM8CbsvMOyLiL4H9mbmHxh+rj0XEKI0H76/qck3zrevtEXE5MFnq2tSDuqa1COZrrpr6NVcDwKfLucwS4OOZ+dmIeDN05/7oK3IlqSI1XN6RJBWGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFflfU/GcCE0fTHQAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# any collinearity?\n",
        "cm = train.corr()\n",
        "print(cm)\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "sns.heatmap(cm, cmap=cmap)\n",
        "plt.show()\n",
        "# high correlation for all of them, especially phraseology+vocabulary"
      ],
      "metadata": {
        "id": "YYjjR6n086-c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "d20be00d-e329-4dcc-ed7f-1e93a7027b39"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             cohesion    syntax  vocabulary  phraseology   grammar  \\\n",
            "cohesion     1.000000  0.695459    0.666151     0.690058  0.638689   \n",
            "syntax       0.695459  1.000000    0.680562     0.725467  0.709525   \n",
            "vocabulary   0.666151  0.680562    1.000000     0.735261  0.654852   \n",
            "phraseology  0.690058  0.725467    0.735261     1.000000  0.719746   \n",
            "grammar      0.638689  0.709525    0.654852     0.719746  1.000000   \n",
            "conventions  0.666151  0.700025    0.664292     0.666842  0.673301   \n",
            "\n",
            "             conventions  \n",
            "cohesion        0.666151  \n",
            "syntax          0.700025  \n",
            "vocabulary      0.664292  \n",
            "phraseology     0.666842  \n",
            "grammar         0.673301  \n",
            "conventions     1.000000  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Any patterns of text? Head & tail has same topics? # range of topics with multiple paragraphs. pprint?\n",
        "[print(train[\"full_text\"][i]) for i in range(5)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rhf00V0E9AV8",
        "outputId": "55ff44cf-d2e1-4ee0-abfa-534441edd293"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I think that students would benefit from learning at home,because they wont have to change and get up early in the morning to shower and do there hair. taking only classes helps them because at there house they'll be pay more attention. they will be comfortable at home.\n",
            "\n",
            "The hardest part of school is getting ready. you wake up go brush your teeth and go to your closet and look at your cloths. after you think you picked a outfit u go look in the mirror and youll either not like it or you look and see a stain. Then you'll have to change. with the online classes you can wear anything and stay home and you wont need to stress about what to wear.\n",
            "\n",
            "most students usually take showers before school. they either take it before they sleep or when they wake up. some students do both to smell good. that causes them do miss the bus and effects on there lesson time cause they come late to school. when u have online classes u wont need to miss lessons cause you can get everything set up and go take a shower and when u get out your ready to go.\n",
            "\n",
            "when your home your comfortable and you pay attention. it gives then an advantage to be smarter and even pass there classmates on class work. public schools are difficult even if you try. some teacher dont know how to teach it in then way that students understand it. that causes students to fail and they may repeat the class.              \n",
            "When a problem is a change you have to let it do the best on you no matter what is happening it can change your mind. sometimes you need to wake up and look what is around you because problems are the best way to change what you want to change along time ago. A\n",
            "\n",
            "problem is a change for you because it can make you see different and help you to understand how tings wok.\n",
            "\n",
            "First of all it can make you see different then the others. For example i remember that when i came to the United States i think that nothing was going to change me because i think that nothing was going to change me because everything was different that my country and then i realist that wrong because a problem may change you but sometimes can not change the way it is, but i remember that i was really shy but i think that change a lot because sometimes my problems make me think that there is more thing that i never see in my life but i just need to see it from a different way and dont let nothing happened and ruing the change that i want to make because of just a problem. For example i think that nothing was going to change me and that i dont need to be shy anymore became i need to start seeing everything in a different ways because you can get mad at every one but you need to know what is going to happened after,\n",
            "\n",
            "people may see you different but the only way that you know how to change is to do the best and don't let nothing or not body to change nothing about you. The way you want to change not one have that and can't do nothing about it because is your choice and your problems and you can decide what to do with it.\n",
            "\n",
            "second of all can help you to understand how things work. For instance my mom have a lot of problems but she have faith when she is around people, my mom is scare of high and i'm not scare of high i did not understand why my mos is scare of high and in not scare of high and every time i see my mom in a airplane it make me laugh because she is scare and is funny, but i see it from a different way and i like the high but also she have to understand that hoe things work in other people because it can no be the same as you. For example i think that my mom and me are different because we are and i have to understand that she does not like high and i need to understand that. to help someone to understand how things work you need to start to see how things work in that persons life.\n",
            "\n",
            "A problem is a change for you and can make you a different and help you to understand. Everyone has a different opinion and a different was to understand then others. everyone can see the different opinion and what other people think.\n",
            "Dear, Principal\n",
            "\n",
            "If u change the school policy of having a grade b average that unfair. Because many students have a C average. So that means that they cant go out for sports or other activities they want to do bad. That's like taking everything they have. What if kids want to become good at something, but now they cant because of that school policy. If they have a C average they should still be able to go out for sports or activities. A C average isn't that bad, its higher then a D average. If the school police was if you have a D average of lower they shouldn't do sports or activities. If they have a D average in school for not working hard, that's means that they in ain't going to try hard. If they have a C average and there trying hard they should be able to out for sports or activities. What if all the good people in sports have a C average in school, that means that they cant play and were going to lose every game we have. That's a good policy to get grade's up but don't take away something they care about. Everyone should be able to go out for sports if they want to. If the school policy happens, schools going to be boarding now, because now students cant go out for sports or other activities. The students that are doing good in school should feel good about themselves but we shouldn't take the other students away from the others ones. If we do this policy student will try to raised their grade but if they cant what happens they them. Should they just be out of it and think that schools boarding. If they do this its like taking away their video games. All I'm saying is that they have the right to go out for sports or activities.\n",
            "The best time in life is when you become yourself. I agree that the greatest accomplishment, is when you be yourself in a world that constantly trying to make you something else. Because you make your own choices, you become more happy, and you respect others.\n",
            "\n",
            "First, you make your own choices by being yourself. Becoming yourself means that you should be able to make your own choices and not be shy or afraid of what you're doing. Because you're defining yourself by doing those things that you want. Some people follow others, therefore, they don't make their own choices. People are afraid to make their own choice because they don't want to get rejected or be wrong. In this world, most of the teenagers act and follow the people who are strong and bullies. The reason they follow them and make the same choices they make, is because they're afraid that they will get beaten up and be left out. Personally when I came to the United States I used to follow other, because I thought they were amazing. But one day, I knew I was walking in the wrong path and I left them. Making your own choices makes you happy because you get to do what you like and what you want to do. It's hard to make your own choices, but one day you're going to have to make your own choices. The more you mature the more it will become easier for you to not be afraid of what you're doing and how other people are looking at it, because at the end of the day It's your life and you do what you think is right.\n",
            "\n",
            "Second, You become more happy by being yourself because you make your own choices, You take responsibilities and you do whatever makes you happy. Also, some people act like they're happy while not being themselves, but matter of fact, they're depressed from the inside. Being yourself makes you more happy because you feel like you achieved something big. Also, Some religious kids who become themselves love everyone. In the bible it says \"Those who know God should love others, because God is love.\"\n",
            "\n",
            "Not being yourself will get you depressed because you don't do everything you want to do, but you do what others wanna do because you want to fit in. Some kids who come from a different country will do stuff they don't want to do just to fit in, because they don't want to be lonely. Some people are afraid they won't be happy because they would lose a lot of friends for being themselves, but they have to know that those people should be there for them and support them for what he's doing since it's his friends.\n",
            "\n",
            "Lastly, Respecting others will gain them to respect you. Some teenagers like to make fun of others, not listen to what they have to say, and they like to beat kids up. Also some kids don't like to not listen to their parents, and that is also disrespectful. The bible says \" Honor your father and mother.\"\n",
            "\n",
            "If you respect your parents and listen to them, then they would trust you and not ask you lots of questions. Respecting other should be really important, because you don't want others to not respect you. Also, Some people are disrespectful because they were never respected. Important things to do to be respectful is, Listen to what the other person has to say, Stand up for them, Being friends with them Doing all of that will gain you respect and you would have good friends. Also, if you see someone bullying a kid don't watch, and stand up for them, don't be afraid to stand up for someone because you're doing the right thing.\n",
            "\n",
            "Some may disagree with me and say that it's not that big of a deal, and it doesn't feel like a big accomplishment. But with all do respect, I agree that it's a big accomplishment because nowadays it's really hard to become yourself.\n",
            "\n",
            "In conclusion, It's hard to become yourself in this world, because others are holding that opportunity back from you and you're afraid to fight it back. That's why I agree that to be yourself in a world that is constantly trying to make you something else is the greatest accomplishment. \n",
            "Small act of kindness can impact in other people can change people to become better persons you can have an impact of kindess with a homeles that can change his life or with some who needed they are going to know you are a nice person if you are a nice person everywhere you go people is going to like your personality so you have to be a nice person with others like a old women triying to cross the road thats a impact of kindness when you do that you feel a greate person you can change people in the way they think by helping others treating nice other people give them some advice when you see someone need it help someone older then you give food and new clothes to a homeless person is a big act of kindess maybe you wount change his life but you would change one day of his life when you do thoose stuffs you feel emotional.\n",
            "\n",
            "Another example of an act of kindess is when you help your friends to study for a test or explain them or when they need money you give some money is an act of kindess because you are helping your friend.\n",
            "\n",
            "Another example is not trowing trash on the floor if you put the trash where belong that is an act of kindess because benefits the earth and the people.                                                                                                                                      \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [print(train[\"full_text\"][i]) for i in range(-5,-1,-1)]\n",
        "train[\"full_text\"][train.shape[1]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "q_OBhpdB9D4L",
        "outputId": "cecfc8b5-8667-4119-ef70-06201544924a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"positive attitude is the key to success. I agree because you can do anything as long as you put your mind and soul into it and motivation you can accomplish it. Then so by doing it you feel good about yourself you'll feel unstable. But do what brings the best in you, what makes you yourself.\\n\\nOne way that importance of attitude is key to success is it motivates you to keep going forward. For example when you come across some difficulties you wont feel discouraged because your the limit and none other then you can or will change it. But the more discouraged you tend to feel at that moment you would wanna overcome it even if it makes whatever just as long as your mind is set to positive attitude you will accomplish it. Then so always remember what you put your mind to such as determination, positive attitude, willing you can its possible you will reach your goals. However you will with a positive mindset have a strong determination and you will build up self confidence in yourself.\\n\\nThen another reason is to grow individually with hard work. Positive attitude isen't quite the easiest for some people its really difficult for others to learn how to or even if they are willing to. But others tend to not always have positive attitude in anything they're just negative and havent tried to give their all they are stuck upon what others think. Then about what really is,you just have to not listen to what anyone thinks and focus on what you can accomplish because if you dont you will let them reflect on you but you dont want that for yourself. Then there's always going to be that one negative person to try to bring you down its up to you if you let it effect you. Just ignore them if you dont you will bet it affect you and you dont wanna fall back just because of others. With that being said in order to move forward you have to go though things individually but thats were you find yourself again and makes you stronger then before what your weaknesses were there not anymore because when facing things individually you blossom to a beatifull flower with of course hard work and it isen't in vane because hard work ends up having a good result for positive attitude.\\n\\nNext reason is to build selfcondifence. When having a positive mindset and most impotently willing we then build selfcondience our mind is a battlefield and we control it and its not about the selfcondicence its more about determination ourselves to it like we can or we will do it. After that its all depending on us weather we want to or not. Then after that the more we practice being positive in our battle fild the more selfcondidence we will have.\\n\\nSo then i hardly agree on positive attitude can guide you to succeed in life its all about three things determination willing and believing with those in mind you can tell all the negitive thoughts to go flying out the window because it has no door to let it in anymore. I truly incuage you to try it out if you want to see a big improvement to succeed in your life. Thank you for hearing me weather I agreed or disagreed,looking to hear back from you soon.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# any pattern for those who are high scorers in all of them?\n",
        "# [\"cohesion\",\"syntax\",\"vocabulary\", \"phraseology\",\"grammar\", \"conventions\"]\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "# high scorer for one is high scorer for the rest.\n",
        "train[(train[\"cohesion\"]==5) & (train[\"syntax\"]==5) & (train[\"vocabulary\"]==5) & (train[\"phraseology\"]==5) & (train[\"grammar\"]==5) & (train[\"conventions\"]==5)]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "xfitgYXHib4D",
        "outputId": "26227859-99d5-4c4a-b0bc-d92c1e472649"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           text_id  \\\n",
              "2389  B1AFACE6704E   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   full_text  \\\n",
              "2389  I agree with Michelangelo's statement as I have found through experience that it benefits me more to set high expectations and not reach my goal, rather than settling on a low goal and achieving it. When setting high goals, I find that I learn more and progress my abilties further than I do with a lower goal. This is because setting high goals requires confidence, challenges, and pride.\\n\\nFirstly, hard work is required to achieve high goals, and requires confidence. Confidence involves having trust and believing that you are capable of accomplishing something. I have found that when I set high goals, I feel more confident in my abilities. With lower goals however, my confidence is lower as I settle for the easier path and do not trust myself to do better. With a higher aim, my confidence motivates me to not give up. I tend to try harder, and always believe in myself. For example, at school, I had to choose whether I wanted to try out for the varsity tennis team or remain in the club team. The varsity team was a higher reach, and required confidence in my abilities for me to try out. The club team was a lower reach, as I knew that I could simply continue with it. I decided to try out for the varsity team, and was confident in my abilities. I practiced hard and did not give up. Despite not making it onto the team, I found that I actually greatly improved my abilities through the confidence I had gained, and tried harder than I had ever before. I managed to progress my skills, and was able to play more confidently on the club team and excel. Therefore, setting high aims requires confidence, which is a very beneficial characteristic to have in life.\\n\\nSecondly, setting a high goal indicates a challenge. It will be more challenging to attempt to aim high rather than aiming low, yet this will also prove more benefits. With a challenge, I tend to be more motivated as I want to prove my capabilites. I try harder, and in return get further. Even if I fall short of my high aim, I tend to get further than I would have if I had settled with a lower aim. With lower goals, there is no challenge, and so once the aim is achieved any effort stops. Learning is limited to that low expectation, and will not be able to go beyond. For example, in my high school career, I have always struggled with picking between two types of courses: honors or academic. Honors is the more challenging course, which requires more work and effort and would pose as more of a challenge. Academic however, requires the bare minimum and is easy to succeed in. In the honors course, I would be setting my aim high yet might not get the grade I want; yet, in the academic course, my aim would be low and I would most likely get the grade I want. After hard consideration, I decided to opt for the honors course. This course may be a challenge, yet would allow me to learn more. I enjoyed the challenge, and found that it motivated me to try harder to try and accomplish my goal. Even if I fell short of my ideal grade, I was proud to have tried my best and felt that I learnt so much more. The course benefited me more than the academic course would have, despite not receiving the grade I hoped for. Therefore, challenging oneself with a high aim allows for more flourishing and encourages more learning. Overall, setting a high aim will pose more benefits.\\n\\nThirdly, pride accompanies setting high goals. When setting low aims, I feel that it is easy to achieve the goal. However, with high aims, it is more challenging to achieve the goal, and so I feel proud of myself for attempting it. Pride is a very important characteristic, as it increases self esteem. With a higher self-esteem, I tend to try harder and feel more powerful with my abilities. I am more likely to learn and improve upon my abilities, which overall allows me to succeed. For example, in high school I had the opportunity to write a poem and submit it into a writing competition. This was a high aim, as the goal was to win. I felt very proud of myself for taking the time to write and challenge myself with this competition, and ended up writing my favorite piece yet. I submitted my writing, and did not win. However, I felt very proud of myself for having tried so hard and was able to learn and flourish in my writing abilities. Now, I am more likely to sign up for competitions, even if I do not win, simply because I enjoy the challenge and feel that I am able to learn. I feel proud of myself for taking on these challenges. Therefore, setting high goals emmits pride, which can prove very beneficial in life.\\n\\nIn conclusion, I agree with Michelangelo's statement. Setting high aims and falling short can bring so many more positive effects than setting low aims and accomplishing them. This is because setting high aims requires confidence, challenges, and pride. These characteristics are very important in succeeding, and can encourage learning and progression in abilities. Through setting low goals, learning becomes stagnant. In order to flourish and continue learning, setting high goals is crucial, even when not achieving the goal.    \n",
              "\n",
              "      cohesion  syntax  vocabulary  phraseology  grammar  conventions  \n",
              "2389       5.0     5.0         5.0          5.0      5.0          5.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a3cdc28-2120-4c81-afde-4ac243b7752a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>full_text</th>\n",
              "      <th>cohesion</th>\n",
              "      <th>syntax</th>\n",
              "      <th>vocabulary</th>\n",
              "      <th>phraseology</th>\n",
              "      <th>grammar</th>\n",
              "      <th>conventions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2389</th>\n",
              "      <td>B1AFACE6704E</td>\n",
              "      <td>I agree with Michelangelo's statement as I have found through experience that it benefits me more to set high expectations and not reach my goal, rather than settling on a low goal and achieving it. When setting high goals, I find that I learn more and progress my abilties further than I do with a lower goal. This is because setting high goals requires confidence, challenges, and pride.\\n\\nFirstly, hard work is required to achieve high goals, and requires confidence. Confidence involves having trust and believing that you are capable of accomplishing something. I have found that when I set high goals, I feel more confident in my abilities. With lower goals however, my confidence is lower as I settle for the easier path and do not trust myself to do better. With a higher aim, my confidence motivates me to not give up. I tend to try harder, and always believe in myself. For example, at school, I had to choose whether I wanted to try out for the varsity tennis team or remain in the club team. The varsity team was a higher reach, and required confidence in my abilities for me to try out. The club team was a lower reach, as I knew that I could simply continue with it. I decided to try out for the varsity team, and was confident in my abilities. I practiced hard and did not give up. Despite not making it onto the team, I found that I actually greatly improved my abilities through the confidence I had gained, and tried harder than I had ever before. I managed to progress my skills, and was able to play more confidently on the club team and excel. Therefore, setting high aims requires confidence, which is a very beneficial characteristic to have in life.\\n\\nSecondly, setting a high goal indicates a challenge. It will be more challenging to attempt to aim high rather than aiming low, yet this will also prove more benefits. With a challenge, I tend to be more motivated as I want to prove my capabilites. I try harder, and in return get further. Even if I fall short of my high aim, I tend to get further than I would have if I had settled with a lower aim. With lower goals, there is no challenge, and so once the aim is achieved any effort stops. Learning is limited to that low expectation, and will not be able to go beyond. For example, in my high school career, I have always struggled with picking between two types of courses: honors or academic. Honors is the more challenging course, which requires more work and effort and would pose as more of a challenge. Academic however, requires the bare minimum and is easy to succeed in. In the honors course, I would be setting my aim high yet might not get the grade I want; yet, in the academic course, my aim would be low and I would most likely get the grade I want. After hard consideration, I decided to opt for the honors course. This course may be a challenge, yet would allow me to learn more. I enjoyed the challenge, and found that it motivated me to try harder to try and accomplish my goal. Even if I fell short of my ideal grade, I was proud to have tried my best and felt that I learnt so much more. The course benefited me more than the academic course would have, despite not receiving the grade I hoped for. Therefore, challenging oneself with a high aim allows for more flourishing and encourages more learning. Overall, setting a high aim will pose more benefits.\\n\\nThirdly, pride accompanies setting high goals. When setting low aims, I feel that it is easy to achieve the goal. However, with high aims, it is more challenging to achieve the goal, and so I feel proud of myself for attempting it. Pride is a very important characteristic, as it increases self esteem. With a higher self-esteem, I tend to try harder and feel more powerful with my abilities. I am more likely to learn and improve upon my abilities, which overall allows me to succeed. For example, in high school I had the opportunity to write a poem and submit it into a writing competition. This was a high aim, as the goal was to win. I felt very proud of myself for taking the time to write and challenge myself with this competition, and ended up writing my favorite piece yet. I submitted my writing, and did not win. However, I felt very proud of myself for having tried so hard and was able to learn and flourish in my writing abilities. Now, I am more likely to sign up for competitions, even if I do not win, simply because I enjoy the challenge and feel that I am able to learn. I feel proud of myself for taking on these challenges. Therefore, setting high goals emmits pride, which can prove very beneficial in life.\\n\\nIn conclusion, I agree with Michelangelo's statement. Setting high aims and falling short can bring so many more positive effects than setting low aims and accomplishing them. This is because setting high aims requires confidence, challenges, and pride. These characteristics are very important in succeeding, and can encourage learning and progression in abilities. Through setting low goals, learning becomes stagnant. In order to flourish and continue learning, setting high goals is crucial, even when not achieving the goal.</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a3cdc28-2120-4c81-afde-4ac243b7752a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2a3cdc28-2120-4c81-afde-4ac243b7752a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2a3cdc28-2120-4c81-afde-4ac243b7752a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# any pattern for those who are low scorers in all of them?\n",
        "# [\"cohesion\",\"syntax\",\"vocabulary\", \"phraseology\",\"grammar\", \"conventions\"]\n",
        "\n",
        "# low scorer for one is low scorer for the rest.\n",
        "train[(train[\"cohesion\"]==1) & (train[\"syntax\"]==1) & (train[\"vocabulary\"]==1) & (train[\"phraseology\"]==1) & (train[\"grammar\"]==1) & (train[\"conventions\"]==1)]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "MEfXT6NfkUSH",
        "outputId": "88e35d1b-ef76-4726-fcbb-99e64ae0193a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           text_id  \\\n",
              "952   48EA282A4EAF   \n",
              "1540  767533E12569   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       full_text  \\\n",
              "952   some student offer distance learning as an option for student to attend classes from homr by wat of online pr video conferencing. i think student would benefit form being able to attend classesfrom home. you are authorized take the electronic version of this you will taking this promptsome student offer distance learning as an option for student to attend classes from homr by wat of online pr video conferencing. some student offer distance learning as an option for student to attend classes from homr by wat of online pr video conferencing. some student offer distance learning as an option.\\n\\nonline pr video conferencing. the right view the prompt and teh checklist for writers vvsome student offer distance learning as an option for student to attend classes from homr by wat of online pr video conferencing. some student offer distance learning as an option for student to attend classes from homr by wat of online pr video conferencing. some student offer distance learning as an option for student to attend classes from homr by wat of online pr video conferencing. some student offer distance learning as an option for student to attend classes from homr by wat of online pr video conferencing. some student offer distance learning as an option for student to attend classes from homr by wat of online pr video conferencing. some student offer distance learning as an option for student to attend classes from homr by wat of online pr video conferencing. some student offer distance learning as an option for student.   \n",
              "1540                                                                                                                                                                                                                                                    he is a good because they are the prescient and us. now the are more procession a other prescient and us. because and the more many education for student and school. the more school for one because he have a one women the have many education for a other because the good more the one is information for access because is not like the brazen giant of Greek fame with conquering one nation for is the imprison the eyes command the one book for people the us give more your lied your huddled Farmer with silent lips the more the one is for you because the are a one solution for you and a other people he have a more the one solution for you here ancient lands your storied pound Aries she with silent lips pledge of a in order people he have a education\\n\\nDo we accomplish more the if we are always doing something or does inactivity also serve a purpose take a position on this for you and specific examples. take a position for you because the are a never much may be a always how many I planned and my people he have the on the right to ported and the after type your response in the space. than new colossus for the more have a one they solution for is came one o two people the same time. because the more that in and here name norther of Exiles form her barmen the are one sous ion.    \n",
              "\n",
              "      cohesion  syntax  vocabulary  phraseology  grammar  conventions  \n",
              "952        1.0     1.0         1.0          1.0      1.0          1.0  \n",
              "1540       1.0     1.0         1.0          1.0      1.0          1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-083006c3-7eb6-4819-8cf1-3e891066a7b2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>full_text</th>\n",
              "      <th>cohesion</th>\n",
              "      <th>syntax</th>\n",
              "      <th>vocabulary</th>\n",
              "      <th>phraseology</th>\n",
              "      <th>grammar</th>\n",
              "      <th>conventions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>952</th>\n",
              "      <td>48EA282A4EAF</td>\n",
              "      <td>some student offer distance learning as an option for student to attend classes from homr by wat of online pr video conferencing. i think student would benefit form being able to attend classesfrom home. you are authorized take the electronic version of this you will taking this promptsome student offer distance learning as an option for student to attend classes from homr by wat of online pr video conferencing. some student offer distance learning as an option for student to attend classes from homr by wat of online pr video conferencing. some student offer distance learning as an option.\\n\\nonline pr video conferencing. the right view the prompt and teh checklist for writers vvsome student offer distance learning as an option for student to attend classes from homr by wat of online pr video conferencing. some student offer distance learning as an option for student to attend classes from homr by wat of online pr video conferencing. some student offer distance learning as an option for student to attend classes from homr by wat of online pr video conferencing. some student offer distance learning as an option for student to attend classes from homr by wat of online pr video conferencing. some student offer distance learning as an option for student to attend classes from homr by wat of online pr video conferencing. some student offer distance learning as an option for student to attend classes from homr by wat of online pr video conferencing. some student offer distance learning as an option for student.</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1540</th>\n",
              "      <td>767533E12569</td>\n",
              "      <td>he is a good because they are the prescient and us. now the are more procession a other prescient and us. because and the more many education for student and school. the more school for one because he have a one women the have many education for a other because the good more the one is information for access because is not like the brazen giant of Greek fame with conquering one nation for is the imprison the eyes command the one book for people the us give more your lied your huddled Farmer with silent lips the more the one is for you because the are a one solution for you and a other people he have a more the one solution for you here ancient lands your storied pound Aries she with silent lips pledge of a in order people he have a education\\n\\nDo we accomplish more the if we are always doing something or does inactivity also serve a purpose take a position on this for you and specific examples. take a position for you because the are a never much may be a always how many I planned and my people he have the on the right to ported and the after type your response in the space. than new colossus for the more have a one they solution for is came one o two people the same time. because the more that in and here name norther of Exiles form her barmen the are one sous ion.</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-083006c3-7eb6-4819-8cf1-3e891066a7b2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-083006c3-7eb6-4819-8cf1-3e891066a7b2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-083006c3-7eb6-4819-8cf1-3e891066a7b2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "* test.csv only had 3 items. just discard and split by train.csv\n",
        "* high correlation, so high scorer in one is high scorer in rest and vice versa\n",
        "* hard to be any extreme\n",
        "* high scorers also tend to write more\n",
        "* low scorers have a lot of misspellings, suggesting a model with more casual language might be more appropriate\n",
        "* there are paragraphs so we need to take note of line separations\n"
      ],
      "metadata": {
        "id": "pxgzZ5_yorIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RetriBERT"
      ],
      "metadata": {
        "id": "wzB82PhPxEqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RetriBERT tokenizer\n",
        "# class transformers.RetriBertTokenizer\n"
      ],
      "metadata": {
        "id": "vLBV7TnUxCwv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install ktrain"
      ],
      "metadata": {
        "id": "Hr_JUf8J7jyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d016af1-e772-4ec1-9316-ad5f4d4e6c3b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ktrain in /usr/local/lib/python3.9/dist-packages (0.33.4)\n",
            "Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.9/dist-packages (from ktrain) (1.0.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from ktrain) (1.1.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from ktrain) (0.1.97)\n",
            "Requirement already satisfied: keras-bert>=0.86.0 in /usr/local/lib/python3.9/dist-packages (from ktrain) (0.89.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from ktrain) (2.27.1)\n",
            "Requirement already satisfied: cchardet in /usr/local/lib/python3.9/dist-packages (from ktrain) (2.1.7)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.9/dist-packages (from ktrain) (1.0.9)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from ktrain) (3.7.1)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from ktrain) (1.4.4)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.9/dist-packages (from ktrain) (4.0.0)\n",
            "Requirement already satisfied: transformers>=4.17.0 in /usr/local/lib/python3.9/dist-packages (from ktrain) (4.27.3)\n",
            "Requirement already satisfied: whoosh in /usr/local/lib/python3.9/dist-packages (from ktrain) (2.7.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from ktrain) (1.2.2)\n",
            "Requirement already satisfied: syntok>1.3.3 in /usr/local/lib/python3.9/dist-packages (from ktrain) (1.4.4)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.9/dist-packages (from ktrain) (0.42.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from ktrain) (23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from keras-bert>=0.86.0->ktrain) (1.22.4)\n",
            "Requirement already satisfied: keras-transformer==0.40.0 in /usr/local/lib/python3.9/dist-packages (from keras-bert>=0.86.0->ktrain) (0.40.0)\n",
            "Requirement already satisfied: keras-position-wise-feed-forward==0.8.0 in /usr/local/lib/python3.9/dist-packages (from keras-transformer==0.40.0->keras-bert>=0.86.0->ktrain) (0.8.0)\n",
            "Requirement already satisfied: keras-layer-normalization==0.16.0 in /usr/local/lib/python3.9/dist-packages (from keras-transformer==0.40.0->keras-bert>=0.86.0->ktrain) (0.16.0)\n",
            "Requirement already satisfied: keras-multi-head==0.29.0 in /usr/local/lib/python3.9/dist-packages (from keras-transformer==0.40.0->keras-bert>=0.86.0->ktrain) (0.29.0)\n",
            "Requirement already satisfied: keras-embed-sim==0.10.0 in /usr/local/lib/python3.9/dist-packages (from keras-transformer==0.40.0->keras-bert>=0.86.0->ktrain) (0.10.0)\n",
            "Requirement already satisfied: keras-pos-embd==0.13.0 in /usr/local/lib/python3.9/dist-packages (from keras-transformer==0.40.0->keras-bert>=0.86.0->ktrain) (0.13.0)\n",
            "Requirement already satisfied: keras-self-attention==0.51.0 in /usr/local/lib/python3.9/dist-packages (from keras-multi-head==0.29.0->keras-transformer==0.40.0->keras-bert>=0.86.0->ktrain) (0.51.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.0->ktrain) (3.0.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.0->ktrain) (1.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.0->ktrain) (5.12.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.0->ktrain) (8.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.0->ktrain) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.0->ktrain) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.0->ktrain) (4.39.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.1->ktrain) (2022.7.1)\n",
            "Requirement already satisfied: regex>2016 in /usr/local/lib/python3.9/dist-packages (from syntok>1.3.3->ktrain) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.17.0->ktrain) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.17.0->ktrain) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers>=4.17.0->ktrain) (3.10.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.17.0->ktrain) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.17.0->ktrain) (4.65.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from langdetect->ktrain) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->ktrain) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->ktrain) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->ktrain) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->ktrain) (1.26.15)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->ktrain) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->ktrain) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers>=4.17.0->ktrain) (4.5.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.0.0->ktrain) (3.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import Optional\n",
        "import transformers\n",
        "from ktrain import text\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from torch import nn\n",
        "# from transformers import TFAutoModel, AutoTokenizer\n",
        "\n",
        "# from ...modeling_utils import PreTrainedModel\n",
        "# from ...utils import add_start_docstrings, logging\n",
        "# from ..bert.modeling_bert import BertModel\n",
        "# from .configuration_retribert import RetriBertConfig"
      ],
      "metadata": {
        "id": "LC-BH85qy3lg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_checkpoint = 'yjernite/retribert-base-uncased'\n",
        "# model = TFAutoModel.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "tUeVSs2z7Fhp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # RetriBERT model\n",
        "# # class transformers.RetriBertModel\n",
        "# # coding=utf-8\n",
        "# # Copyright 2019-present, the HuggingFace Inc. team, The Google AI Language Team and Facebook, Inc.\n",
        "# #\n",
        "# # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# # you may not use this file except in compliance with the License.\n",
        "# # You may obtain a copy of the License at\n",
        "# #\n",
        "# #     http://www.apache.org/licenses/LICENSE-2.0\n",
        "# #\n",
        "# # Unless required by applicable law or agreed to in writing, software\n",
        "# # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# # See the License for the specific language governing permissions and\n",
        "# # limitations under the License.\n",
        "# \"\"\"\n",
        "# RetriBERT model\n",
        "# \"\"\"\n",
        "\n",
        "# logger = transformers.logging.get_logger(__name__)\n",
        "\n",
        "# RETRIBERT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "#     \"yjernite/retribert-base-uncased\",\n",
        "#     # See all RetriBert models at https://huggingface.co/models?filter=retribert\n",
        "# ]\n",
        "\n",
        "\n",
        "# # INTERFACE FOR ENCODER AND TASK SPECIFIC MODEL #\n",
        "# class RetriBertPreTrainedModel(PreTrainedModel):\n",
        "#     \"\"\"\n",
        "#     An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
        "#     models.\n",
        "#     \"\"\"\n",
        "\n",
        "#     config_class = RetriBertConfig\n",
        "#     load_tf_weights = None\n",
        "#     base_model_prefix = \"retribert\"\n",
        "\n",
        "#     def _init_weights(self, module):\n",
        "#         \"\"\"Initialize the weights\"\"\"\n",
        "#         if isinstance(module, nn.Linear):\n",
        "#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "#             if module.bias is not None:\n",
        "#                 module.bias.data.zero_()\n",
        "#         elif isinstance(module, nn.Embedding):\n",
        "#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "#             if module.padding_idx is not None:\n",
        "#                 module.weight.data[module.padding_idx].zero_()\n",
        "#         elif isinstance(module, nn.LayerNorm):\n",
        "#             module.bias.data.zero_()\n",
        "#             module.weight.data.fill_(1.0)\n",
        "\n",
        "\n",
        "# RETRIBERT_START_DOCSTRING = r\"\"\"\n",
        "#     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
        "#     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
        "#     etc.)\n",
        "#     This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
        "#     Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
        "#     and behavior.\n",
        "#     Parameters:\n",
        "#         config ([`RetriBertConfig`]): Model configuration class with all the parameters of the model.\n",
        "#             Initializing with a config file does not load the weights associated with the model, only the\n",
        "#             configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "# @add_start_docstrings(\n",
        "#     \"\"\"Bert Based model to embed queries or document for document retrieval.\"\"\",\n",
        "#     RETRIBERT_START_DOCSTRING,\n",
        "# )\n",
        "# class RetriBertModel(RetriBertPreTrainedModel):\n",
        "#     def __init__(self, config: RetriBertConfig) -> None:\n",
        "#         super().__init__(config)\n",
        "#         self.projection_dim = config.projection_dim\n",
        "\n",
        "#         self.bert_query = BertModel(config)\n",
        "#         self.bert_doc = None if config.share_encoders else BertModel(config)\n",
        "#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "#         self.project_query = nn.Linear(config.hidden_size, config.projection_dim, bias=False)\n",
        "#         self.project_doc = nn.Linear(config.hidden_size, config.projection_dim, bias=False)\n",
        "\n",
        "#         self.ce_loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "#         # Initialize weights and apply final processing\n",
        "#         self.post_init()\n",
        "\n",
        "#     def embed_sentences_checkpointed(\n",
        "#         self,\n",
        "#         input_ids,\n",
        "#         attention_mask,\n",
        "#         sent_encoder,\n",
        "#         checkpoint_batch_size=-1,\n",
        "#     ):\n",
        "#         # reproduces BERT forward pass with checkpointing\n",
        "#         if checkpoint_batch_size < 0 or input_ids.shape[0] < checkpoint_batch_size:\n",
        "#             return sent_encoder(input_ids, attention_mask=attention_mask)[1]\n",
        "#         else:\n",
        "#             # prepare implicit variables\n",
        "#             device = input_ids.device\n",
        "#             input_shape = input_ids.size()\n",
        "#             token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "#             head_mask = [None] * sent_encoder.config.num_hidden_layers\n",
        "#             extended_attention_mask: torch.Tensor = sent_encoder.get_extended_attention_mask(\n",
        "#                 attention_mask, input_shape\n",
        "#             )\n",
        "\n",
        "#             # define function for checkpointing\n",
        "#             def partial_encode(*inputs):\n",
        "#                 encoder_outputs = sent_encoder.encoder(\n",
        "#                     inputs[0],\n",
        "#                     attention_mask=inputs[1],\n",
        "#                     head_mask=head_mask,\n",
        "#                 )\n",
        "#                 sequence_output = encoder_outputs[0]\n",
        "#                 pooled_output = sent_encoder.pooler(sequence_output)\n",
        "#                 return pooled_output\n",
        "\n",
        "#             # run embedding layer on everything at once\n",
        "#             embedding_output = sent_encoder.embeddings(\n",
        "#                 input_ids=input_ids, position_ids=None, token_type_ids=token_type_ids, inputs_embeds=None\n",
        "#             )\n",
        "#             # run encoding and pooling on one mini-batch at a time\n",
        "#             pooled_output_list = []\n",
        "#             for b in range(math.ceil(input_ids.shape[0] / checkpoint_batch_size)):\n",
        "#                 b_embedding_output = embedding_output[b * checkpoint_batch_size : (b + 1) * checkpoint_batch_size]\n",
        "#                 b_attention_mask = extended_attention_mask[b * checkpoint_batch_size : (b + 1) * checkpoint_batch_size]\n",
        "#                 pooled_output = checkpoint.checkpoint(partial_encode, b_embedding_output, b_attention_mask)\n",
        "#                 pooled_output_list.append(pooled_output)\n",
        "#             return torch.cat(pooled_output_list, dim=0)\n",
        "\n",
        "#     def embed_questions(\n",
        "#         self,\n",
        "#         input_ids,\n",
        "#         attention_mask=None,\n",
        "#         checkpoint_batch_size=-1,\n",
        "#     ):\n",
        "#         q_reps = self.embed_sentences_checkpointed(\n",
        "#             input_ids,\n",
        "#             attention_mask,\n",
        "#             self.bert_query,\n",
        "#             checkpoint_batch_size,\n",
        "#         )\n",
        "#         return self.project_query(q_reps)\n",
        "\n",
        "#     def embed_answers(\n",
        "#         self,\n",
        "#         input_ids,\n",
        "#         attention_mask=None,\n",
        "#         checkpoint_batch_size=-1,\n",
        "#     ):\n",
        "#         a_reps = self.embed_sentences_checkpointed(\n",
        "#             input_ids,\n",
        "#             attention_mask,\n",
        "#             self.bert_query if self.bert_doc is None else self.bert_doc,\n",
        "#             checkpoint_batch_size,\n",
        "#         )\n",
        "#         return self.project_doc(a_reps)\n",
        "\n",
        "#     def forward(\n",
        "#         self,\n",
        "#         input_ids_query: torch.LongTensor,\n",
        "#         attention_mask_query: Optional[torch.FloatTensor],\n",
        "#         input_ids_doc: torch.LongTensor,\n",
        "#         attention_mask_doc: Optional[torch.FloatTensor],\n",
        "#         checkpoint_batch_size: int = -1,\n",
        "#     ) -> torch.FloatTensor:\n",
        "#         r\"\"\"\n",
        "#         Args:\n",
        "#             input_ids_query (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "#                 Indices of input sequence tokens in the vocabulary for the queries in a batch.\n",
        "#                 Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "#                 [`PreTrainedTokenizer.__call__`] for details.\n",
        "#                 [What are input IDs?](../glossary#input-ids)\n",
        "#             attention_mask_query (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "#                 Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "#                 - 1 for tokens that are **not masked**,\n",
        "#                 - 0 for tokens that are **masked**.\n",
        "#                 [What are attention masks?](../glossary#attention-mask)\n",
        "#             input_ids_doc (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "#                 Indices of input sequence tokens in the vocabulary for the documents in a batch.\n",
        "#             attention_mask_doc (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "#                 Mask to avoid performing attention on documents padding token indices.\n",
        "#             checkpoint_batch_size (`int`, *optional*, defaults to `-1`):\n",
        "#                 If greater than 0, uses gradient checkpointing to only compute sequence representation on\n",
        "#                 `checkpoint_batch_size` examples at a time on the GPU. All query representations are still compared to\n",
        "#                 all document representations in the batch.\n",
        "#         Return:\n",
        "#             `torch.FloatTensor``: The bidirectional cross-entropy loss obtained while trying to match each query to its\n",
        "#             corresponding document and each document to its corresponding query in the batch\n",
        "#         \"\"\"\n",
        "#         device = input_ids_query.device\n",
        "#         q_reps = self.embed_questions(input_ids_query, attention_mask_query, checkpoint_batch_size)\n",
        "#         a_reps = self.embed_answers(input_ids_doc, attention_mask_doc, checkpoint_batch_size)\n",
        "#         compare_scores = torch.mm(q_reps, a_reps.t())\n",
        "#         loss_qa = self.ce_loss(compare_scores, torch.arange(compare_scores.shape[1]).to(device))\n",
        "#         loss_aq = self.ce_loss(compare_scores.t(), torch.arange(compare_scores.shape[0]).to(device))\n",
        "#         loss = (loss_qa + loss_aq) / 2\n",
        "#         return loss"
      ],
      "metadata": {
        "id": "oiF9WJjLxtfa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retribert_model = transformers.TFAutoModel.from_pretrained(\"yjernite/retribert-base-uncased\")\n",
        "# text.Transformer(\"yjernite/retribert-base-uncased/\")\n",
        "# transformers.AutoConfig.from_pretrained(\"yjernite/retribert-base-uncased/\")\n",
        "\n",
        "# retriconfig = transformers.RetriBertConfig(vocab_size=30522, \n",
        "#                              hidden_size=768, \n",
        "#                              num_hidden_layers=8, \n",
        "#                              num_attention_heads=12, \n",
        "#                              intermediate_size=3072, \n",
        "#                              hidden_act='gelu', \n",
        "#                              hidden_dropout_prob=0.1, \n",
        "#                              attention_probs_dropout_prob=0.1, \n",
        "#                              max_position_embeddings=512, \n",
        "#                              type_vocab_size=2, \n",
        "#                              initializer_range=0.02, \n",
        "#                              layer_norm_eps=1e-12, \n",
        "#                              share_encoders=True, \n",
        "#                              projection_dim=128, \n",
        "#                              pad_token_id=0)\n",
        "# # retribert_model = transformers.TFAutoModel.from_pretrained(retriconfig)\n",
        "# transformers.AutoConfig.from_pretrained(retriconfig) # tried using the config function\n",
        "\n",
        "retribert_model_checkpoints = \"yjernite/retribert-base-uncased\"\n",
        "retribert_model = transformers.BertModel.from_pretrained(retribert_model_checkpoints)\n",
        "# retribert_model\n",
        "# retribert_tokenizer = transformers.BertTokenizer.from_pretrained(retribert_model_checkpoints)\n",
        "retribert_tokenizer = transformers.AutoTokenizer.from_pretrained(retribert_model_checkpoints, use_fast=False, normalization = True)"
      ],
      "metadata": {
        "id": "saSRFRT_Vvqy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retri_cuda = retribert_model.cuda()\n",
        "retri_cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osrK4zpiryVZ",
        "outputId": "60ea0eea-7a61-4e9d-b901-1e6e6500188a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_train_df = train\n",
        "input_test_df = test\n",
        "label_cols = input_train_df.columns[2:]\n",
        "orig_train_df = copy.deepcopy(input_train_df)\n",
        "orig_train_df.head()\n",
        "     \n",
        "shuffle = np.random.permutation(np.arange(orig_train_df.shape[0]))\n",
        "orig_train_df = orig_train_df.iloc[shuffle]\n",
        "split=(0.8,0.1,0.1)\n",
        "splits = np.multiply(len(orig_train_df), split).astype(int)\n",
        "df_train, df_val, df_test = np.split(orig_train_df, [splits[0], splits[0] + splits[1]])\n",
        "\n",
        "X_train, X_val, X_test = df_train['full_text'], df_val['full_text'], df_test['full_text']\n",
        "y_train, y_val, y_test = np.array(df_train[label_cols]), np.array(df_val[label_cols]), np.array(df_test[label_cols])"
      ],
      "metadata": {
        "id": "QG6hNNysrmyU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_config_param(seed = 99):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    tf.keras.backend.clear_session()\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/Kaggle\"\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    \n",
        "    \n",
        "# set_config_param(20230214)\n",
        "set_config_param()"
      ],
      "metadata": {
        "id": "j20twVVktlZY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 512\n",
        "epochs = 5\n",
        "batch_size = 8\n",
        "dropout = .1\n",
        "learning_rate = .00005\n",
        "number_of_hidden_layer = 1\n",
        "hidden_layer_node_count = 256\n",
        "trainable_flag = False\n",
        "retrain_layer_count = 0"
      ],
      "metadata": {
        "id": "k6dKVou5uMo5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MCRMSE(y_true, y_pred):\n",
        "    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n",
        "    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=-1, keepdims=True)\n",
        "\n",
        "def plot_loss_accuracy(history, col_list):\n",
        "    fig, ax = plt.subplots(2, 6, figsize=(16, 6), sharex='col', sharey='row')\n",
        "    fig.tight_layout(pad=5.0)\n",
        "    for idx, col in enumerate(col_list):\n",
        "\n",
        "        ax[0, idx].plot(history[col + '_loss'], lw=2, color='darkgoldenrod')\n",
        "        ax[0, idx].plot(history['val_' + col + '_loss'], lw=2, color='indianred')\n",
        "        #ax[0, idx].legend(loc='center left')\n",
        "        ax[0, idx].legend(['Train', 'Validation'], fontsize=5)\n",
        "        ax[0, idx].set_xlabel('Epochs', size=10)\n",
        "        ax[0, idx].set_title('Loss: ' + col)\n",
        "\n",
        "        ax[1, idx].plot(history[col + '_accuracy'], lw=2, color='darkgoldenrod')\n",
        "        ax[1, idx].plot(history['val_' + col + '_accuracy'], lw=2, color='indianred')\n",
        "        #ax[0, idx].legend(loc='center left')\n",
        "        ax[1, idx].legend(['Train', 'Validation'], fontsize=5)\n",
        "        ax[1, idx].set_xlabel('Epochs', size=10)\n",
        "        ax[1, idx].set_title('Accuracy: ' + col)\n",
        "\n",
        "def encode_text(text, tokenizer):\n",
        "    \n",
        "    encoded = tokenizer.batch_encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors=\"tf\",\n",
        "    )\n",
        "\n",
        "    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_masks\": attention_masks\n",
        "    }"
      ],
      "metadata": {
        "id": "DLosOxRss8B2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = encode_text(df_train['full_text'].tolist(), retribert_tokenizer)\n",
        "val_encodings = encode_text(df_val['full_text'].tolist(), retribert_tokenizer)\n",
        "test_encodings = encode_text(df_test['full_text'].tolist(), retribert_tokenizer)"
      ],
      "metadata": {
        "id": "XKTyGEb7wRwo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# double check encodings and length=512"
      ],
      "metadata": {
        "id": "gwu_HuGqzvCG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### from BERT TF\n",
        "# def create_retribert_classification_model(retribert_model,\n",
        "#                                           num_train_layers=0,\n",
        "#                                           hidden_size = 200,\n",
        "#                                           dropout=0.3,\n",
        "#                                           learning_rate=0.00005):\n",
        "#     \"\"\"\n",
        "#     Build a simple classification model with BERT. Use the CLS Output for classification purposes\n",
        "#     \"\"\"\n",
        "#     if num_train_layers == 0:\n",
        "#         # Freeze all layers of pre-trained BERT model\n",
        "#         retribert_model.trainable = False\n",
        "\n",
        "#     elif num_train_layers == 12: ##### NUMBER OF RETRIBERT MODELS. NEED TO UPDATE.\n",
        "#         # Train all layers of the BERT model\n",
        "#         retribert_model.trainable = True\n",
        "\n",
        "#     else:\n",
        "#         # Restrict training to the num_train_layers outer transformer layers\n",
        "#         retrain_layers = []\n",
        "\n",
        "#         for retrain_layer_number in range(num_train_layers):\n",
        "\n",
        "#             layer_code = '_' + str(11 - retrain_layer_number)\n",
        "#             retrain_layers.append(layer_code)\n",
        "          \n",
        "        \n",
        "#         print('retrain layers: ', retrain_layers)\n",
        "\n",
        "#         for w in retribert_model.weights:\n",
        "#             if not any([x in w.name for x in retrain_layers]):\n",
        "#                 #print('freezing: ', w)\n",
        "#                 w._trainable = False\n",
        "\n",
        "#     input_ids = tf.keras.layers.Input(shape=(MAX_LENGTH,), dtype=tf.int64, name='input_ids_layer')\n",
        "#     token_type_ids = tf.keras.layers.Input(shape=(MAX_LENGTH,), dtype=tf.int64, name='token_type_ids_layer')\n",
        "#     attention_mask = tf.keras.layers.Input(shape=(MAX_LENGTH,), dtype=tf.int64, name='attention_mask_layer')\n",
        "\n",
        "#     retribert_inputs = {'input_ids': input_ids,\n",
        "#                         'token_type_ids': token_type_ids,\n",
        "#                         'attention_mask': attention_mask}      \n",
        "\n",
        "#     retribert_out = retribert_model(retribert_inputs)\n",
        "\n",
        "#     cls_token = retribert_out[0][:, 0, :]\n",
        "\n",
        "#     hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(cls_token)\n",
        "\n",
        "\n",
        "#     hidden = tf.keras.layers.Dropout(dropout)(hidden)\n",
        "\n",
        "\n",
        "#     classification = tf.keras.layers.Dense(1, activation='sigmoid',name='classification_layer')(hidden)\n",
        "    \n",
        "#     classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[classification])\n",
        "    \n",
        "#     classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "#                                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \n",
        "#                                  metrics='accuracy')\n",
        "    \n",
        "#     return classification_model"
      ],
      "metadata": {
        "id": "Am8IcOsF0Y4s"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retribert_classification_model = create_retribert_classification_model(retribert_model, num_train_layers=0)\n",
        "# retribert_classification_model.summary()"
      ],
      "metadata": {
        "id": "Kiw3dT5U0rjI"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries/code\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "     \n"
      ],
      "metadata": {
        "id": "eXEi1T5WBmpv"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.utils.data import Dataset\n",
        "\n",
        "# class PandasDataset(Dataset):\n",
        "#     def __init__(self, dataframe):\n",
        "#         self.dataframe = dataframe\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.dataframe)\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         return self.dataframe.iloc[index]\n",
        "\n",
        "# from torch.utils.data import DataLoader\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# train, test = train_test_split(orig_train_df, test_size=0.2, random_state=0)\n",
        "# train_dataset = PandasDataset(train)\n",
        "# test_dataset = PandasDataset(test)\n",
        "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16)\n",
        "# test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# train_dataloader = torch.utils.data.DataLoader(X_train, batch_size=64, shuffle=True)\n",
        "# test_dataloader = torch.utils.data.DataLoader(X_test, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "6_CpwOxtEIQ0"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the manual seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "class_names = [1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0]\n",
        "# Get the length of class_names (one output unit for each class)\n",
        "output_shape = len(class_names)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n",
        "\n",
        "# Recreate the classifier layer and seed it to the target device\n",
        "retribert_model.classifier = torch.nn.Sequential(\n",
        "    torch.nn.Dropout(p=0.2, inplace=True), \n",
        "    torch.nn.Linear(in_features=1280, \n",
        "                    out_features=output_shape, # same number of output units as our number of classes\n",
        "                    bias=True)).to(device)\n",
        "\n",
        "\n",
        "# Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n",
        "for param in retribert_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# with torch.no_grad():\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(retribert_model.parameters(), lr=0.001)\n",
        "\n",
        "# retribert_model_results = engine.train(model=retribert_model,\n",
        "#                                        train_dataloader=train_dataloader,\n",
        "#                                        test_dataloader=test_dataloader,\n",
        "#                                        optimizer=optimizer,\n",
        "#                                        loss_fn=loss_fn,\n",
        "#                                        epochs=5,\n",
        "#                                        device=device)"
      ],
      "metadata": {
        "id": "um1mFaPz7shm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm"
      ],
      "metadata": {
        "id": "SsQiinBXmpPV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9TCblE5n5Ht",
        "outputId": "a8eae1d5-1013-4578-e655-4cf07e33d513"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': array([[ 101, 2493, 2323, ...,    0,    0,    0],\n",
              "        [ 101, 5674, 1037, ..., 2020, 2183,  102],\n",
              "        [ 101, 2035, 3633, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [ 101, 2031, 2017, ..., 2037, 2155,  102],\n",
              "        [ 101, 2045, 2024, ...,    0,    0,    0],\n",
              "        [ 101, 9454, 2003, ..., 1996, 2154,  102]], dtype=int32),\n",
              " 'attention_masks': array([[1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 1, 1, 1],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 1, 1, ..., 1, 1, 1],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 1, 1, 1]], dtype=int32)}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## copy of code above\n",
        "X_train, X_val, X_test = df_train['full_text'], df_val['full_text'], df_test['full_text']\n",
        "y_train, y_val, y_test = np.array(df_train[label_cols]), np.array(df_val[label_cols]), np.array(df_test[label_cols])\n",
        "\n",
        "#### from\n",
        "# https://stackoverflow.com/questions/55369821/how-to-train-a-neural-network-model-with-bert-embeddings-instead-of-static-embed\n",
        "retribert_tokenizer = transformers.BertTokenizer.from_pretrained(retribert_model_checkpoints)\n",
        "# retribert_tokenizer = transformers.AutoTokenizer.from_pretrained(retribert_model_checkpoints, use_fast=False, normalization = True)\n",
        "\n",
        "X_train = [retribert_tokenizer.tokenize('[CLS] ' + sent + ' [SEP]') for sent in X_train] \n",
        "X_train_tokens = [retribert_tokenizer.convert_tokens_to_ids(sent) for sent in X_train]\n",
        "# # X_train[0]\n",
        "# # X_train_tokens[0]\n",
        "X_test = [retribert_tokenizer.tokenize('[CLS] ' + sent + ' [SEP]') for sent in X_test] \n",
        "X_test_tokens = [retribert_tokenizer.convert_tokens_to_ids(sent) for sent in X_test]"
      ],
      "metadata": {
        "id": "O710Kp2ClQpP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### anothing thing\n",
        "### https://stackoverflow.com/questions/68115993/input-ids-torch-tensorinput-ids-valueerror-expected-sequence-of-length-133\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "# print(flaubert)\n",
        "for sent in X_train:\n",
        "    encoded_sent = retribert_tokenizer.encode_plus(sent, add_special_tokens=True, truncation=True, padding=True, return_attention_mask=True, return_tensors='pt', max_length=510)\n",
        "\n",
        "    # Add the outputs to the lists\n",
        "    # input_ids.append(encoded_sent.get('input_ids'))\n",
        "    # attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "\n",
        "# input_ids = torch.as_tensor(input_ids)\n",
        "# attention_masks = torch.as_tensor(attention_masks)\n",
        "\n",
        "hidden_state = retribert_model(input_ids=encoded_sent[\"input_ids\"].to(\"cuda\"), attention_mask=encoded_sent[\"attention_mask\"].to(\"cuda\"))\n",
        "hidden_state\n",
        "# # # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "last_hidden_state_cls = hidden_state[0][:, 0, :]"
      ],
      "metadata": {
        "id": "RlP4F2WSXDLd"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# results = torch.zeros((len(X_test_tokens), retri_cuda.config.hidden_size)).long()\n",
        "# results[0]\n",
        "# with torch.no_grad():\n",
        "#     for stidx in range(0, len(X_test_tokens), batch_size):\n",
        "#         X = X_test_tokens[stidx:stidx + batch_size]\n",
        "#         X = torch.LongTensor(X).cuda()\n",
        "#         embed, pooled_output = retri_cuda(X)\n",
        "#         results[stidx:stidx + batch_size,:] = embed.cpu()\n",
        "# torch.LongTensor(X)\n",
        "\n",
        "#### from https://machinelearningmastery.com/building-a-regression-model-in-pytorch/\n",
        "# X_train_torch = torch.tensor(X_train_tokens, dtype=torch.float32)\n",
        "# y_train_torch = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
        "# X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n",
        "# y_test_torch = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
        " \n",
        "# # training parameters\n",
        "# n_epochs = 5   # number of epochs to run\n",
        "# batch_size = 8  # size of each batch\n",
        "# batch_start = torch.arange(0, len(X_train), batch_size)\n",
        " \n",
        "# # Hold the best model\n",
        "# best_mse = np.inf   # init to infinity\n",
        "# best_weights = None\n",
        "# history = []\n",
        " \n",
        "# # training loop\n",
        "# for epoch in range(n_epochs):\n",
        "#     retribert_model.train()\n",
        "#     with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
        "#         bar.set_description(f\"Epoch {epoch}\")\n",
        "#         for start in bar:\n",
        "#             # take a batch\n",
        "#             X_batch = X_train[start:start+batch_size]\n",
        "#             y_batch = y_train[start:start+batch_size]\n",
        "#             # forward pass\n",
        "#             y_pred = retribert_model(X_batch)\n",
        "#             loss = loss_fn(y_pred, y_batch)\n",
        "#             # backward pass\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             # update weights\n",
        "#             optimizer.step()\n",
        "#             # print progress\n",
        "#             bar.set_postfix(mse=float(loss))\n",
        "#     # evaluate accuracy at end of each epoch\n",
        "#     retribert_model.eval()\n",
        "#     y_pred = retribert_model(X_test)\n",
        "#     mse = loss_fn(y_pred, y_test)\n",
        "#     mse = float(mse)\n",
        "#     history.append(mse)\n",
        "#     if mse < best_mse:\n",
        "#         best_mse = mse\n",
        "#         best_weights = copy.deepcopy(retribert_model.state_dict())\n",
        " \n",
        "# # restore model and return best accuracy\n",
        "# retribert_model.load_state_dict(best_weights)"
      ],
      "metadata": {
        "id": "thhTWuFiV1VN"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}