{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilamaiti/spring_2023_w266_final_project_heesuk_iris_srila/blob/main/iris/W266_essay_evaluation_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhf_T8cMjGsp"
      },
      "source": [
        "# **Installing new libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD4BChywisTm"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install emoji==0.6.0\n",
        "!pip install scikit-multilearn\n",
        "!pip install iterative-stratification\n",
        "!pip install tensorflow==2.11.0\n",
        "!pip install yellowbrick\n",
        "!pip install tensorflow_gpu==1.15.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC3s_6EZjMBt"
      },
      "source": [
        "# **Importing libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaPH5XqxjU_z"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "print(f'transformers version: {transformers.__version__}')\n",
        "from transformers import logging as hf_logging\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "hf_logging.set_verbosity_error()\n",
        "'''\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import spacy      \n",
        "from spacy import displacy\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS\n",
        "from wordcloud import ImageColorGenerator\n",
        "nltk.download('punkt')\n",
        "'''\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "ROBERTA_MODEL_CHKPT = \"roberta-base\"\n",
        "BERTWEET_MODEL_CHKPT = \"vinai/bertweet-base\"\n",
        "BERT_MODEL_CHKPT = 'bert-base-cased'\n",
        "\n",
        "# Other required libraries\n",
        "import math\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import copy\n",
        "import sys\n",
        "import gc\n",
        "import pprint\n",
        "import statistics\n",
        "\n",
        "# data visualization\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "\n",
        "# others\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from scipy.cluster.hierarchy import set_link_color_palette\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "from skmultilearn.model_selection import iterative_train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# distances\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Data visualization libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tensorflow libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils.layer_utils import count_params\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.losses import mae\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from keras.models import Model\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "\n",
        "import torch\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fjpBUG5jbmt"
      },
      "source": [
        "# **General functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahNGWcMIFD8u"
      },
      "source": [
        "## **Rounding Off to Custom Decimal Places**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5opWTwwnFLLP"
      },
      "outputs": [],
      "source": [
        "def roundPartial(value, resolution):\n",
        "    return round (value / resolution) * resolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYfihW3Mjgkf"
      },
      "source": [
        "## **Set parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pr1-rS8tjiqn"
      },
      "outputs": [],
      "source": [
        "def set_config_param(seed = 99):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    tf.keras.backend.clear_session()\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/Colab Notebooks\"\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    \n",
        "    \n",
        "set_config_param(20230214)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yYB47Gdjo0L"
      },
      "source": [
        "## **Plot loss and accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BE9NqiNWjrPy"
      },
      "outputs": [],
      "source": [
        "def plot_loss_accuracy(history, col_list):\n",
        "    fig, ax = plt.subplots(2, 6, figsize=(16, 6), sharex='col', sharey='row')\n",
        "    fig.tight_layout(pad=5.0)\n",
        "    for idx, col in enumerate(col_list):\n",
        "\n",
        "        ax[0, idx].plot(history[col + '_loss'], lw=2, color='darkgoldenrod')\n",
        "        ax[0, idx].plot(history['val_' + col + '_loss'], lw=2, color='indianred')\n",
        "        #ax[0, idx].legend(loc='center left')\n",
        "        ax[0, idx].legend(['Train', 'Validation'], fontsize=5)\n",
        "        ax[0, idx].set_xlabel('Epochs', size=10)\n",
        "        ax[0, idx].set_title('Loss: ' + col)\n",
        "\n",
        "        ax[1, idx].plot(history[col + '_accuracy'], lw=2, color='darkgoldenrod')\n",
        "        ax[1, idx].plot(history['val_' + col + '_accuracy'], lw=2, color='indianred')\n",
        "        #ax[0, idx].legend(loc='center left')\n",
        "        ax[1, idx].legend(['Train', 'Validation'], fontsize=5)\n",
        "        ax[1, idx].set_xlabel('Epochs', size=10)\n",
        "        ax[1, idx].set_title('Accuracy: ' + col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dII35P-P_J_t"
      },
      "source": [
        "## **Plot Loss and other KPI specified**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQgOGDu6_PVK"
      },
      "outputs": [],
      "source": [
        "def custom_plot(df, model_name, kpi_name, kpi_string):\n",
        "    x_arr = np.arange(len(df['loss'])) + 1\n",
        "    fig = plt.figure(figsize=(12, 4))\n",
        "    ax = fig.add_subplot(1, 2, 1)\n",
        "    ax.plot(x_arr, df['loss'], '-o', label = model_name + ' : Train loss')\n",
        "    ax.plot(x_arr, df['val_loss'], '--<', label = model_name + ' :  Validation loss')\n",
        "    ax.legend(fontsize = 15)\n",
        "    ax.set_xlabel('Epoch', size = 15)\n",
        "    ax.set_ylabel('Loss', size = 15)\n",
        "\n",
        "    ax = fig.add_subplot(1, 2, 2)\n",
        "    ax.plot(x_arr, df[kpi_name], '-o', label = model_name + ' : Train ' + kpi_string)\n",
        "    ax.plot(x_arr, df['val_' + kpi_name], '--<', label = model_name + ' : Validation ' + kpi_string)\n",
        "    ax.legend(fontsize = 15)\n",
        "    ax.set_xlabel('Epoch', size = 15)\n",
        "    ax.set_ylabel(kpi_name, size = 15)\n",
        "    #ax.set_ylim(0,1)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3y3byq0Fk9P"
      },
      "source": [
        "## **Text Encode**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSxlBmVZFm2n"
      },
      "outputs": [],
      "source": [
        "def text_encode(texts, tokenizer, max_len):\n",
        "    input_ids = []\n",
        "    # token_type_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for text in texts:\n",
        "        token = tokenizer(text, \n",
        "                          max_length = max_len, \n",
        "                          truncation = True, \n",
        "                          padding = 'max_length',\n",
        "                          add_special_tokens = True)\n",
        "        input_ids.append(token['input_ids'])\n",
        "        # token_type_ids.append(token['token_type_ids'])\n",
        "        attention_mask.append(token['attention_mask'])\n",
        "    \n",
        "    return np.array(input_ids), np.array(attention_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJw9YpJXjwSv"
      },
      "source": [
        "## **Custom metric**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VCdZF9ZjzN6"
      },
      "outputs": [],
      "source": [
        "def MCRMSE(y_true, y_pred):\n",
        "    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis = 1)\n",
        "    return tf.reduce_mean(tf.sqrt(colwise_mse), axis = -1, keepdims = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdYaqDeEGJE-"
      },
      "source": [
        "## **Build Base Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brWkfFsUk0wl"
      },
      "outputs": [],
      "source": [
        "def build_regression_model(loss = 'MCRMSE',\n",
        "                           model_name = 'Roberta', \n",
        "                           dense_dim = 6, \n",
        "                           MAX_LEN = 512,\n",
        "                           learning_rate = 1e-5,\n",
        "                           dropout = .1,\n",
        "                           number_of_hidden_layers = 1,\n",
        "                           hidden_layer_node_count = 64,\n",
        "                           retrain_layer_count = 0):\n",
        "    \n",
        "    # Define inputs\n",
        "    input_ids = tf.keras.Input(shape = (MAX_LEN ,), dtype = 'int64', name = 'input_ids')\n",
        "    attention_masks = tf.keras.Input(shape = (MAX_LEN ,), dtype = 'int64', name = 'attention_masks')\n",
        "    \n",
        "    if model_name == 'Roberta':\n",
        "        model_tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_MODEL_CHKPT)\n",
        "        model = TFRobertaModel.from_pretrained(ROBERTA_MODEL_CHKPT)\n",
        "    elif model_name == 'Bertweet':\n",
        "        model_tokenizer = AutoTokenizer.from_pretrained(BERTWEET_MODEL_CHKPT)\n",
        "        model = TFRobertaModel.from_pretrained(BERTWEET_MODEL_CHKPT)\n",
        "    elif model_name == 'Bert':\n",
        "        model_tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_CHKPT)\n",
        "        model = TFBertModel.from_pretrained(BERT_MODEL_CHKPT)  \n",
        "\n",
        "    # Adjust the trainable layer weights based on retrain_layer_count\n",
        "    # If retrain_layer_count is 0, then base model is frozen.\n",
        "    # If retrain_layer_count is 12, then the entire base model is trainable.\n",
        "    # And that implies that all the pretrained weights are lost and it relearns\n",
        "    # from the input data.\n",
        "    # If retrain_layer_count is between 1 and 11, then the last n layers of\n",
        "    # the pretrained model retrained.\n",
        "    if retrain_layer_count == 0:\n",
        "        # The pretained model is frozen\n",
        "        model.trainable = False           \n",
        "\n",
        "    elif retrain_layer_count == 12:  \n",
        "        # The pretrained model is retrained thru all layers.       \n",
        "        model.trainable = True     \n",
        "\n",
        "    else:    \n",
        "        # Restrict training to the num_train_layers outer transformer layers\n",
        "        retrain_layer_list = []\n",
        "        model.trainable = False  \n",
        "        for retrain_layer_number in range(retrain_layer_count):\n",
        "\n",
        "            layer_code = '_' + str(11 - retrain_layer_number)\n",
        "            retrain_layer_list.append(layer_code)\n",
        "        \n",
        "        print('Retrain layers: \\n', retrain_layer_list)\n",
        "        #model.compile()\n",
        "        print(f\"Number of trainable parameters : {count_params(model.trainable_weights)}\")\n",
        "        print(f\"Number of non-trainable parameters : {count_params(model.non_trainable_variables)}\")\n",
        "        for weight in model.weights:\n",
        "            weight._trainable = False\n",
        "            #print(\"***\", layer.name, layer._trainable)\n",
        "            if 'layer_' in weight.name and weight.name.split(\".\")[1].split(\"/\")[0] in retrain_layer_list:\n",
        "                weight._trainable = True\n",
        "                # print(\"$$$\", weight.name, weight._trainable)\n",
        "            elif 'layer_' not in weight.name :\n",
        "                weight._trainable = True\n",
        "                # print(\"###\", weight.name, weight._trainable)\n",
        "        model.compile()\n",
        "\n",
        "        for weight_details in model.weights:\n",
        "            print(weight_details.name, weight_details.trainable)\n",
        "    print(f\"Number of trainable parameters : {count_params(model.trainable_weights)}\")\n",
        "    print(f\"Number of non-trainable parameters : {count_params(model.non_trainable_variables)}\")\n",
        "                \n",
        "    # Insert pretrained model layer\n",
        "    pretrained_transformer = model([input_ids, attention_masks])\n",
        "\n",
        "    # Get the CLS output off the pretrained model\n",
        "    cls_token = pretrained_transformer[0][:, 0, :]\n",
        "\n",
        "    # Append the hidden layer and dropout layer\n",
        "    layer_list = []\n",
        "    for layer in range(number_of_hidden_layers):\n",
        "        if layer == 0:\n",
        "            hidden_layer = tf.keras.layers.Dense(units      = hidden_layer_node_count\n",
        "                                               , activation = 'relu'\n",
        "                                               , name       = 'hidden_layer_' + str(layer + 1)\n",
        "                                                )(cls_token)\n",
        "        else:\n",
        "            hidden_layer = tf.keras.layers.Dense(units      = hidden_layer_node_count\n",
        "                                               , activation = 'relu'\n",
        "                                               , name       = 'hidden_layer_' + str(layer + 1)\n",
        "                                            )(layer_list[-1])\n",
        "        layer_list.append(hidden_layer)\n",
        "        dropout_layer = tf.keras.layers.Dropout(dropout, \n",
        "                                                name = 'dropout_layer_' + str(layer + 1)\n",
        "                                               )(hidden_layer) \n",
        "        layer_list.append(dropout_layer)\n",
        "\n",
        "    # Add the output layer\n",
        "    output = tf.keras.layers.Dense(6,)(layer_list[-1])\n",
        "\n",
        "    # Build the final model\n",
        "    regression_model = tf.keras.Model(inputs = [input_ids, attention_masks], outputs = output)\n",
        "    \n",
        "    # Model compile\n",
        "    if loss == 'MCRMSE':\n",
        "        regression_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
        "                                 loss      = MCRMSE,\n",
        "                                 metrics   = MCRMSE\n",
        "                                )\n",
        "    \n",
        "    print(regression_model.summary())\n",
        "    keras.utils.plot_model(regression_model, \n",
        "                           show_shapes = False, \n",
        "                           show_dtype = False, \n",
        "                           show_layer_names = True, \n",
        "                           dpi = 90)\n",
        "    return regression_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzxEshCMv3Te"
      },
      "outputs": [],
      "source": [
        "def model_fit(model, \n",
        "              df_train, \n",
        "              train_indices,\n",
        "              val_indices,\n",
        "              model_name = 'Roberta', \n",
        "              MAX_LEN = 512,\n",
        "              epochs = 5,\n",
        "              batch_size = 4,\n",
        "              validation_split = .2):\n",
        "  \n",
        "    # Building the tokenizer for the given model\n",
        "    if model_name == 'Roberta':\n",
        "        tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_MODEL_CHKPT)\n",
        "    elif model_name == 'Bertweet':\n",
        "        tokenizer = AutoTokenizer.from_pretrained(BERTWEET_MODEL_CHKPT)\n",
        "    elif model_name == 'Bert':\n",
        "        tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_CHKPT)\n",
        "        \n",
        "    train_encoded_input_ids, train_encoded_attention_masks = text_encode(df_train.iloc[list(train_indices)]['full_text'], tokenizer, MAX_LEN)\n",
        "    val_encoded_input_ids, val_encoded_attention_masks = text_encode(df_train.iloc[list(val_indices)]['full_text'], tokenizer, MAX_LEN)\n",
        "\n",
        "    y_train = np.array(df_train.iloc[list(train_indices)][label_cols], dtype = \"float32\")\n",
        "    y_val = np.array(df_train.iloc[list(val_indices)][label_cols], dtype = \"float32\")\n",
        "    \n",
        "    hist = model.fit([train_encoded_input_ids, train_encoded_attention_masks],\n",
        "                     y_train,\n",
        "                     validation_data = ([val_encoded_input_ids, val_encoded_attention_masks], \n",
        "                                        y_val\n",
        "                                       ),\n",
        "                     batch_size = batch_size,        \n",
        "                     epochs = epochs\n",
        "                    )\n",
        "\n",
        "    df_history = pd.DataFrame(hist.history)\n",
        "    return df_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "202ecOu4GLFG"
      },
      "outputs": [],
      "source": [
        "def build_base_model(model_layer, learning_rate, dense_dim = 6):\n",
        "    \n",
        "    #define inputs\n",
        "    input_ids = tf.keras.Input(shape = (MAX_LEN ,), dtype = 'int64', name = 'input_ids')\n",
        "    attention_masks = tf.keras.Input(shape = (MAX_LEN ,), dtype = 'int64', name = 'attention_masks')\n",
        "    \n",
        "    #insert BERT layer\n",
        "    transformer_layer = model_layer([input_ids, attention_masks])\n",
        "    \n",
        "    #choose only last hidden-state\n",
        "    x = transformer_layer[1]\n",
        "    output = tf.keras.layers.Dense(dense_dim)(x)\n",
        "    #output = tf.keras.layers.Rescaling(scale=4.0, offset=1.0)(x)\n",
        "    model = tf.keras.models.Model(inputs = [input_ids, attention_masks], outputs = output)\n",
        "\n",
        "    model.compile(tf.keras.optimizers.Adam(learning_rate), loss = mse_loss, metrics = mse_metrics)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmCPltuehyyQ"
      },
      "source": [
        "## **Build a model with custom loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnaCwE2Uhx-4"
      },
      "outputs": [],
      "source": [
        "def build_base_model_with_custom_loss(model_layer, learning_rate, dense_dim = 6):\n",
        "    \n",
        "    #define inputs\n",
        "    input_ids = tf.keras.Input(shape = (MAX_LEN ,), dtype = 'int64', name = 'input_ids')\n",
        "    attention_masks = tf.keras.Input(shape = (MAX_LEN ,), dtype = 'int64', name = 'attention_masks')\n",
        "    \n",
        "    #insert BERT layer\n",
        "    transformer_layer = model_layer([input_ids, attention_masks])\n",
        "    \n",
        "    #choose only last hidden-state\n",
        "    x = transformer_layer[1]\n",
        "    output = tf.keras.layers.Dense(dense_dim)(x)\n",
        "    #output = tf.keras.layers.Rescaling(scale=4.0, offset=1.0)(x)\n",
        "    model = tf.keras.models.Model(inputs = [input_ids, attention_masks], outputs = output)\n",
        "\n",
        "    model.compile(tf.keras.optimizers.Adam(learning_rate), loss = MCRMSE, metrics = MCRMSE)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96pZ-IUrCD30"
      },
      "source": [
        "##**Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQ1hLAW2CGm4"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, y_test, test_encoded_input_ids, test_encoded_attention_masks):\n",
        "    score = model.evaluate([test_encoded_input_ids, test_encoded_attention_masks], \n",
        "                           y_test\n",
        "                          ) \n",
        "    print('\\nTest Loss : {:.2f}%'.format(score[0]))\n",
        "    print('\\nTest Accuracy :  {:.2f}%'.format(score[1]))\n",
        "    return score[0], score[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJV4COxjCc9j"
      },
      "source": [
        "## **Predict**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJsH7HXpCiAT"
      },
      "outputs": [],
      "source": [
        "def predict_model(model, df_test, test_encoded_input_ids, test_encoded_attention_masks, label_cols):\n",
        "    predictions = model.predict([test_encoded_input_ids, test_encoded_attention_masks])\n",
        "    df_predictions = pd.DataFrame(predictions, columns=['pred_' + c for c in label_cols])\n",
        "    for col in label_cols:\n",
        "        df_predictions['transformed_pred_' + col] = df_predictions['pred_' + col].apply(lambda x : roundPartial(x, .5))\n",
        "    df_comparison = pd.merge(df_test, df_predictions, left_index = True, right_index = True)\n",
        "    return df_predictions, df_comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CGFbdlnFEMs"
      },
      "source": [
        "## **Plot Model Structure**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmSoWa7kFHCc"
      },
      "outputs": [],
      "source": [
        "def plot_model_structure(model):\n",
        "    keras.utils.plot_model(model, show_shapes = False, show_dtype = False, show_layer_names = True, dpi = 90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N73w6RpFsbPx"
      },
      "source": [
        "## **Samples of predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sP5jgL0fsmpV"
      },
      "outputs": [],
      "source": [
        "def hall_of_fame(df, component, num):\n",
        "  samp = df.query(\"transformed_pred_\"+component+\"==\"+component).sample(num)\n",
        "  samp = samp.reset_index()\n",
        "  for index, row in samp.iterrows():\n",
        "      print(\"predicted: \",row[\"transformed_pred_\"+component])\n",
        "      print(\"original: \",row[component])\n",
        "      pprint.pprint(row[\"full_text\"])\n",
        "      print(\"**********\")\n",
        "\n",
        "def hall_of_shame(df, component, num):\n",
        "  samp = df.query(\"transformed_pred_\"+component+\"!=\"+component).sample(num)\n",
        "  samp = samp.reset_index()\n",
        "  for index, row in samp.iterrows():\n",
        "      print(\"predicted: \",row[\"transformed_pred_\"+component])\n",
        "      print(\"original: \",row[component])\n",
        "      pprint.pprint(row[\"full_text\"])\n",
        "      print(\"**********\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP0n_Dd8j26W"
      },
      "source": [
        "# **Read input files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLDHxYTxjxVl"
      },
      "outputs": [],
      "source": [
        "# data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "%cd \"gdrive/MyDrive/Colab Notebooks/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le5HU_86j4-9"
      },
      "outputs": [],
      "source": [
        "input_train_df = pd.read_csv('train.csv')\n",
        "input_test_df = pd.read_csv('test.csv')\n",
        "# Cleaning up full_text : Removing tabl and carriage return characters\n",
        "input_train_df['full_text'] = input_train_df[\"full_text\"].replace(re.compile(r'[\\n\\r\\t]'), ' ', regex = True)\n",
        "input_test_df['full_text'] = input_test_df[\"full_text\"].replace(re.compile(r'[\\n\\r\\t]'), ' ', regex = True)\n",
        "\n",
        "label_cols = input_train_df.columns[2:]\n",
        "input_train_df['score_sum'] = np.sum(input_train_df[label_cols], axis = 1)\n",
        "pred_col_list = ['transformed_pred_' + col for col in label_cols]\n",
        "\n",
        "orig_train_df = copy.deepcopy(input_train_df)\n",
        "orig_train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B51vr1w3upZy"
      },
      "source": [
        "# **Model building**\n",
        "\n",
        "As we do not have labels for our test data, we are repurposing our training data by splitting it into 80:20 ratio.\n",
        "\n",
        "The train part is then going thru k fold cross validation and get tested on validation set and final test is done on the test set. Final test accuracy will be the average MCRMSE score across k-folds.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9c7XpOGZi46"
      },
      "outputs": [],
      "source": [
        "# shuffling them back again\n",
        "shuffle = np.random.permutation(np.arange(orig_train_df.shape[0]))\n",
        "orig_train_df = orig_train_df.iloc[shuffle]\n",
        "\n",
        "# Splitting the data in 80:20 split\n",
        "split = (0.8, 0.2)\n",
        "splits = np.multiply(len(orig_train_df), split).astype(int)\n",
        "df_train, df_test = orig_train_df[ : splits[0]], orig_train_df[splits[0] : ]\n",
        "y_test = np.array(df_test[label_cols], dtype = \"float32\")\n",
        "\n",
        "print(f\"Length of train data : {len(df_train)}\")\n",
        "print(f\"Length of test data : {len(df_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viDJ9yPuZM7a"
      },
      "outputs": [],
      "source": [
        "# Fixed parameters\n",
        "dense_dim = 6\n",
        "number_of_splits = 2\n",
        "random_state = 2023\n",
        "MAX_LEN = 128\n",
        "mse_loss = MCRMSE\n",
        "mse_metrics = MCRMSE\n",
        "model_name_list = ['Bert', 'Bertweet'] # Roberta\n",
        "\n",
        "# Variable parameters\n",
        "epochs = 5\n",
        "batch_size = 4\n",
        "learning_rate = 1e-5\n",
        "validation_split = .2\n",
        "dropout = .1\n",
        "number_of_hidden_layers = 1\n",
        "hidden_layer_node_count = 64\n",
        "retrain_layer_count = 0\n",
        "\n",
        "# Variable parameter dictionary\n",
        "param_list = [\n",
        "                 # Completely frozen base layer\n",
        "                 {'epochs'                  : 5,\n",
        "                  'batch_size'              : 4,\n",
        "                  'learning_rate'           : 1e-5,\n",
        "                  'validation_split'        : .2,\n",
        "                  'dropout'                 : .1,\n",
        "                  'number_of_hidden_layers' : 1,\n",
        "                  'hidden_layer_node_count' : 64,\n",
        "                  'retrain_layer_count'     : 0\n",
        "                 },\n",
        "                 # Partially frozen base layer\n",
        "                 {'epochs'                  : 5,\n",
        "                  'batch_size'              : 4,\n",
        "                  'learning_rate'           : 1e-5,\n",
        "                  'validation_split'        : .2,\n",
        "                  'dropout'                 : .1,\n",
        "                  'number_of_hidden_layers' : 1,\n",
        "                  'hidden_layer_node_count' : 64,\n",
        "                  'retrain_layer_count'     : 6\n",
        "                 },\n",
        "                 # Completely unfrozen base layer\n",
        "                 {'epochs'                  : 5,\n",
        "                  'batch_size'              : 4,\n",
        "                  'learning_rate'           : 1e-5,\n",
        "                  'validation_split'        : .2,\n",
        "                  'dropout'                 : .1,\n",
        "                  'number_of_hidden_layers' : 1,\n",
        "                  'hidden_layer_node_count' : 64,\n",
        "                  'retrain_layer_count'     : 12\n",
        "                 },\n",
        "             ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3dExc7WuXWn"
      },
      "source": [
        "BERT-base-cased"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, param_entry in enumerate(param_list):\n",
        "    MAX_LEN = 512\n",
        "    epoch_val = param_entry['epochs']\n",
        "    batch_size_val = param_entry['batch_size']\n",
        "    learning_rate_val = param_entry['learning_rate']\n",
        "    validation_split_val = param_entry['validation_split']\n",
        "    dropout_val = param_entry['dropout']\n",
        "    number_of_hidden_layers_val = param_entry['number_of_hidden_layers']\n",
        "    hidden_layer_node_count_val = param_entry['hidden_layer_node_count']\n",
        "    retrain_layer_count_val = param_entry['retrain_layer_count']\n",
        "\n",
        "    print(\"************************\")\n",
        "    print(f\"Iteration : {idx + 1}\")\n",
        "    print(\"Parameters...\")\n",
        "    print(f\"Epochs : {epoch_val}\")\n",
        "    print(f\"Batch size : {batch_size_val}\")\n",
        "    print(f\"Learning rate : {learning_rate_val}\")\n",
        "    print(f\"Validation split : {validation_split_val}\")\n",
        "    print(f\"Dropout : {dropout_val}\")\n",
        "    print(f\"Number of hidden layers : {number_of_hidden_layers_val}\")\n",
        "    print(f\"Hidden layer node count : {hidden_layer_node_count_val}\")\n",
        "    print(f\"Retrain layer count : {retrain_layer_count_val}\")\n",
        "    print(\"************************\")\n",
        "    set_config_param(20230214)\n",
        "\n",
        "    model_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_CHKPT)\n",
        "    model = TFRobertaModel.from_pretrained(BERT_MODEL_CHKPT)\n",
        "\n",
        "    train_input_ids, train_attention_masks = text_encode(df_train['full_text'], model_tokenizer, MAX_LEN)\n",
        "    test_input_ids, test_attention_masks = text_encode(df_test['full_text'], model_tokenizer, MAX_LEN)\n",
        "\n",
        "    y_train = np.array(df_train[label_cols], dtype = \"float32\")\n",
        "\n",
        "    bert = build_regression_model(loss= \"MCRMSE\", \n",
        "                                      model_name = \"Bert\",\n",
        "                                      dense_dim = 6, \n",
        "                                      MAX_LEN = 512,\n",
        "                                      learning_rate = learning_rate_val,\n",
        "                                      dropout=dropout_val,\n",
        "                                      number_of_hidden_layers = number_of_hidden_layers_val,\n",
        "                                      hidden_layer_node_count = hidden_layer_node_count_val,\n",
        "                                      retrain_layer_count = retrain_layer_count_val)\n",
        "    bert.summary()\n",
        "\n",
        "    history_v1 = bert.fit((train_input_ids, train_attention_masks),\n",
        "                  y_train,\n",
        "                  batch_size = batch_size_val,     \n",
        "                  epochs = epoch_val,\n",
        "                  validation_split = validation_split_val\n",
        "                  )\n",
        "\n",
        "    history_v1_df = pd.DataFrame(history_v1.history)\n",
        "\n",
        "    score_v1 = bert.evaluate([test_input_ids, test_attention_masks], \n",
        "                    y_test\n",
        "                  ) \n",
        "\n",
        "    predictions_v1 = bert.predict([test_input_ids, test_attention_masks])\n",
        "    df_pred_v1 = pd.DataFrame(predictions_v1, columns=['pred_' + c for c in label_cols])\n",
        "\n",
        "    for col in label_cols:\n",
        "      df_pred_v1['transformed_pred_' + col] = df_pred_v1['pred_' + col].apply(lambda x : roundPartial(x, .5))\n",
        "    \n",
        "    df_compare_v1= pd.merge(df_test, df_pred_v1, left_index = True, right_index = True)\n",
        "    all = []\n",
        "    for col in label_cols:\n",
        "      all.append(((df_compare_v1[col]-df_compare_v1[\"transformed_pred_\"+col]).pow(2).mean())**(0.5))\n",
        "    RMSE_scaled = statistics.fmean(all)\n",
        "    print(f\"RMSE_scaled: {RMSE_scaled}\")\n",
        "    \n",
        "    for label in label_cols:\n",
        "      print(label)\n",
        "      hall_of_fame(df_compare_v1,label,1)\n",
        "      print(\"************************\")\n",
        "      hall_of_shame(df_compare_v1,label,1)\n",
        "      print(\"************************\")\n",
        "    for component in label_cols:\n",
        "      print(component)\n",
        "      print(\"% Predicting too high: \" + str(len(df_compare_v1.query(\"transformed_pred_\"+component+\">\"+component))/len(df_compare_v1)))\n",
        "      print(\"% Predicted correctly: \" + str(len(df_compare_v1.query(\"transformed_pred_\"+component+\"==\"+component))/len(df_compare_v1)))\n",
        "      print(\"% Predicting too low: \" + str(len(df_compare_v1.query(\"transformed_pred_\"+component+\"<\"+component))/len(df_compare_v1)))\n",
        "      print(\"****\")\n",
        "    # Predicting too high or low for every score? \n",
        "    for component in label_cols:\n",
        "      print(component)\n",
        "\n",
        "      for score in [1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0]:\n",
        "        if len(df_compare_v1[df_compare_v1[component]==score])==0:\n",
        "          print(component+\"==\"+str(score)+\" has 0 rows\")\n",
        "        else:\n",
        "          print(f'length: {len(df_compare_v1[df_compare_v1[component]==score])}')\n",
        "          print(\"% Predicting the above (\"+str(score)+\"): \" +\n",
        "                str(len(df_compare_v1[(df_compare_v1[component]==score) &\n",
        "                                      (df_compare_v1[\"transformed_pred_\"+component]>score)])/len(df_compare_v1[df_compare_v1[component]==score])))\n",
        "          print(\"% Predicting the same (\"+str(score)+\"): \" +\n",
        "                str(len(df_compare_v1[(df_compare_v1[component]==score) & \n",
        "                                      (df_compare_v1[\"transformed_pred_\"+component]==score)])/len(df_compare_v1[df_compare_v1[component]==score])))\n",
        "          print(\"% Predicting the below (\"+str(score)+\"): \" + \n",
        "                str(len(df_compare_v1[(df_compare_v1[component]==score) & \n",
        "                                      (df_compare_v1[\"transformed_pred_\"+component]<score)])/len(df_compare_v1[df_compare_v1[component]==score])))\n",
        "        print(\"****\")\n",
        "\n",
        "    for component in label_cols:\n",
        "      print(component)\n",
        "      print(\"% Predicting within .5: \" + str(len(df_compare_v1.query(\"transformed_pred_\"+component+\"<=(\"+component+\"+.5) and transformed_pred_\"+component+\">=(\"+component+\"-.5)\"))/len(df_compare_v1)))"
      ],
      "metadata": {
        "id": "_oBQigtnjgWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN5pnfXQua0e"
      },
      "source": [
        "BERTweet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, param_entry in enumerate(param_list):\n",
        "    MAX_LEN = 512\n",
        "    epoch_val = param_entry['epochs']\n",
        "    batch_size_val = param_entry['batch_size']\n",
        "    learning_rate_val = param_entry['learning_rate']\n",
        "    validation_split_val = param_entry['validation_split']\n",
        "    dropout_val = param_entry['dropout']\n",
        "    number_of_hidden_layers_val = param_entry['number_of_hidden_layers']\n",
        "    hidden_layer_node_count_val = param_entry['hidden_layer_node_count']\n",
        "    retrain_layer_count_val = param_entry['retrain_layer_count']\n",
        "\n",
        "    print(\"************************\")\n",
        "    print(f\"Iteration : {idx + 1}\")\n",
        "    print(\"Parameters...\")\n",
        "    print(f\"Epochs : {epoch_val}\")\n",
        "    print(f\"Batch size : {batch_size_val}\")\n",
        "    print(f\"Learning rate : {learning_rate_val}\")\n",
        "    print(f\"Validation split : {validation_split_val}\")\n",
        "    print(f\"Dropout : {dropout_val}\")\n",
        "    print(f\"Number of hidden layers : {number_of_hidden_layers_val}\")\n",
        "    print(f\"Hidden layer node count : {hidden_layer_node_count_val}\")\n",
        "    print(f\"Retrain layer count : {retrain_layer_count_val}\")\n",
        "    print(\"************************\")\n",
        "    set_config_param(20230214)\n",
        "\n",
        "    model_tokenizer = AutoTokenizer.from_pretrained(BERTWEET_MODEL_CHKPT)\n",
        "    model = TFRobertaModel.from_pretrained(BERTWEET_MODEL_CHKPT)\n",
        "\n",
        "    train_input_ids, train_attention_masks = text_encode(df_train['full_text'], model_tokenizer, MAX_LEN)\n",
        "    test_input_ids, test_attention_masks = text_encode(df_test['full_text'], model_tokenizer, MAX_LEN)\n",
        "\n",
        "    y_train = np.array(df_train[label_cols], dtype = \"float32\")\n",
        "\n",
        "    bertweet = build_regression_model(loss= \"MCRMSE\", \n",
        "                                      model_name = \"Bertweet\",\n",
        "                                      dense_dim = 6, \n",
        "                                      MAX_LEN = 512,\n",
        "                                      learning_rate = learning_rate_val,\n",
        "                                      dropout=dropout_val,\n",
        "                                      number_of_hidden_layers = number_of_hidden_layers_val,\n",
        "                                      hidden_layer_node_count = hidden_layer_node_count_val,\n",
        "                                      retrain_layer_count = retrain_layer_count_val)\n",
        "    bertweet.summary()\n",
        "\n",
        "    history_v1 = bertweet.fit((train_input_ids, train_attention_masks),\n",
        "                  y_train,\n",
        "                  batch_size = batch_size_val,     \n",
        "                  epochs = epoch_val,\n",
        "                  validation_split = validation_split_val\n",
        "                  )\n",
        "\n",
        "    history_v1_df = pd.DataFrame(history_v1.history)\n",
        "\n",
        "    score_v1 = bertweet.evaluate([test_input_ids, test_attention_masks], \n",
        "                    y_test\n",
        "                  ) \n",
        "\n",
        "    predictions_v1 = bertweet.predict([test_input_ids, test_attention_masks])\n",
        "    df_pred_v1 = pd.DataFrame(predictions_v1, columns=['pred_' + c for c in label_cols])\n",
        "\n",
        "    for col in label_cols:\n",
        "      df_pred_v1['transformed_pred_' + col] = df_pred_v1['pred_' + col].apply(lambda x : roundPartial(x, .5))\n",
        "    \n",
        "    df_compare_v1= pd.merge(df_test, df_pred_v1, left_index = True, right_index = True)\n",
        "\n",
        "    all = []\n",
        "    for col in label_cols:\n",
        "      all.append(((df_compare_v1[col]-df_compare_v1[\"transformed_pred_\"+col]).pow(2).mean())**(0.5))\n",
        "    RMSE_scaled = statistics.fmean(all)\n",
        "    print(f\"RMSE_scaled: {RMSE_scaled}\")\n",
        "\n",
        "    for label in label_cols:\n",
        "      print(label)\n",
        "      hall_of_fame(df_compare_v1,label,1)\n",
        "      print(\"************************\")\n",
        "      hall_of_shame(df_compare_v1,label,1)\n",
        "      print(\"************************\")\n",
        "    for component in label_cols:\n",
        "      print(component)\n",
        "      print(\"% Predicting too high: \" + str(len(df_compare_v1.query(\"transformed_pred_\"+component+\">\"+component))/len(df_compare_v1)))\n",
        "      print(\"% Predicted correctly: \" + str(len(df_compare_v1.query(\"transformed_pred_\"+component+\"==\"+component))/len(df_compare_v1)))\n",
        "      print(\"% Predicting too low: \" + str(len(df_compare_v1.query(\"transformed_pred_\"+component+\"<\"+component))/len(df_compare_v1)))\n",
        "      print(\"****\")\n",
        "    # Predicting too high or low for every score? \n",
        "\n",
        "    for component in label_cols:\n",
        "      print(component)\n",
        "\n",
        "      for score in [1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0]:\n",
        "        if len(df_compare_v1[df_compare_v1[component]==score])==0:\n",
        "          print(component+\"==\"+str(score)+\" has 0 rows\")\n",
        "        else:\n",
        "          print(f'length: {len(df_compare_v1[df_compare_v1[component]==score])}')\n",
        "          print(\"% Predicting the above (\"+str(score)+\"): \" +\n",
        "                str(len(df_compare_v1[(df_compare_v1[component]==score) &\n",
        "                                      (df_compare_v1[\"transformed_pred_\"+component]>score)])/len(df_compare_v1[df_compare_v1[component]==score])))\n",
        "          print(\"% Predicting the same (\"+str(score)+\"): \" +\n",
        "                str(len(df_compare_v1[(df_compare_v1[component]==score) & \n",
        "                                      (df_compare_v1[\"transformed_pred_\"+component]==score)])/len(df_compare_v1[df_compare_v1[component]==score])))\n",
        "          print(\"% Predicting the below (\"+str(score)+\"): \" + \n",
        "                str(len(df_compare_v1[(df_compare_v1[component]==score) & \n",
        "                                      (df_compare_v1[\"transformed_pred_\"+component]<score)])/len(df_compare_v1[df_compare_v1[component]==score])))\n",
        "        print(\"****\")\n",
        "    for component in label_cols:\n",
        "      print(component)\n",
        "      print(\"% Predicting within .5: \" + str(len(df_compare_v1.query(\"transformed_pred_\"+component+\"<=(\"+component+\"+.5) and transformed_pred_\"+component+\">=(\"+component+\"-.5)\"))/len(df_compare_v1)))"
      ],
      "metadata": {
        "id": "v2WHf9FO5mV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_compare_v1)"
      ],
      "metadata": {
        "id": "y573G-r-HG33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_compare_v1[\"character_count\"] = df_compare_v1.full_text.str.len()"
      ],
      "metadata": {
        "id": "Vg4xRReLGqLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_compare_v1[df_compare_v1[\"character_count\"]<280])"
      ],
      "metadata": {
        "id": "XsevoYXxR96P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2ET_cAludgH"
      },
      "source": [
        "Concat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LED6NugFs3Ec"
      },
      "outputs": [],
      "source": [
        "set_config_param(20230214)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tuz4PZrbh1vk"
      },
      "source": [
        "# **Clustering**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"score_sum\"].hist()\n",
        "plt.title(\"Train Dataset's Total Scores\")\n",
        "plt.xlabel(\"Total Score\")\n",
        "plt.ylabel(\"Number of Essays\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hWXYOWjpPEUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTMJpfp8YzNz"
      },
      "outputs": [],
      "source": [
        "df_rating = copy.deepcopy(df_train[label_cols])\n",
        "rating_values_array = np.array(df_rating[label_cols])\n",
        "\n",
        "# standardize\n",
        "sc = StandardScaler()\n",
        "rating_values_array_std = sc.fit(rating_values_array).transform(rating_values_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHFkIEq5Cgul"
      },
      "outputs": [],
      "source": [
        "km = KMeans(random_state=42)\n",
        "visualizer = KElbowVisualizer(km, k=(2,10))\n",
        " \n",
        "visualizer.fit(rating_values_array_std)        # Fit the data to the visualizer\n",
        "visualizer.show()        # Finalize and render the figure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLpAtecfC72h"
      },
      "source": [
        "Here is how the Elbow / SSE Plot would look like. As per the plot given below, for n_clusters = 3 that represents the elbow you start seeing diminishing returns by increasing k. The line starts looking linear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2cZ4AyoDOoU"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 3, figsize=(20,5))\n",
        "for idx, i in enumerate([2, 3, 4]):\n",
        "    '''\n",
        "    Create KMeans instance for different number of clusters\n",
        "    '''\n",
        "    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)\n",
        "    #q, mod = divmod(i, 2)\n",
        "    '''\n",
        "    Create SilhouetteVisualizer instance with KMeans instance\n",
        "    Fit the visualizer\n",
        "    '''\n",
        "    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[idx])\n",
        "    visualizer.fit(rating_values_array_std) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-e9bqdIGWnZ"
      },
      "source": [
        "Given above, the Silhouette plot for n_clusters = 3 looks to be most appropriate than others as it stands well against all the three measuring criteria (scores below average Silhouette score, Wide fluctuations in the size of the plot, and non-uniform thickness)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Spyt5ysCGSEp"
      },
      "outputs": [],
      "source": [
        "km_base = KMeans(n_clusters=3,\n",
        "           #init='random',\n",
        "           init='k-means++',\n",
        "           n_init=10,\n",
        "           max_iter=300,\n",
        "           tol=1e-04,\n",
        "           random_state=1234)\n",
        "\n",
        "# predict k-means classes\n",
        "y_km_base = km_base.fit_predict(rating_values_array_std)\n",
        "\n",
        "# Assigning cluster value to the datafarme\n",
        "df_train['cluster_id'] = y_km_base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8Jlal-4bqri"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgY72Q0Qj5rF"
      },
      "source": [
        "We can see that an increase in *k* is associated with a decrease in the within-cluster SSE. \n",
        "\n",
        "This is because the examples are closer to the centroid they assigned to.\n",
        "\n",
        "**The elbow solution**: the optimal *k* is where the within-cluster SSE begings to increase most rapidly.\n",
        "\n",
        "For this particular example the elbow is at *k=2* so we started with a good number of clusters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2RZdPo215xy"
      },
      "outputs": [],
      "source": [
        "df_train_cluster0 = df_train[df_train.cluster_id == 0]\n",
        "df_train_cluster1 = df_train[df_train.cluster_id == 1]\n",
        "df_train_cluster2 = df_train[df_train.cluster_id == 2]\n",
        "\n",
        "print(f\"Length of cluster 0 : {len(df_train_cluster0)}\")\n",
        "print(f\"Length of cluster 1 : {len(df_train_cluster1)}\")\n",
        "print(f\"Length of cluster 2 : {len(df_train_cluster2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1Hd6zkQeqvc"
      },
      "outputs": [],
      "source": [
        "print(f\"Min and max score in cluster 0 are : {np.min(df_train_cluster0['score_sum'])} and {np.max(df_train_cluster0['score_sum'])}\")\n",
        "print(f\"Min and Max score in cluster 1 are : {np.min(df_train_cluster1['score_sum'])} and {np.max(df_train_cluster1['score_sum'])}\")\n",
        "print(f\"Min and Max score in cluster 2 are : {np.min(df_train_cluster2['score_sum'])} and {np.max(df_train_cluster2['score_sum'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61PU78V5HbuI"
      },
      "source": [
        "The cluster is divided based on the distribution of the data. Low-scores are in one bucket, medium scores are placed in another and top scrores are placed in the higher bucket."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kbjNbnOZGJi"
      },
      "source": [
        "# **Model parameter setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHeyCiPxENqg"
      },
      "outputs": [],
      "source": [
        "# Combining the two clusters' data\n",
        "df_train = pd.concat([df_train_cluster0, df_train_cluster1, df_train_cluster2])\n",
        "\n",
        "# shuffling them back again\n",
        "shuffle = np.random.permutation(np.arange(df_train.shape[0]))\n",
        "df_train = df_train.iloc[shuffle]\n",
        "\n",
        "\n",
        "MCRMSE_list = []\n",
        "\n",
        "'''\n",
        "rating_cluster has two values 0 and 1.\n",
        "We are doing k fold with stratification using rating_cluster.\n",
        "We introduced this new column to split on as as our data ouput is multi class\n",
        "and multi label with continuous values and traditional k fold split does not\n",
        "support that.\n",
        "This new column will help us to see if our model is performing better for which \n",
        "group : above or below average.\n",
        "'''\n",
        "for param_val_model_name in model_name_list:\n",
        "\n",
        "    for idx, param_entry in enumerate(param_list):\n",
        "    \n",
        "        param_val_epoch = param_entry['epochs']\n",
        "        param_val_batch_size = param_entry['batch_size']\n",
        "        param_val_learning_rate = param_entry['learning_rate']\n",
        "        param_val_validation_split = param_entry['validation_split']\n",
        "        param_val_dropout = param_entry['dropout']\n",
        "        param_val_number_of_hidden_layers = param_entry['number_of_hidden_layers']\n",
        "        param_val_hidden_layer_node_count = param_entry['hidden_layer_node_count']\n",
        "        param_val_retrain_layer_count = param_entry['retrain_layer_count']\n",
        "\n",
        "        for kfold, (train_indices, val_indices) in enumerate(StratifiedKFold(n_splits     = number_of_splits, \n",
        "                                                                             shuffle      = True, \n",
        "                                                                             random_state = random_state\n",
        "                                                                             ).split(df_train['cluster_id'].values.tolist(), \n",
        "                                                                                     df_train['cluster_id'].values.tolist()\n",
        "                                                                                    )\n",
        "                                                            ):\n",
        "            print(\"************************\")\n",
        "            print(f\"Model : {param_val_model_name}\")\n",
        "            print(f\"Iteration : {idx + 1}\")\n",
        "            print(\"Parameters...\")\n",
        "            print(f\"Epochs : {param_val_epoch}\")\n",
        "            print(f\"Batch size : {param_val_batch_size}\")\n",
        "            print(f\"Learning rate : {param_val_learning_rate}\")\n",
        "            print(f\"Validation split : {param_val_validation_split}\")\n",
        "            print(f\"Dropout : {param_val_dropout}\")\n",
        "            print(f\"Number of hidden layers : {param_val_number_of_hidden_layers}\")\n",
        "            print(f\"Hidden layer node count : {param_val_hidden_layer_node_count}\")\n",
        "            print(f\"Retrain layer count : {param_val_retrain_layer_count}\")\n",
        "            print(f\"k-fold : {kfold + 1}\")\n",
        "            print(f\"length of train data : {len(train_indices)}\")\n",
        "            print(f\"length of validation data : {len(val_indices)}\")\n",
        "            print(\"************************\")\n",
        "            set_config_param(20230214)\n",
        "\n",
        "            # Building the tokenizer for the given model\n",
        "            if param_val_model_name == 'Roberta':\n",
        "                tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_MODEL_CHKPT)\n",
        "            elif param_val_model_name == 'Bertweet':\n",
        "                tokenizer = AutoTokenizer.from_pretrained(BERTWEET_MODEL_CHKPT)\n",
        "            elif param_val_model_name == 'Bert':\n",
        "                tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_CHKPT)\n",
        "            \n",
        "            # Model building\n",
        "            print(\"Building model...\")\n",
        "            regression_model = build_regression_model(loss                    = 'MCRMSE',\n",
        "                                                      model_name              = param_val_model_name, \n",
        "                                                      dense_dim               = dense_dim, \n",
        "                                                      MAX_LEN                 = MAX_LEN,\n",
        "                                                      learning_rate           = param_val_learning_rate,\n",
        "                                                      dropout                 = param_val_dropout,\n",
        "                                                      number_of_hidden_layers = param_val_number_of_hidden_layers,\n",
        "                                                      hidden_layer_node_count = param_val_hidden_layer_node_count,\n",
        "                                                      retrain_layer_count     = param_val_retrain_layer_count\n",
        "                                                     )\n",
        "        \n",
        "            # Model fitting\n",
        "            print(\"Fitting model...\")\n",
        "            df_history = model_fit(model            = regression_model, \n",
        "                                   df_train         = df_train, \n",
        "                                   train_indices    = train_indices,\n",
        "                                   val_indices      = val_indices,\n",
        "                                   model_name       = param_val_model_name, \n",
        "                                   MAX_LEN          = MAX_LEN,\n",
        "                                   epochs           = param_val_epoch,\n",
        "                                   batch_size       = param_val_batch_size,\n",
        "                                   validation_split = param_val_validation_split\n",
        "                                  )\n",
        "            print(df_history.T)\n",
        "\n",
        "            print(\"Plotting loss and MCRMSE...\")\n",
        "            custom_plot(df         = df_history, \n",
        "                        model_name = param_val_model_name, \n",
        "                        kpi_name   = 'MCRMSE', \n",
        "                        kpi_string = 'MCRMSE'\n",
        "                       )\n",
        "\n",
        "            # Prep for model evaluation with test data\n",
        "            print(\"Evaluating mode...\")\n",
        "            test_encoded_input_ids, test_encoded_attention_masks = text_encode(texts      = df_test['full_text'], \n",
        "                                                                               tokenizer = tokenizer, \n",
        "                                                                               max_len    = MAX_LEN\n",
        "                                                                              )\n",
        "            # Model evaluation\n",
        "            test_loss, test_accuracy = evaluate_model(regression_model, \n",
        "                                                      y_test, \n",
        "                                                      test_encoded_input_ids, \n",
        "                                                      test_encoded_attention_masks\n",
        "                                                     )\n",
        "\n",
        "            # Model prediction\n",
        "            print(\"Model prediction...\")\n",
        "            df_prediction, df_comparison = predict_model(regression_model, \n",
        "                                                         df_test, \n",
        "                                                         test_encoded_input_ids, \n",
        "                                                         test_encoded_attention_masks, \n",
        "                                                         label_cols\n",
        "                                                        )\n",
        "\n",
        "            print(\"Plotting model structure...\")\n",
        "            keras.utils.plot_model(regression_model, \n",
        "                                   show_shapes      = False, \n",
        "                                   show_dtype       = False, \n",
        "                                   show_layer_names = True, \n",
        "                                   dpi              = 90\n",
        "                                  )\n",
        "\n",
        "            print(\"Appending to kpi list...\")\n",
        "            temp_dict = {'model_name'                  : param_val_model_name,\n",
        "                         'iteration'                   : idx + 1,\n",
        "                         'epoch'                       : param_val_epoch,\n",
        "                         'batch_size'                  : param_val_batch_size,\n",
        "                         'learning_rate'               : param_val_learning_rate,\n",
        "                         'validation_split'            : param_val_validation_split,\n",
        "                         'dropout'                     : param_val_dropout,\n",
        "                         'number_of_hidden_layers'     : param_val_number_of_hidden_layers,\n",
        "                         'hidden_layer_node_count'     : param_val_hidden_layer_node_count,\n",
        "                         'retrain_layer_count'         : param_val_retrain_layer_count,\n",
        "                         'fold'                        : kfold + 1, \n",
        "                         'train_loss'                  : df_history.iloc[-1][0],\n",
        "                         'train_accuracy'              : df_history.iloc[-1][1],\n",
        "                         'val_loss'                    : df_history.iloc[-1][2],\n",
        "                         'val_accuracy'                : df_history.iloc[-1][3],\n",
        "                         'test_loss'                   : test_loss,\n",
        "                         'test_accuracy'               : test_accuracy\n",
        "                        }\n",
        "            MCRMSE_list.append(temp_dict)\n",
        "            \n",
        "            # Saving the model\n",
        "            print(\"Saving the model...\")\n",
        "            model_file_name = 'regression_model_' + param_val_model_name.lower() + '_iter_' + str(idx + 1) + '_kfold_' + str(kfold + 1) + \".h5\"\n",
        "            regression_model.save(model_file_name)\n",
        "\n",
        "            for label in label_cols:\n",
        "              print(label)\n",
        "              hall_of_fame(df_comparison,label,1)\n",
        "              print(\"************************\")\n",
        "              hall_of_shame(df_comparison,label,1)\n",
        "              print(\"************************\")\n",
        "            for component in label_cols:\n",
        "              print(component)\n",
        "              print(\"% Predicted too high: \" + str(len(df_comparison.query(\"transformed_pred_\"+component+\">\"+component))/len(df_comparison)))\n",
        "              print(\"% Predicted correctly: \" + str(len(df_comparison.query(\"transformed_pred_\"+component+\"==\"+component))/len(df_comparison)))\n",
        "              print(\"% Predicted too low: \" + str(len(df_comparison.query(\"transformed_pred_\"+component+\"<\"+component))/len(df_comparison)))\n",
        "              print(\"****\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pk6ZkXlSjt5B"
      },
      "outputs": [],
      "source": [
        "kpi_col_list = ['model_name',\n",
        "                'iteration',\n",
        "                'epoch_val',\n",
        "                'batch_size_val',\n",
        "                'learning_rate_val',\n",
        "                'validation_split_val',\n",
        "                'dropout_val',\n",
        "                'number_of_hidden_layers_val',\n",
        "                'hidden_layer_node_count_val',\n",
        "                'retrain_layer_count_val',\n",
        "                'fold', \n",
        "                'train_loss', \n",
        "                'train_accuracy', \n",
        "                'val_loss', \n",
        "                'val_accuracy', \n",
        "                'test_loss', \n",
        "                'test_accuracy'\n",
        "               ]\n",
        "df_MCRMSE = pd.DataFrame(MCRMSE_list, columns = kpi_col_list)    \n",
        "df_MCRMSE.to_csv(\"kpi_stats_bertweet.csv\", index = False)\n",
        "df_MCRMSE    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDwRZh-7ENeh"
      },
      "outputs": [],
      "source": [
        "print(\"Average test accuracy and loss...\")\n",
        "df_MCRMSE.groupby(['model_name', 'iteration']).agg({'test_loss'      : [np.mean, np.min, np.max],  \n",
        "                                                    'test_accuracy'  : [np.mean, np.min, np.max] \n",
        "                                                   }\n",
        "                                                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix\n",
        "Other experiments with the hyper parameters"
      ],
      "metadata": {
        "id": "phqVvESQdeRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def regression_model_with_bert(num_classes=9,                  # [1, 1.5, 2, 2.5....4.5, 5]: 9 classes\n",
        "                               num_train_layers=0,\n",
        "                               num_hidden_layer=1,\n",
        "                               num_hidden_units=256,\n",
        "                               dropout=0.3,\n",
        "                               learning_rate=0.00005,\n",
        "                               activation = 'relu',\n",
        "                               optimizer='adam'):\n",
        "    \"\"\"\n",
        "    Build a simple regression model with BERT. Use the CLS Output for regression purposes.\n",
        "    \"\"\"\n",
        "    # =========== BEGIN generate \"input features\" using pre-trained model tokenizer ==================================\n",
        "    if num_train_layers == 0:\n",
        "        bert_model.trainable = False                 # Freeze all layers of pre-trained BERT model\n",
        "\n",
        "    elif num_train_layers == 12:         \n",
        "        bert_model.trainable = True                  # Train all layers of the BERT model\n",
        "\n",
        "    else:                                            # Restrict training to the num_train_layers outer transformer layers\n",
        "        retrain_layers = []\n",
        "        for retrain_layer_number in range(num_train_layers):\n",
        "            layer_code = '_' + str(11 - retrain_layer_number)\n",
        "            retrain_layers.append(layer_code) \n",
        "        # print('retrain layers: ', retrain_layers)\n",
        "\n",
        "        for w in bert_model.weights:\n",
        "            if not any([x in w.name for x in retrain_layers]):\n",
        "                #print('freezing: ', w)\n",
        "                w._trainable = False\n",
        "    \n",
        "    # Input Layer\n",
        "    input_ids = tf.keras.layers.Input(shape=(MAX_LENGTH), dtype=tf.int64, name='input_ids_layer')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(MAX_LENGTH), dtype=tf.int64, name='attention_mask_layer')\n",
        "\n",
        "    bert_inputs = {'input_ids': input_ids,\n",
        "                  'attention_mask': attention_mask\n",
        "                  }\n",
        "                      \n",
        "    # Bert output: being used as an input feature in the classification model below\n",
        "    bert_out = bert_model(bert_inputs)        # full features as an input to the following classification model\n",
        "    # pooler_output = bert_out[1]             # one vector for each\n",
        "    cls_token = bert_out[0][:, 0, :]          # give us a raw CLS tokens\n",
        "\n",
        "\n",
        "    layer_list = []\n",
        "    for hidden_layer_number in range(num_hidden_layer):\n",
        "        if hidden_layer_number == 0:\n",
        "            hidden_layer = tf.keras.layers.Dense(units = num_hidden_units\n",
        "                                        , activation = activation\n",
        "                                        , name = 'hidden_layer_' + str(hidden_layer_number + 1)\n",
        "                                        )(cls_token)\n",
        "        else:\n",
        "            hidden_layer = tf.keras.layers.Dense(units = num_hidden_units\n",
        "                                        , activation = activation\n",
        "                                        , name = 'hidden_layer_' + str(hidden_layer_number + 1)\n",
        "                                        )(layer_list[-1])\n",
        "        layer_list.append(hidden_layer)\n",
        "        dropout_layer = tf.keras.layers.Dropout(dropout, name = 'dropout_layer_' + str(hidden_layer_number + 1))(hidden_layer) \n",
        "        layer_list.append(dropout_layer)\n",
        "\n",
        "    output = tf.keras.layers.Dense(6,)(layer_list[-1])\n",
        "    regression_model = tf.keras.Model(inputs = [input_ids, attention_mask], outputs = output)\n",
        "\n",
        "    def selected_optimizer(optimizer):\n",
        "      if optimizer.lower() == 'sgd':\n",
        "        return SGD(learning_rate=learning_rate)           \n",
        "      elif optimizer.lower() == 'adam':\n",
        "        return Adam(learning_rate=learning_rate)          \n",
        "\n",
        "    regression_model.compile(optimizer = selected_optimizer(optimizer),\n",
        "                             loss=MCRMSE,\n",
        "                             metrics=MCRMSE) \n",
        "\n",
        "    return regression_model, count_params(regression_model.trainable_weights), count_params(regression_model.non_trainable_weights)\n",
        "def train_regression(model, batch_size, epochs):  \n",
        "  checkpoint_filepath = '/content/gdrive/MyDrive/Kaggle/Model_Checkpoint'         #  Create a new directory, Model_Checkpoint, in my Google Drive first and navigate the path here\n",
        "  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                                                  save_weights_only=True,\n",
        "                                                                  monitor='val_loss',\n",
        "                                                                  mode='min',\n",
        "                                                                  save_best_only=True)  \n",
        "  # The following parameters say: \"If there hasn't been at least an improvement of 0.001 in the validation loss over the previous 3 epochs, then stop the training and keep the best model you found.\"\n",
        "  early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', \n",
        "                                              min_delta=0.001, # minimium amount of change to count as an improvement\n",
        "                                              patience=3,      # how many epochs to wait before stopping\n",
        "                                              restore_best_weights=True)\n",
        "\n",
        "  print('Training Regression with BERT.....\\n====================================='  )\n",
        "  regression_model_history = model.fit([train_encodings.input_ids, \n",
        "                                        train_encodings.attention_mask\n",
        "                                        ], \n",
        "                                        y_train,   \n",
        "                                        validation_split = .1,\n",
        "                                        # validation_data =([val_encodings.input_ids, \n",
        "                                        #                     val_encodings.attention_mask], \n",
        "                                        #                   y_val\n",
        "                                        #                   ),    \n",
        "                                        batch_size = batch_size, \n",
        "                                        # callbacks=[callback, model_checkpoint_callback, tensorboard_callback],\n",
        "                                        callbacks=[early_stopping_callback, model_checkpoint_callback],\n",
        "                                        epochs = epochs \n",
        "                                        # verbose=0    # make output invisible\n",
        "                                        )    \n",
        "  df_regression_model_history = pd.DataFrame(regression_model_history.history)\n",
        "  display(df_regression_model_history.T)     \n",
        "  return df_regression_model_history\n",
        "def plot_loss_mcrmse(df, eval_metric):\n",
        "    x_arr = np.arange(len(df['loss'])) + 1\n",
        "    fig = plt.figure(figsize=(12, 4))\n",
        "    ax = fig.add_subplot(1, 2, 1)\n",
        "    ax.plot(x_arr, df['loss'], '-o', label = 'Train Loss')\n",
        "    ax.plot(x_arr, df['val_loss'], '--<', label = 'Validation Loss')\n",
        "    ax.legend(fontsize = 12)\n",
        "    ax.set_xlabel('Epoch', size = 12)\n",
        "    ax.set_ylabel('Loss', size = 12)\n",
        "\n",
        "    ax = fig.add_subplot(1, 2, 2)\n",
        "    ax.plot(x_arr, df[eval_metric], '-o', label = 'Train ' + eval_metric)\n",
        "    ax.plot(x_arr, df['val_' + eval_metric], '--<', label = 'Validation ' + eval_metric)\n",
        "    ax.legend(fontsize = 12)\n",
        "    ax.set_xlabel('Epoch', size = 12)\n",
        "    ax.set_ylabel('MCRMSE', size = 12)\n",
        "    #ax.set_ylim(0,1)\n",
        "    plt.show()\n",
        "def evaluate_test_labels(model):\n",
        "  score_regression = model.evaluate([test_encodings.input_ids, \n",
        "                                          test_encodings.attention_mask\n",
        "                                          ], \n",
        "                                          y_test\n",
        "                                          ) \n",
        "  print('\\nEvaluate Test Metrics:\\n=================================')\n",
        "  print('\\nTest loss: {:.4f}'.format(score_regression[0]))\n",
        "  print('\\nTest MCRMSE score: {:.4f}'.format(score_regression[1]),'\\n')\n",
        "  return score_regression\n",
        "def predict_test_labels(model):\n",
        "  predictions = model.predict([test_encodings.input_ids, test_encodings.attention_mask])    # -1 in reshape function is used when you don't know or want to explicitly tell the dimension of that axis.\n",
        "  df_pred = pd.DataFrame(predictions, columns=['pred_'+ col for col in label_cols])\n",
        "  return df_pred\n",
        "def scaled_pred(df):\n",
        "  pred_scaled = []\n",
        "  for col in df:\n",
        "    df[col + '_scaled'] = df[col].apply(lambda val: round(val/0.5) * 0.5)\n",
        "    pred_scaled.append(df[col + '_scaled'])\n",
        "  return pd.DataFrame(pred_scaled).T\n",
        "def run_regression_experiment(num_train_layers=0,\n",
        "                              num_hidden_layer=1,\n",
        "                              num_hidden_units=256,\n",
        "                              dropout=0.3,\n",
        "                              learning_rate=0.00005,\n",
        "                              batch_size=8,\n",
        "                              csv_filename='perf_summary_regression_w_BERT.csv',\n",
        "                              activation = 'relu',                                    # 'relu', 'leaky_relu', 'gelu'\n",
        "                              optimizer='adam',                                       # 'adam', 'sgd'\n",
        "                              epochs=1): ### UPDATE AT THE END\n",
        "  set_config_param(20230214)\n",
        "  df_perf_summary = pd.DataFrame()\n",
        "  \n",
        "  for layer in num_train_layers:  \n",
        "    print('\\n******************************************************')\n",
        "    print(f'Regression with BERT: Number of Unfrozen Layers = {layer}')\n",
        "    print('******************************************************\\n')\n",
        "\n",
        "    # build a regression model\n",
        "    regression_with_bert, num_trainable_params, num_non_trainable_params = regression_model_with_bert(num_classes = 9,                          # [1, 1.5, 2, 2.5....4.5, 5]: 9 classes\n",
        "                                                                                                      num_train_layers = layer,\n",
        "                                                                                                      num_hidden_layer = num_hidden_layer,\n",
        "                                                                                                      num_hidden_units = num_hidden_units,\n",
        "                                                                                                      dropout = dropout,\n",
        "                                                                                                      learning_rate = learning_rate,\n",
        "                                                                                                      activation = activation,\n",
        "                                                                                                      optimizer=optimizer)\n",
        "    # print(f'Parameter Values:\\n======================\\nnum_hidden_layer = {num_hidden_layer}\\nnum_hidden_units = {num_hidden_units}\\ndropout = {dropout}\\nlearning_rate = {learning_rate}\\nbatch_size = {batch_size}\\n')\n",
        "    \n",
        "    # model summary and plot model structure\n",
        "    display(regression_with_bert.summary())\n",
        "    display(keras.utils.plot_model(regression_with_bert, show_shapes=False, show_dtype=False, show_layer_names=True, dpi=90))\n",
        "\n",
        "    # train model\n",
        "    df_regression_model_history = train_regression(regression_with_bert, batch_size, epochs)\n",
        "    print(\"\\nPlotting loss and MCRMSE...\")\n",
        "    plot_loss_mcrmse(df_regression_model_history, 'MCRMSE')  \n",
        "    # print(\"\\nTensorBoard: Evolution of Loss and MCRMSE:\\n=============================================\")\n",
        "    # %tensorboard --logdir logs/fit\n",
        "\n",
        "    # Evaluate test set\n",
        "    score_regression = evaluate_test_labels(regression_with_bert)\n",
        "\n",
        "    # Predict test set\n",
        "    df_pred = predict_test_labels(regression_with_bert)\n",
        "    df_pred_scaled = scaled_pred(df_pred)\n",
        "    \n",
        "    # Create a final table with y_true, y_pred_raw, and y_pred_scaled\n",
        "    # display(generate_final_table(df_pred))\n",
        "\n",
        "    # ========== Performace metrics summary ===================================\n",
        "    perf_metrics = pd.DataFrame({'NLP Model':\"bert-base-cased\",\n",
        "                                'Num_Trainable_layers': layer,\n",
        "                                # 'Trainable_Params':  f'{num_trainable_params:,}',\n",
        "                                # 'Non-Trainable_Params':  f'{num_non_trainable_params:,}',\n",
        "                                'Epochs':epochs,                                                              \n",
        "                                'Test_MCRMSE':round(score_regression[1], 4), \n",
        "                                'Test_Loss':round(score_regression[0], 4), \n",
        "                                'Train_MCRMSE':round(df_regression_model_history.iloc[-1][1], 4), \n",
        "                                'Train_Loss':round(df_regression_model_history.iloc[-1][0], 4), \n",
        "                                'Val_MCRMSE':round(df_regression_model_history.iloc[-1][3], 4), \n",
        "                                'Val_Loss':round(df_regression_model_history.iloc[-1][2], 4),  \n",
        "                                'Optimizer': optimizer, \n",
        "                                'Activation': activation,  \n",
        "                                'Learning_Rate':learning_rate,                               \n",
        "                                'Num_Hidden_Layers':num_hidden_layer, \n",
        "                                'Num_hidden_Units':num_hidden_units,                                 \n",
        "                                'Dropout': dropout, \n",
        "                                'Batch_Size': batch_size}, index=[0])\n",
        "    df_perf_summary = df_perf_summary.append(perf_metrics)\n",
        "  df_perf_summary.to_csv(csv_filename, index=False)\n",
        "  display(df_perf_summary.reset_index(drop=True))\n",
        "def generate_final_table(df_pred):\n",
        "  print('\\nFinal Table: y_true vs. y_pred_raw vs. y_pred_scaled\\n======================================================')\n",
        "  df_final = pd.concat([df_test[['full_text']].reset_index(drop=True), df_test[label_cols].reset_index(drop=True), df_pred], axis=1)\n",
        "  display(df_final)\n",
        "  return df_final\n",
        "def run_regression_experiment_1(num_train_layers=0,\n",
        "                              num_hidden_layer=1,\n",
        "                              num_hidden_units=256,\n",
        "                              dropout=0.3,\n",
        "                              learning_rate=0.00005,\n",
        "                              batch_size=8,\n",
        "                              csv_filename='perf_summary_regression_w_BERT.csv',\n",
        "                              activation = 'relu',                                    # 'relu', 'leaky_relu', 'gelu'\n",
        "                              optimizer='adam',                                       # 'adam', 'sgd'\n",
        "                              epochs=10):\n",
        "\n",
        "  # df_perf_summary = pd.DataFrame()\n",
        "  # for layer in num_train_layers:  \n",
        "  print('\\n******************************************************')\n",
        "  print(f'Regression with BERT: Number of Unfrozen Layers = {num_train_layers}')\n",
        "  print('******************************************************\\n')\n",
        "\n",
        "\n",
        "  # build a regression model\n",
        "  regression_with_bert, num_trainable_params, num_non_trainable_params = regression_model_with_bert(num_classes = 9,                          # [1, 1.5, 2, 2.5....4.5, 5]: 9 classes\n",
        "                                                                                                    num_train_layers = num_train_layers,\n",
        "                                                                                                    num_hidden_layer = num_hidden_layer,\n",
        "                                                                                                    num_hidden_units = num_hidden_units,\n",
        "                                                                                                    dropout = dropout,\n",
        "                                                                                                    learning_rate = learning_rate,\n",
        "                                                                                                    activation = activation,\n",
        "                                                                                                    optimizer=optimizer)\n",
        "  # print(f'Parameter Values:\\n======================\\nnum_hidden_layer = {num_hidden_layer}\\nnum_hidden_units = {num_hidden_units}\\ndropout = {dropout}\\nlearning_rate = {learning_rate}\\nbatch_size = {batch_size}\\n')\n",
        "  \n",
        "  # model summary and plot model structure\n",
        "  display(regression_with_bert.summary())\n",
        "  display(keras.utils.plot_model(regression_with_bert, show_shapes=False, show_dtype=False, show_layer_names=True, dpi=90))\n",
        "\n",
        "  # train model\n",
        "  df_regression_model_history = train_regression(regression_with_bert, batch_size, epochs)\n",
        "  print(\"\\nPlotting loss and MCRMSE...\")\n",
        "  plot_loss_mcrmse(df_regression_model_history, 'MCRMSE')  \n",
        "  # print(\"\\nTensorBoard: Evolution of Loss and MCRMSE:\\n=============================================\")\n",
        "  # %tensorboard --logdir logs/fit\n",
        "\n",
        "  # Evaluate test set\n",
        "  score_regression = evaluate_test_labels(regression_with_bert)\n",
        "\n",
        "  # Predict test set\n",
        "  df_pred = predict_test_labels(regression_with_bert)\n",
        "  df_pred_scaled = scaled_pred(df_pred)\n",
        "  df_pred.to_csv('df_pred.csv', index=False)\n",
        "  \n",
        "  # Create a final table with y_true, y_pred_raw, and y_pred_scaled\n",
        "  df_final = generate_final_table(df_pred)\n",
        "  display(generate_final_table(df_pred))\n",
        "  df_final.to_csv('df_final.csv', index=False)\n",
        "\n",
        "  # ========== Performace metrics summary ===================================\n",
        "  perf_metrics = pd.DataFrame({'NLP Model':\"bert-base-cased\",\n",
        "                              'Num_Trainable_layers': num_train_layers,\n",
        "                              # 'Trainable_Params':  f'{num_trainable_params:,}',\n",
        "                              # 'Non-Trainable_Params':  f'{num_non_trainable_params:,}',\n",
        "                              'Epochs':epochs,                                                              \n",
        "                              'Test_MCRMSE':round(score_regression[1], 4), \n",
        "                              'Test_Loss':round(score_regression[0], 4), \n",
        "                              'Train_MCRMSE':round(df_regression_model_history.iloc[-1][1], 4), \n",
        "                              'Train_Loss':round(df_regression_model_history.iloc[-1][0], 4), \n",
        "                              'Val_MCRMSE':round(df_regression_model_history.iloc[-1][3], 4), \n",
        "                              'Val_Loss':round(df_regression_model_history.iloc[-1][2], 4),  \n",
        "                              'Optimizer': optimizer, \n",
        "                              'Activation': activation,  \n",
        "                              'Learning_Rate':learning_rate,                               \n",
        "                              'Num_Hidden_Layers':num_hidden_layer, \n",
        "                              'Num_hidden_Units':num_hidden_units,                                 \n",
        "                              'Dropout': dropout, \n",
        "                              'Batch_Size': batch_size}, index=[0])\n",
        "    # df_perf_summary = df_perf_summary.append(perf_metrics)\n",
        "  perf_metrics.to_csv(csv_filename, index=False)\n",
        "  display(perf_metrics.reset_index(drop=True))"
      ],
      "metadata": {
        "id": "X6fxpQJFlDH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_regression_experiment(num_train_layers=np.arange(0,13,6),\n",
        "                          activation='relu',\n",
        "                          optimizer='adam',                          \n",
        "                          csv_filename='perf_summary_regression_w_BERT_1.csv')"
      ],
      "metadata": {
        "id": "6PZfOmUGddGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_regression_experiment(num_train_layers=np.arange(0,13,2),\n",
        "                          num_hidden_layer=2,\n",
        "                          num_hidden_units=64,\n",
        "                          dropout=0.1,\n",
        "                          learning_rate=0.00001,\n",
        "                          batch_size=16,\n",
        "                          csv_filename='perf_summary_regression_w_BERT_2.csv',\n",
        "                          activation='relu',\n",
        "                          optimizer='adam',\n",
        "                          epochs=10)"
      ],
      "metadata": {
        "id": "akVSmP58v_We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_regression_experiment(num_train_layers=np.arange(0,13,2),\n",
        "                          csv_filename='perf_summary_regression_w_BERT_5.csv')"
      ],
      "metadata": {
        "id": "5PeAFzQhv_eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_regression_experiment(num_train_layers=np.arange(0,13,6),\n",
        "                          # num_hidden_layer=1,\n",
        "                          # num_hidden_units=256,\n",
        "                          # dropout=0.3,\n",
        "                          # learning_rate=0.00005,\n",
        "                          batch_size=16,\n",
        "                          activation='relu',\n",
        "                          optimizer='adam',\n",
        "                          csv_filename='perf_summary_regression_w_BERT_4.csv')"
      ],
      "metadata": {
        "id": "ADKyWC2Dv_lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_regression_experiment(num_train_layers=np.arange(0,13,6),\n",
        "                          # num_hidden_layer=1,\n",
        "                          # num_hidden_units=256,\n",
        "                          # dropout=0.3,\n",
        "                          # learning_rate=0.00005,\n",
        "                          batch_size=16,\n",
        "                          activation='gelu',\n",
        "                          optimizer='adam',\n",
        "                          csv_filename='perf_summary_regression_w_BERT_batch16_gelu.csv')"
      ],
      "metadata": {
        "id": "IQkEBRrEv5mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_regression_experiment(num_train_layers=np.arange(0,7,2),\n",
        "                          # num_hidden_layer=1,\n",
        "                          # num_hidden_units=256,\n",
        "                          # dropout=0.3,\n",
        "                          # learning_rate=0.00005,\n",
        "                          batch_size=16,\n",
        "                          activation='leaky_relu',\n",
        "                          optimizer='adam',\n",
        "                          csv_filename='perf_summary_regression_w_BERT_batch16_leakyRelu.csv')"
      ],
      "metadata": {
        "id": "9KsoMwRRv5pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_regression_experiment(num_train_layers=np.arange(2,5,2),\n",
        "# run_regression_experiment(num_train_layers=np.arange(0,9,2),\n",
        "                          # num_hidden_layer=1,\n",
        "                          # num_hidden_units=256,\n",
        "                          dropout=0.1,\n",
        "                          # learning_rate=0.00005,\n",
        "                          batch_size=16,\n",
        "                          activation='relu',\n",
        "                          optimizer='adam',\n",
        "                          csv_filename='perf_summary_regression_w_BERT_batch16_leakyRelu.csv')"
      ],
      "metadata": {
        "id": "h2-i0Bccv5wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_regression_experiment(num_train_layers=np.arange(0,7,2),\n",
        "                          # num_hidden_layer=1,\n",
        "                          # num_hidden_units=256,\n",
        "                          dropout=0.2,\n",
        "                          # learning_rate=0.00005,\n",
        "                          batch_size=16,\n",
        "                          activation='leaky_relu',\n",
        "                          optimizer='adam',\n",
        "                          csv_filename='perf_summary_regression_w_BERT_batch16_leakyRelu_sgd.csv')"
      ],
      "metadata": {
        "id": "dp1zCoQTvxAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " run_regression_experiment(num_train_layers=np.arange(0,7,2),\n",
        "                          # num_hidden_layer=1,\n",
        "                          # num_hidden_units=256,\n",
        "                          dropout=0.2,\n",
        "                          learning_rate=0.0001,\n",
        "                          batch_size=16,\n",
        "                          activation='leaky_relu',\n",
        "                          optimizer='adam',\n",
        "                          csv_filename='perf_summary_regression_w_BERT_batch16_leakyRelu_lowerLR.csv')"
      ],
      "metadata": {
        "id": "FieQsIpzvxC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_regression_experiment(num_train_layers=np.arange(0,13,4),\n",
        "                          # num_hidden_layer=1,\n",
        "                          num_hidden_units=128,\n",
        "                          dropout=0.2,\n",
        "                          learning_rate=0.0005,\n",
        "                          batch_size=16,\n",
        "                          activation='leaky_relu',\n",
        "                          optimizer='adam',\n",
        "                          csv_filename='perf_summary_regression_w_BERT_batch16_leakyRelu_lowerLR2.csv')"
      ],
      "metadata": {
        "id": "N5_xlwdavxFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " run_regression_experiment(num_train_layers=np.arange(0,13,4),\n",
        "                          # num_hidden_layer=1,\n",
        "                          num_hidden_units=64,\n",
        "                          dropout=0.2,\n",
        "                          learning_rate=0.0005,\n",
        "                          batch_size=16,\n",
        "                          activation='leaky_relu',\n",
        "                          optimizer='adam',\n",
        "                          csv_filename='perf_summary_regression_w_BERT_batch16_leakyRelu_lowerLR128.csv')"
      ],
      "metadata": {
        "id": "drc4AOITvxIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_regression_experiment_1(num_train_layers=12,\n",
        "                          num_hidden_layer=2,\n",
        "                          num_hidden_units=64,\n",
        "                          dropout=0.1,\n",
        "                          learning_rate=0.00001,\n",
        "                          batch_size=16,\n",
        "                          csv_filename='perf_summary_regression_w_BERT_final1.csv',\n",
        "                          activation='relu',\n",
        "                          optimizer='adam',\n",
        "                          epochs=10)"
      ],
      "metadata": {
        "id": "BDn1FN-ZvxLD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FYfihW3Mjgkf",
        "6yYB47Gdjo0L",
        "dII35P-P_J_t",
        "k3y3byq0Fk9P",
        "N73w6RpFsbPx",
        "1kbjNbnOZGJi"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}