{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7KEXkejkI7tk",
        "sjrm1a84QMVM",
        "3fpxoKBywieJ",
        "_KeKQmlyxvNm",
        "0RxLdSc1yYOj",
        "MXmeI1vhz7KC",
        "rv7WCZF31zg6",
        "fS2RzaGi2zS-",
        "ni3sz9wxa1nR",
        "NUMZkNvJcufY",
        "kndLU2phLRns",
        "LbgDBdUTFIa7",
        "hhWJgI2QONaS",
        "A1TQQfdIPNn0",
        "FQzHk9gJTpGF",
        "QTjC60_xP1gv",
        "KXwT00dRHxGN",
        "R-EOY4qnIcUW",
        "FPKsmw8oIlLy",
        "Lm57jeEsN-Vf",
        "v1l2yTGnKzsy",
        "wAW3Z-BxX9qi",
        "kdda-LBvO-Qd",
        "_AGO4SrlYcZQ",
        "Blag-pumPIgu",
        "gEfySDbNOTJN",
        "4e-8H751RROi",
        "rcB_ojpNXh4r",
        "x08cEWvzKdEF",
        "C-wgt-ouU3it",
        "bILNga0wabjS"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilamaiti/spring_2023_w266_final_project_heesuk_iris_srila/blob/main/srila/ELL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install New Libraries**"
      ],
      "metadata": {
        "id": "SITfG-PPCctj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qOq60doCAwZ",
        "outputId": "c2b0d056-901d-4996-a8d2-7391a6bcf7b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.9/dist-packages (1.8.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from wordcloud) (8.4.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.9/dist-packages (from wordcloud) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (1.4.4)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (5.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (23.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (1.0.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (4.39.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->wordcloud) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install wordcloud\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Libraries**"
      ],
      "metadata": {
        "id": "KvNGnUjZCg62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NLP related libraries\n",
        "import transformers\n",
        "print(f'transformers version: {transformers.__version__}')\n",
        "from transformers import logging as hf_logging\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "hf_logging.set_verbosity_error()\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import spacy      \n",
        "from spacy import displacy\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS\n",
        "from wordcloud import ImageColorGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Other required libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import copy\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Data visualization libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tensorflow libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils.layer_utils import count_params\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.losses import mae\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from keras.models import Model\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.models import load_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "rTT4UBIqCRxT",
        "outputId": "4a45f903-5b19-46f9-d9d5-9c3955dc446e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers version: 4.27.1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1125\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mtokenization_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_is_control\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_is_punctuation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_is_whitespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m from .tokenization_utils_base import (\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mENCODE_KWARGS_DOCSTRING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_tokenizers_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtokenizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAddedToken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtokenizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoding\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mEncodingFast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AddedToken' from 'tokenizers' (unknown location)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-1ab26f489d50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'transformers version: {transformers.__version__}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhf_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFBertModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mhf_logging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module {self.__name__} has no attribute {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1128\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1129\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.tokenization_bert because of the following error (look up to see its traceback):\ncannot import name 'AddedToken' from 'tokenizers' (unknown location)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Functions**"
      ],
      "metadata": {
        "id": "CM7dVhGpQF-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set Config Parameters**"
      ],
      "metadata": {
        "id": "7KEXkejkI7tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_config_param(seed = 99):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    tf.keras.backend.clear_session()\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/Kaggle\"\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    \n",
        "    \n",
        "set_config_param(20230214)"
      ],
      "metadata": {
        "id": "52YR9iGODH8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Count Plot**"
      ],
      "metadata": {
        "id": "sjrm1a84QMVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_count(df, labels):\n",
        "  sns.set_style('whitegrid')\n",
        "  plt.figure(figsize=(18,10))\n",
        "  for idx, label in enumerate(labels):\n",
        "      plt.subplot(2, 3, idx+1)\n",
        "      sns.countplot(x = label, data = df)"
      ],
      "metadata": {
        "id": "E-XKyBFhQO3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Adding Feature Columns**"
      ],
      "metadata": {
        "id": "3fpxoKBywieJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_feature(df):\n",
        "\n",
        "    # Cleaning up full_text : Removing tabl and carriage return characters\n",
        "    df['full_text'] = df[\"full_text\"].replace(re.compile(r'[\\n\\r\\t]'), ' ', regex=True)\n",
        "\n",
        "    # Adding word count, sentence count, total score and full text length\n",
        "    df['word_count'] = df['full_text'].apply(lambda x: len(x.split()))\n",
        "    df['sentence_count'] = df['full_text'].apply(lambda x: len(sent_tokenize(x)))\n",
        "    df['total_score'] = df['cohesion'] + df_train['syntax'] + df['vocabulary'] + df['phraseology'] + df['grammar'] + df['conventions']\n",
        "    df['full_text_len'] = df['full_text'].apply(lambda x: len(x))\n",
        "\n",
        "    # Adding mean, median score per label and indicator column \n",
        "    # whether the label value is below or above mean or median value\n",
        "    for label in label_cols:\n",
        "        df[label + '_avg_score'] = np.mean(df[label])\n",
        "        df[label + '_above_or_below_avg_flag'] = np.where(df[label] > np.mean(df[label]), 1, 0)  \n",
        "        df[label + '_median_score'] = np.median(df[label])\n",
        "        df[label + '_above_or_below_median_flag'] = np.where(df[label] > np.median(df[label]), 1, 0)  \n",
        "        df[label + '_rounded_val'] = np.round(df[label])  \n",
        "    \n",
        "    return df"
      ],
      "metadata": {
        "id": "ZuDGICk6wlrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Getting Unique Values for Each Label**"
      ],
      "metadata": {
        "id": "_KeKQmlyxvNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unique_values_for_labels(df, col_list):\n",
        "    print('Unique Values in Each Metric:\\n==================================================')\n",
        "    for col in col_list:\n",
        "        print(f'{col}: {df[col].unique()}')"
      ],
      "metadata": {
        "id": "dmV_f0MpxzcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Getting Value Counts for Each Label**"
      ],
      "metadata": {
        "id": "0RxLdSc1yYOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_value_counts_for_labels(df, col_list):\n",
        "    print('Counts for Each Metric:\\n==================================================')\n",
        "    for col in col_list:\n",
        "        print(f\"Column: {col}\")\n",
        "        print(f'{df[col].value_counts().sort_values()}')\n",
        "        print(\"*****\")"
      ],
      "metadata": {
        "id": "yX2dg3dFydT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Getting Histogram for a Column**"
      ],
      "metadata": {
        "id": "MXmeI1vhz7KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_histogram_for_col(df, col_list, col_name):\n",
        "    sns.set_style('whitegrid')\n",
        "    plt.figure(figsize=(18,10))\n",
        "    for idx, label in enumerate(col_list):\n",
        "        plt.subplot(2, 3, idx+1)\n",
        "        sns.histplot(x=col_name, hue = label, data = df)\n",
        "        plt.xlabel('Word Count', fontsize=10)"
      ],
      "metadata": {
        "id": "a3jN8_D-z-32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Getting Count Plot for Indicator Columns**"
      ],
      "metadata": {
        "id": "rv7WCZF31zg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_count_plot_for_ind_col(df, search_string, col_list):\n",
        "    fig, ax = plt.subplots(1, len(col_list), figsize=(50,10))\n",
        "    for idx, label in enumerate(col_list):\n",
        "        sns.countplot(x = df[label + search_string], ax = ax[idx])\n",
        "        ax[idx].set_title(label)"
      ],
      "metadata": {
        "id": "zZ6f_gYB1-BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Getting Bar Plot for Each Label**"
      ],
      "metadata": {
        "id": "fS2RzaGi2zS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_barplot_for_each_label(df, col_list, search_string):\n",
        "    avg_score_cols = [col + search_string for col in col_list]\n",
        "    plt.figure(figsize=(8,5))\n",
        "    ax = sns.barplot(x=col_list, y=np.array(df[avg_score_cols].drop_duplicates())[0], palette='rocket')\n",
        "    plt.xlabel('Scoring Metric', fontsize=12)\n",
        "    if search_string == '_avg_score':\n",
        "        plt.ylabel('Average Score', fontsize=12)\n",
        "        plt.title('Average Score in Each Metric', fontsize=16)\n",
        "    elif search_string == '_median_score':\n",
        "        plt.ylabel('Median Score', fontsize=12)\n",
        "        plt.title('Median Score in Each Metric', fontsize=16)"
      ],
      "metadata": {
        "id": "uheekcFB223E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Label Processing**"
      ],
      "metadata": {
        "id": "ni3sz9wxa1nR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cat_label_cols(col_list):\n",
        "    return ['cat_' + col for col in col_list]"
      ],
      "metadata": {
        "id": "Q0wGK-_1cIpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_label_map(df, label_map, col_list):\n",
        "  for col in col_list:\n",
        "      df[col + '_map'] = df[col].map(label_map)\n",
        "  return df"
      ],
      "metadata": {
        "id": "rMwmH1dUfOLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label_values(df, col_list):\n",
        "    return [np.array(df[col]) for col in col_list]"
      ],
      "metadata": {
        "id": "yvaG7kEHpI7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label_dict(df, col_list_1, col_list_2):\n",
        "    return dict(zip(col_list_2, get_label_values(df, col_list_1)))"
      ],
      "metadata": {
        "id": "UPdesUBqpLW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plot Loss and Accuracy**"
      ],
      "metadata": {
        "id": "NHXFA4eNQNFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_accuracy(history, col_list):\n",
        "    fig, ax = plt.subplots(2, 6, figsize=(16, 6), sharex='col', sharey='row')\n",
        "    fig.tight_layout(pad=5.0)\n",
        "    for idx, col in enumerate(col_list):\n",
        "\n",
        "        ax[0, idx].plot(history[col + '_loss'], lw=2, color='darkgoldenrod')\n",
        "        ax[0, idx].plot(history['val_' + col + '_loss'], lw=2, color='indianred')\n",
        "        #ax[0, idx].legend(loc='center left')\n",
        "        ax[0, idx].legend(['Train', 'Validation'], fontsize=5)\n",
        "        ax[0, idx].set_xlabel('Epochs', size=10)\n",
        "        ax[0, idx].set_title('Loss: ' + col)\n",
        "\n",
        "        ax[1, idx].plot(history[col + '_accuracy'], lw=2, color='darkgoldenrod')\n",
        "        ax[1, idx].plot(history['val_' + col + '_accuracy'], lw=2, color='indianred')\n",
        "        #ax[0, idx].legend(loc='center left')\n",
        "        ax[1, idx].legend(['Train', 'Validation'], fontsize=5)\n",
        "        ax[1, idx].set_xlabel('Epochs', size=10)\n",
        "        ax[1, idx].set_title('Accuracy: ' + col)"
      ],
      "metadata": {
        "id": "kAnS-9wCQP80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read Input Files**"
      ],
      "metadata": {
        "id": "MdKA5le7FE3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_train_df = pd.read_csv('train.csv')\n",
        "input_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "float_labels, int_map_labels = np.arange(1, 5.5, 0.5), np.arange(9)\n",
        "label_map = dict(zip(float_labels, int_map_labels))\n",
        "\n",
        "float_scaled_labels, int_scaled_labels = np.arange(1, 6, 1), np.arange(6)\n",
        "label_scaled_map = dict(zip(float_scaled_labels, int_scaled_labels))\n",
        "\n",
        "orig_train_df = copy.deepcopy(input_train_df)\n",
        "orig_train_df.head()"
      ],
      "metadata": {
        "id": "Hyh8MQ_0DH_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Map\" columns are basically scaled columns of the original metric values. There are total 9 levels in map columns. Original mretric columns range from 1 to 5. Through map columns, they range from 0 to 8.\n",
        "\n",
        "\"Scaled\" columns map numbers .5, 1.5, 2.5, 3.5 and 4.5 to nearset integers. Thus it will have range from 1 to 5."
      ],
      "metadata": {
        "id": "BWvHPTt5UI7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_map"
      ],
      "metadata": {
        "id": "GOY_-Ooc_dCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_scaled_map"
      ],
      "metadata": {
        "id": "sat02ZKdTliA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Spliting the Data**\n",
        "\n",
        "Original test data is very limited, there are only 3 records and it does not have labels to test. So we decided to repurpose the given train data to split into train, test and validation sets."
      ],
      "metadata": {
        "id": "NUMZkNvJcufY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle = np.random.permutation(np.arange(orig_train_df.shape[0]))\n",
        "orig_train_df = orig_train_df.iloc[shuffle]\n",
        "split=(0.8,0.1,0.1)\n",
        "splits = np.multiply(len(orig_train_df), split).astype(int)\n",
        "df_train, df_val, df_test = np.split(orig_train_df, [splits[0], splits[0] + splits[1]])"
      ],
      "metadata": {
        "id": "Z7ELuUWlcxAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_cols = df_train.columns[2:]\n",
        "label_rounded_cols = [col + '_rounded_val' for col in label_cols]\n",
        "label_map_cols = [col + '_map' for col in label_cols]\n",
        "cat_label_cols = get_cat_label_cols(label_cols)\n",
        "\n",
        "df_train = apply_label_map(df_train, label_map, label_cols)\n",
        "df_test = apply_label_map(df_test, label_map, label_cols)\n",
        "df_val = apply_label_map(df_val, label_map, label_cols)"
      ],
      "metadata": {
        "id": "iDAM32Xagb7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_cols, label_rounded_cols, label_map_cols, cat_label_cols"
      ],
      "metadata": {
        "id": "O42263iZjHQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Addiung Other Feature Columns**"
      ],
      "metadata": {
        "id": "kndLU2phLRns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = add_feature(df_train)\n",
        "df_test = add_feature(df_test)\n",
        "df_val = add_feature(df_val)"
      ],
      "metadata": {
        "id": "qqe2GSfzLY1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EDA**"
      ],
      "metadata": {
        "id": "LbgDBdUTFIa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "01k0v_RODICu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in label_cols:\n",
        "    print(df_train[col + '_rounded_val'].unique())"
      ],
      "metadata": {
        "id": "P0wTM1KR4IWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.iloc[0]"
      ],
      "metadata": {
        "id": "T3y2j7by33P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "id": "2JBKHMhFDIGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_val.head()"
      ],
      "metadata": {
        "id": "m_aKUdVEdf-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape, df_test.shape, df_val.shape"
      ],
      "metadata": {
        "id": "pDxgsliNCR0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.describe()"
      ],
      "metadata": {
        "id": "FtENTOWsCR66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.columns"
      ],
      "metadata": {
        "id": "eO3UOIBgfvef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Unique Values for Each Label**"
      ],
      "metadata": {
        "id": "hhWJgI2QONaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_unique_values_for_labels(df_train, label_cols)"
      ],
      "metadata": {
        "id": "iS7iZnEIHpOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Unique Values for Each Rounded Label**"
      ],
      "metadata": {
        "id": "A1TQQfdIPNn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_unique_values_for_labels(df_train, label_rounded_cols)"
      ],
      "metadata": {
        "id": "3DWAOVw49kZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Value Counts for Each Label**"
      ],
      "metadata": {
        "id": "FQzHk9gJTpGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_value_counts_for_labels(df_train, label_cols)"
      ],
      "metadata": {
        "id": "vEOP_FvYTtL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_count(df_train, label_cols)"
      ],
      "metadata": {
        "id": "prGSzmkPZ3gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Value Counts for Each Rounded Label**"
      ],
      "metadata": {
        "id": "QTjC60_xP1gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_value_counts_for_labels(df_train, label_rounded_cols)"
      ],
      "metadata": {
        "id": "Pldhpwbx9o30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_count(df_train, label_rounded_cols)"
      ],
      "metadata": {
        "id": "KIaZwxVU9vqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Full Text Length Stats**"
      ],
      "metadata": {
        "id": "KXwT00dRHxGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.full_text.str.len().describe()"
      ],
      "metadata": {
        "id": "xmi1gFYbHsGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Count Stats**"
      ],
      "metadata": {
        "id": "R-EOY4qnIcUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.word_count.describe()"
      ],
      "metadata": {
        "id": "qvE2zoXjIfyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentence Count Stats**"
      ],
      "metadata": {
        "id": "FPKsmw8oIlLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.sentence_count.describe()"
      ],
      "metadata": {
        "id": "IDyYyghnIoW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[df_train.sentence_count == 1][['full_text']]"
      ],
      "metadata": {
        "id": "ZbPqbB4DIyEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualization**"
      ],
      "metadata": {
        "id": "oYdTWC6zAFzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Corelation Matrix of the Label Columns**"
      ],
      "metadata": {
        "id": "Lm57jeEsN-Vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr = df_train[label_cols].corr()\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(corr, dtype = bool))\n",
        "\n",
        "sns.set(rc = {\"figure.figsize\": (10, 8)})\n",
        "\n",
        "sns.heatmap(corr, \n",
        "            annot = True, \n",
        "            cmap = \"coolwarm\", \n",
        "            mask = mask,\n",
        "            fmt  = \".5f\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OsNghRH8GBHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Essay Length, Word Count, Total Score, Sentence Count Distribution**\n",
        "\n",
        "Essay length, word count and sentence count diostributions have normal shape, though left skewed. Total score distribution looks bi-modal."
      ],
      "metadata": {
        "id": "v1l2yTGnKzsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "\n",
        "plt.subplot(1,4,1)\n",
        "sns.histplot(data=df_train, x='word_count', kde=True)\n",
        "plt.axvline(x=df_train['word_count'].mean(),color='red')\n",
        "plt.axvline(x=df_train['word_count'].median(),color='black')\n",
        "plt.xlabel('Word Count Distribution',fontsize=12)\n",
        "plt.title('Word Count Distribution',fontsize=16)\n",
        "\n",
        "plt.subplot(1,4,2)\n",
        "sns.histplot(data=df_train, x='full_text_len', kde=True)\n",
        "plt.axvline(x=df_train['full_text_len'].mean(),color='red')\n",
        "plt.axvline(x=df_train['full_text_len'].median(),color='black')\n",
        "plt.xlabel('Full Text Length Distribution',fontsize=12)\n",
        "plt.title('Full Text Length Distribution',fontsize=16)\n",
        "\n",
        "plt.subplot(1,4,3)\n",
        "sns.histplot(data=df_train, x='total_score', kde=True)\n",
        "plt.axvline(x=df_train['total_score'].mean(),color='red')\n",
        "plt.axvline(x=df_train['total_score'].median(),color='black')\n",
        "plt.xlabel('Total Score Distribution',fontsize=12)\n",
        "plt.title('Total Score Distribution',fontsize=16)\n",
        "\n",
        "plt.subplot(1,4,4)\n",
        "sns.histplot(data=df_train, x='sentence_count', kde=True)\n",
        "plt.axvline(x=df_train['sentence_count'].mean(),color='red')\n",
        "plt.axvline(x=df_train['sentence_count'].median(),color='black')\n",
        "plt.xlabel('Sentence Count Distribution',fontsize=12)\n",
        "plt.title('Sentence Count Distribution',fontsize=16)"
      ],
      "metadata": {
        "id": "jMSfYQqLK3UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Count Vs Individual Label Scores**"
      ],
      "metadata": {
        "id": "wAW3Z-BxX9qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_histogram_for_col(df_train, label_cols, 'word_count')"
      ],
      "metadata": {
        "id": "uDO43g-fzqqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Count Vs Individual Rounded Label Scores**"
      ],
      "metadata": {
        "id": "kdda-LBvO-Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_histogram_for_col(df_train, label_rounded_cols, 'word_count')"
      ],
      "metadata": {
        "id": "N4orWBDq0ERo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentence Count Vs Individual Label Scores**"
      ],
      "metadata": {
        "id": "_AGO4SrlYcZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_histogram_for_col(df_train, label_cols, 'sentence_count')"
      ],
      "metadata": {
        "id": "vWBjXi0R0MUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentence Count Vs Individual Rounded Label Scores**"
      ],
      "metadata": {
        "id": "Blag-pumPIgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_histogram_for_col(df_train, label_rounded_cols, 'sentence_count')"
      ],
      "metadata": {
        "id": "un2OywUX9Zkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Distribution of Labels Above and Below Average in the Respective Category**"
      ],
      "metadata": {
        "id": "gEfySDbNOTJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_count_plot_for_ind_col(df_train, '_above_or_below_avg_flag', label_cols)"
      ],
      "metadata": {
        "id": "J0ilIF_dE7-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Distribution of Labels Above and Below Median in the Respective Category**"
      ],
      "metadata": {
        "id": "4e-8H751RROi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_count_plot_for_ind_col(df_train, '_above_or_below_median_flag', label_cols)"
      ],
      "metadata": {
        "id": "NWcHq1tTRUwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For all the labels we see that most of the label values are below average and median values."
      ],
      "metadata": {
        "id": "zOZHsj_VRlg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Distribution of Labels**"
      ],
      "metadata": {
        "id": "rcB_ojpNXh4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, len(label_cols), figsize=(40,10))\n",
        "\n",
        "for idx, label in enumerate(label_cols):\n",
        "    sns.distplot(x = df_train[label],\n",
        "                 ax = ax[idx]\n",
        "                )\n",
        "    ax[idx].set_title(label)\n",
        "    #plt.show(block = False)"
      ],
      "metadata": {
        "id": "qs86knBLIEOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Average Score Per Label**"
      ],
      "metadata": {
        "id": "x08cEWvzKdEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_barplot_for_each_label(df_train, label_cols, '_avg_score')"
      ],
      "metadata": {
        "id": "dO58Qh1VE8B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Median Score Per Label**"
      ],
      "metadata": {
        "id": "C-wgt-ouU3it"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_barplot_for_each_label(df_train, label_cols, '_median_score')"
      ],
      "metadata": {
        "id": "VsGe1C0bWUGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Most Frequent Words**"
      ],
      "metadata": {
        "id": "bILNga0wabjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = df_train[df_train[label_cols].sum(axis=1)==30]['full_text'].values[0]\n",
        "word_cloud = WordCloud(stopwords=STOPWORDS, colormap='Pastel1', collocations=False, width=1200, height=700, background_color = \"black\").generate(text)\n",
        "plt.figure(figsize=(20,8))\n",
        "plt.imshow(word_cloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "     "
      ],
      "metadata": {
        "id": "gk6gXM4_E8E2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Building**"
      ],
      "metadata": {
        "id": "Tkyi4qkae3pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make it easier to use a variety of BERT subword models\n",
        "model_checkpoint = 'bert-base-cased'   # case sensitive (care about upper and lower case)\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(model_checkpoint)\n",
        "MAX_LENGTH = 512"
      ],
      "metadata": {
        "id": "prqm6UhKE8H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generate Input Embeddings - Train/Validation/Test Set**"
      ],
      "metadata": {
        "id": "Tf6FATfjfVOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test = df_train['full_text'], df_val['full_text'], df_test['full_text']\n",
        "y_train, y_val, y_test = np.array(df_train[label_cols]), np.array(df_val[label_cols]), np.array(df_test[label_cols])\n",
        "\n",
        "# Handling map columns, this maintains the same number of levels as present in the original kpi sets.\n",
        "y_train_map, y_val_map, y_test_map = np.array(df_train[label_map_cols]), np.array(df_val[label_map_cols]), np.array(df_test[label_map_cols])\n",
        "y_train_map_combined = get_label_dict(df_train, label_map_cols, cat_label_cols)\n",
        "y_test_map_combined = get_label_dict(df_test, label_map_cols, cat_label_cols)\n",
        "y_val_map_combined = get_label_dict(df_val, label_map_cols, cat_label_cols)\n",
        "\n",
        "# Handling scaled values. Here we are converting the decimal values to nearest integers.\n",
        "# Thus .5, 1.5, 2.5, 3.5 and 4.5 map to 1, 2, 3, 4 and 5 respectively.\n",
        "y_train_scaled, y_val_scaled, y_test_scaled = np.array(df_train[label_rounded_cols]), np.array(df_val[label_rounded_cols]), np.array(df_test[label_rounded_cols])\n",
        "y_train_scaled_combined = get_label_dict(df_train, label_rounded_cols, cat_label_cols)\n",
        "y_test_scaled_combined = get_label_dict(df_test, label_rounded_cols, cat_label_cols)\n",
        "y_val_scaled_combined = get_label_dict(df_val, label_rounded_cols, cat_label_cols)"
      ],
      "metadata": {
        "id": "bZSjgsFgfhNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_scaled_combined"
      ],
      "metadata": {
        "id": "Hz38KR6DWaSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_map_combined"
      ],
      "metadata": {
        "id": "oFC-edvXBtHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = bert_tokenizer(X_train.tolist(), dtype=\"int32\", truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
        "val_encodings = bert_tokenizer(X_val.tolist(), dtype=\"int32\", truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
        "test_encodings = bert_tokenizer(X_test.tolist(), dtype=\"int32\", truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')"
      ],
      "metadata": {
        "id": "pm1gV7bIE8Kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification_loss_dict = {label : tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False) for label in cat_label_cols}\n",
        "classification_loss_dict"
      ],
      "metadata": {
        "id": "8AR9XNTfHQN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification_metrics_dict = {label : 'accuracy' for label in cat_label_cols}\n",
        "classification_metrics_dict"
      ],
      "metadata": {
        "id": "48bEmCuAJVv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regression_loss_dict = {label : 'huber_loss' for label in label_cols}\n",
        "regression_loss_dict"
      ],
      "metadata": {
        "id": "LBTFzq_3mIeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regression_metrics_dict = {label : tf.keras.metrics.RootMeanSquaredError() for label in label_cols}\n",
        "regression_metrics_dict"
      ],
      "metadata": {
        "id": "StNBi5Qylq6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bert_model(checkpoint = model_checkpoint,\n",
        "                      num_classes = 9,   # [1, 1.5, 2, 2.5....4.5, 5]: 9 classes\n",
        "                      number_of_hidden_layer = 1,\n",
        "                      hidden_layer_node_count = 256,\n",
        "                      dropout = 0.3,\n",
        "                      learning_rate = 0.00001,\n",
        "                      trainable_flag = True,\n",
        "                      retrain_layer_count = 999,#All layers are trainable\n",
        "                      classification_regression_flag = 'C',\n",
        "                      max_length = MAX_LENGTH):\n",
        "    \"\"\"\n",
        "    Build a simple classification model with BERT. Use the Pooler Output for classification purposes.\n",
        "    \"\"\"\n",
        "    set_config_param()\n",
        "    bert_model = TFBertModel.from_pretrained(checkpoint, name = 'bert_model')    \n",
        "    if trainable_flag:\n",
        "        if retrain_layer_count == 999:\n",
        "            # Train all layers of the BERT model\n",
        "            bert_model.trainable = True         \n",
        "        else:\n",
        "            retrain_layers_list = []\n",
        "            for retrain_layer_number in range(retrain_layer_count):\n",
        "                retrain_layers_list.append('retrain_layer_' + str(retrain_layer_number)) \n",
        "            print('retrain layers: ', retrain_layers_list)\n",
        "\n",
        "            for weight in bert_model.weights:\n",
        "                if not any([x in weight.name for layer in retrain_layers_list]):\n",
        "                    #print('freezing: ', w)\n",
        "                    weight._trainable = False\n",
        "    else:\n",
        "        # Freeze all layers of pre-trained BERT model\n",
        "        bert_model.trainable = False \n",
        "    \n",
        "    # Input layer\n",
        "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='input_ids_layer')\n",
        "    token_type_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='token_type_ids_layer')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='attention_mask_layer')\n",
        "\n",
        "    bert_inputs = {'input_ids': input_ids,\n",
        "                   'token_type_ids': token_type_ids,\n",
        "                   'attention_mask': attention_mask}\n",
        "                   \n",
        "    # Bert output: being used as an input feature in the classification model below\n",
        "    bert_out = bert_model(bert_inputs)        # full features as an input to the following classification model\n",
        "    # pooler_output = bert_out[1]             # one vector for each\n",
        "    cls_token = bert_out[0][:, 0, :]          # give us a raw CLS tokens\n",
        "\n",
        "    # =========== END generate \"input features\" using BERT tokenizer ==================================\n",
        "\n",
        "    # =========== BEGIN build a \"multi-classification model\" below passing the BERT input features ======\n",
        "    layer_list = []\n",
        "    for hidden_layer_number in range(number_of_hidden_layer):\n",
        "        if hidden_layer_number == 0:\n",
        "            hidden_layer = tf.keras.layers.Dense(units = hidden_layer_node_count\n",
        "                                               , activation = 'relu'\n",
        "                                               , name = 'hidden_layer_' + str(hidden_layer_number + 1)\n",
        "                                                )(cls_token)\n",
        "        else:\n",
        "            hidden_layer = tf.keras.layers.Dense(units = hidden_layer_node_count\n",
        "                                               , activation = 'relu'\n",
        "                                               , name = 'hidden_layer_' + str(hidden_layer_number + 1)\n",
        "                                                )(layer_list[-1])\n",
        "        layer_list.append(hidden_layer)\n",
        "        #dropout_layer = tf.keras.layers.Dropout(dropout, name = 'dropout_layer_' + str(hidden_layer_number + 1))(hidden_layer) \n",
        "        #layer_list.append(dropout_layer)\n",
        "\n",
        "    if number_of_hidden_layer > 0:\n",
        "        dropout_layer = tf.keras.layers.Dropout(dropout, name = 'dropout_layer')(hidden_layer) \n",
        "        layer_list.append(dropout_layer)\n",
        "\n",
        "    if classification_regression_flag == 'C':\n",
        "        # Output classification layer\n",
        "        classification_cohesion = tf.keras.layers.Dense(num_classes, activation = 'softmax', name = 'cat_cohesion')(layer_list[-1]) \n",
        "        classification_syntax = tf.keras.layers.Dense(num_classes, activation = 'softmax', name = 'cat_syntax')(layer_list[-1])\n",
        "        classification_vocabulary = tf.keras.layers.Dense(num_classes, activation = 'softmax', name = 'cat_vocabulary')(layer_list[-1])\n",
        "        classification_phraseology = tf.keras.layers.Dense(num_classes, activation = 'softmax', name = 'cat_phraseology')(layer_list[-1])\n",
        "        classification_grammar = tf.keras.layers.Dense(num_classes, activation = 'softmax', name = 'cat_grammar')(layer_list[-1])\n",
        "        classification_conventions = tf.keras.layers.Dense(num_classes, activation = 'softmax', name = 'cat_conventions')(layer_list[-1])\n",
        "\n",
        "        outputs = [classification_cohesion, \n",
        "                   classification_syntax, \n",
        "                   classification_vocabulary, \n",
        "                   classification_phraseology, \n",
        "                   classification_grammar, \n",
        "                   classification_conventions\n",
        "                  ]\n",
        "        classification_model = tf.keras.Model(inputs = [input_ids, \n",
        "                                                        token_type_ids, \n",
        "                                                        attention_mask\n",
        "                                                       ], \n",
        "                                              outputs = outputs\n",
        "                                             )\n",
        "        classification_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
        "                                     loss = classification_loss_dict,\n",
        "                                     metrics = classification_metrics_dict\n",
        "                                    )                \n",
        "        return classification_model\n",
        "\n",
        "    elif classification_regression_flag == 'R':\n",
        "\n",
        "        # Output regression layer\n",
        "        regression_cohesion = tf.keras.layers.Dense(num_classes, activation = 'softmax', name = 'cat_cohesion')(layer_list[-1]) \n",
        "        regression_syntax = tf.keras.layers.Dense(num_classes, activation = 'softmax', name = 'cat_syntax')(layer_list[-1])\n",
        "        regression_vocabulary = tf.keras.layers.Dense(num_classes, activation = 'softmax', name = 'cat_vocabulary')(layer_list[-1])\n",
        "        regression_phraseology = tf.keras.layers.Dense(num_classes, activation = 'softmax', name = 'cat_phraseology')(layer_list[-1])\n",
        "        regression_grammar = tf.keras.layers.Dense(num_classes, activation = 'softmax', name = 'cat_grammar')(layer_list[-1])\n",
        "        regression_conventions = tf.keras.layers.Dense(num_classes, activation = 'softmax', name = 'cat_conventions')(layer_list[-1])\n",
        "\n",
        "        outputs = [regression_cohesion, \n",
        "                   regression_syntax, \n",
        "                   regression_vocabulary, \n",
        "                   regression_phraseology, \n",
        "                   regression_grammar, \n",
        "                   regression_conventions\n",
        "                  ]\n",
        "        regression_model = tf.keras.Model(inputs = [input_ids, \n",
        "                                                    token_type_ids, \n",
        "                                                    attention_mask\n",
        "                                                   ], \n",
        "                                          outputs = outputs\n",
        "                                         )\n",
        "        regression_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
        "                                 loss = regression_loss_dict,\n",
        "                                 metrics = regression_metrics_dict\n",
        "                                )                \n",
        "        return regression_model"
      ],
      "metadata": {
        "id": "5espg0GofL4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **\"bert_classification_model_1\" is the baseline classification model with multilabel multi output(1..9). We are retraining the pre-trained model.**"
      ],
      "metadata": {
        "id": "UkJZ_SoKQBoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_classification_model_1 = create_bert_model(checkpoint = model_checkpoint,\n",
        "                                                num_classes = 9,   # [1, 1.5, 2, 2.5....4.5, 5]: 9 classes\n",
        "                                                number_of_hidden_layer = 1,\n",
        "                                                hidden_layer_node_count = 128,\n",
        "                                                dropout = 0.3,\n",
        "                                                learning_rate = 0.00001,\n",
        "                                                trainable_flag = True,\n",
        "                                                retrain_layer_count = 999,#All layers trainable\n",
        "                                                classification_regression_flag = 'C',\n",
        "                                                max_length = MAX_LENGTH\n",
        "                                              )\n",
        "\n",
        "print(bert_classification_model_1.summary())"
      ],
      "metadata": {
        "id": "0DaGGf0PfL7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(bert_classification_model_1, show_shapes=False, show_dtype=False, show_layer_names=True, dpi=90)"
      ],
      "metadata": {
        "id": "uW8lITjfM3rK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_classification_model_1_history = bert_classification_model_1.fit([train_encodings.input_ids, \n",
        "                                                                       train_encodings.token_type_ids, \n",
        "                                                                       train_encodings.attention_mask\n",
        "                                                                      ], \n",
        "                                                                      y_train_map_combined,   \n",
        "                                                                      validation_data =([val_encodings.input_ids, \n",
        "                                                                                         val_encodings.token_type_ids, \n",
        "                                                                                         val_encodings.attention_mask\n",
        "                                                                                        ], \n",
        "                                                                                        y_val_map_combined\n",
        "                                                                                      ),    \n",
        "                                                                      batch_size = 8, \n",
        "                                                                      epochs = 5\n",
        "                                                                     )                                                  \n",
        "bert_classification_model_1_history_df = pd.DataFrame(bert_classification_model_1_history.history)\n",
        "bert_classification_model_1_history_df.T"
      ],
      "metadata": {
        "id": "RFrrKIbiM7eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_classification_model_1 = bert_classification_model_1.evaluate([test_encodings.input_ids, \n",
        "                                                                     test_encodings.token_type_ids, \n",
        "                                                                     test_encodings.attention_mask\n",
        "                                                                    ], \n",
        "                                                                    y_test_map_combined\n",
        "                                                                   ) \n",
        "\n",
        "print('Test loss:', score_classification_model_1[0]) \n",
        "print('Test accuracy:', score_classification_model_1[1])"
      ],
      "metadata": {
        "id": "xWfPpaQaS46B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_classification_model_1 = bert_classification_model_1.predict([test_encodings.input_ids, \n",
        "                                                                          test_encodings.token_type_ids, \n",
        "                                                                          test_encodings.attention_mask\n",
        "                                                                          ]\n",
        "                                                                         )\n",
        "predictions_classification_model_1 = np.clip(predictions_classification_model_1, 0, 8)\n",
        "predictions_classification_model_1"
      ],
      "metadata": {
        "id": "lGfnj7mRTmtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_accuracy(bert_classification_model_1_history_df, cat_label_cols)"
      ],
      "metadata": {
        "id": "hlOLZlvYd9Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **\"bert_classification_model_2\" is the baseline classification model with multilabel multi output. We are freezing the pre-trained model.**"
      ],
      "metadata": {
        "id": "P2TH7rk9Q6lC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_classification_model_2 = create_bert_model(checkpoint = model_checkpoint,\n",
        "                                                num_classes = 9,   # [1, 1.5, 2, 2.5....4.5, 5]: 9 classes\n",
        "                                                number_of_hidden_layer = 1,\n",
        "                                                hidden_layer_node_count = 128,\n",
        "                                                dropout = 0.3,\n",
        "                                                learning_rate = 0.00001,\n",
        "                                                trainable_flag = False,\n",
        "                                                retrain_layer_count = 999,#All layers trainable\n",
        "                                                classification_regression_flag = 'C',\n",
        "                                                max_length = MAX_LENGTH\n",
        "                                              )\n",
        "\n",
        "print(bert_classification_model_2.summary())"
      ],
      "metadata": {
        "id": "qrkamSqZNXOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(bert_classification_model_2, show_shapes=False, show_dtype=False, show_layer_names=True, dpi=90)"
      ],
      "metadata": {
        "id": "g393mEddNXST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_classification_model_2_history = bert_classification_model_2.fit([train_encodings.input_ids, \n",
        "                                                                       train_encodings.token_type_ids, \n",
        "                                                                       train_encodings.attention_mask\n",
        "                                                                      ], \n",
        "                                                                      y_train_map_combined,   \n",
        "                                                                      validation_data =([val_encodings.input_ids, \n",
        "                                                                                         val_encodings.token_type_ids, \n",
        "                                                                                         val_encodings.attention_mask\n",
        "                                                                                        ], \n",
        "                                                                                        y_val_map_combined\n",
        "                                                                                      ),    \n",
        "                                                                      batch_size = 8, \n",
        "                                                                      epochs = 5\n",
        "                                                                     )                                                  \n",
        "bert_classification_model_2_history_df = pd.DataFrame(bert_classification_model_2_history.history)\n",
        "bert_classification_model_2_history_df.T"
      ],
      "metadata": {
        "id": "EkuQS1N4Qtpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_classification_model_2 = bert_classification_model_2.evaluate([test_encodings.input_ids, \n",
        "                                                                     test_encodings.token_type_ids, \n",
        "                                                                     test_encodings.attention_mask\n",
        "                                                                    ], \n",
        "                                                                    y_test_map_combined\n",
        "                                                                   ) \n",
        "\n",
        "print('Test loss:', score_classification_model_2[0]) \n",
        "print('Test accuracy:', score_classification_model_2[1])"
      ],
      "metadata": {
        "id": "_l14t1I0Tc4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_classification_model_2 = bert_classification_model_2.predict([test_encodings.input_ids, \n",
        "                                                                          test_encodings.token_type_ids, \n",
        "                                                                          test_encodings.attention_mask\n",
        "                                                                          ]\n",
        "                                                                         )\n",
        "predictions_classification_model_2 = np.clip(predictions_classification_model_2, 0, 8)\n",
        "predictions_classification_model_2"
      ],
      "metadata": {
        "id": "apgV4pWmUQbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_accuracy(bert_classification_model_2_history_df, cat_label_cols)"
      ],
      "metadata": {
        "id": "8SK5ttSiNXWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **\"bert_classification_model_3\" is the baseline classification model with multilabel multi output (1..5). We are un-freezing the pre-trained model and running with scaled output levels.**"
      ],
      "metadata": {
        "id": "QoGqVA1qcT-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_classification_model_3 = create_bert_model(checkpoint = model_checkpoint,\n",
        "                                                num_classes = 5,   # [1, 1.5, 2, 2.5....4.5, 5]: 9 classes\n",
        "                                                number_of_hidden_layer = 1,\n",
        "                                                hidden_layer_node_count = 128,\n",
        "                                                dropout = 0.3,\n",
        "                                                learning_rate = 0.00001,\n",
        "                                                trainable_flag = True,\n",
        "                                                retrain_layer_count = 999,#All layers trainable\n",
        "                                                classification_regression_flag = 'C',\n",
        "                                                max_length = MAX_LENGTH\n",
        "                                              )\n",
        "\n",
        "print(bert_classification_model_3.summary())"
      ],
      "metadata": {
        "id": "XQfLH8lKNXZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(bert_classification_model_3, show_shapes=False, show_dtype=False, show_layer_names=True, dpi=90)"
      ],
      "metadata": {
        "id": "N0tyJScBccRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_classification_model_3_history = bert_classification_model_3.fit([train_encodings.input_ids, \n",
        "                                                                       train_encodings.token_type_ids, \n",
        "                                                                       train_encodings.attention_mask\n",
        "                                                                      ], \n",
        "                                                                      y_train_scaled_combined,   \n",
        "                                                                      validation_data =([val_encodings.input_ids, \n",
        "                                                                                         val_encodings.token_type_ids, \n",
        "                                                                                         val_encodings.attention_mask\n",
        "                                                                                        ], \n",
        "                                                                                        y_val_scaled_combined\n",
        "                                                                                      ),    \n",
        "                                                                      batch_size = 8, \n",
        "                                                                      epochs = 5\n",
        "                                                                     )                                                  \n",
        "bert_classification_model_3_history_df = pd.DataFrame(bert_classification_model_3_history.history)\n",
        "bert_classification_model_3_history_df.T"
      ],
      "metadata": {
        "id": "0VqCBVmWccUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_classification_model_3 = bert_classification_model_3.evaluate([test_encodings.input_ids, \n",
        "                                                                     test_encodings.token_type_ids, \n",
        "                                                                     test_encodings.attention_mask\n",
        "                                                                    ], \n",
        "                                                                    y_test_scaled_combined\n",
        "                                                                   ) \n",
        "\n",
        "print('Test loss:', score_classification_model_3[0]) \n",
        "print('Test accuracy:', score_classification_model_3[1])"
      ],
      "metadata": {
        "id": "OapnRe5yccXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_classification_model_3 = bert_classification_model_3.predict([test_encodings.input_ids, \n",
        "                                                                          test_encodings.token_type_ids, \n",
        "                                                                          test_encodings.attention_mask\n",
        "                                                                          ]\n",
        "                                                                         )\n",
        "predictions_classification_model_3 = np.clip(predictions_classification_model_3, 1, 5)\n",
        "predictions_classification_model_3"
      ],
      "metadata": {
        "id": "b7pVQoOKccai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_accuracy(bert_classification_model_3_history_df, cat_label_cols)"
      ],
      "metadata": {
        "id": "N81DBEzWNXb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UvZob1D0NXe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sVxTurgBNXh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6cZu020HfMBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y_yhzq_dfMEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FDoYoIFhE8OM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}