{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilamaiti/spring_2023_w266_final_project_heesuk_iris_srila/blob/main/srila/bertweet_ell.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing new libraries**"
      ],
      "metadata": {
        "id": "Lhf_T8cMjGsp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD4BChywisTm",
        "outputId": "2dd15d5a-f6c0-42ac-cf3d-7d3151495e5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.9/dist-packages (1.8.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from wordcloud) (8.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.9/dist-packages (from wordcloud) (1.22.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (5.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (23.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (4.39.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (1.0.7)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->wordcloud) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji==0.6.0\n",
            "  Downloading emoji-0.6.0.tar.gz (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.6.0-py3-none-any.whl size=49732 sha256=a66c97d8b082e87f5f21e0a11c225ff70f1c9806dee86cbca305d518c2f301c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/2a/7f/1a0012c86b1061c6ee2ed9568b1f830f857a51e8e416452af2\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.9/dist-packages (0.13.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install wordcloud\n",
        "!pip install transformers\n",
        "!pip install emoji==0.6.0\n",
        "!pip3 install tokenizers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing libraries**"
      ],
      "metadata": {
        "id": "bC3s_6EZjMBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(f'transformers version: {transformers.__version__}')\n",
        "from transformers import logging as hf_logging\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "hf_logging.set_verbosity_error()\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import spacy      \n",
        "from spacy import displacy\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS\n",
        "from wordcloud import ImageColorGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Other required libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import copy\n",
        "import sys\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Data visualization libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tensorflow libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils.layer_utils import count_params\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.losses import mae\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from keras.models import Model\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.models import load_model\n",
        "\n",
        "import torch\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaPH5XqxjU_z",
        "outputId": "0144232f-7333-4b97-e214-860738013a35"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers version: 4.27.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General functions**"
      ],
      "metadata": {
        "id": "5fjpBUG5jbmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Set parameters**"
      ],
      "metadata": {
        "id": "FYfihW3Mjgkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_config_param(seed = 99):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    tf.keras.backend.clear_session()\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/Kaggle\"\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    \n",
        "    \n",
        "set_config_param(20230214)"
      ],
      "metadata": {
        "id": "pr1-rS8tjiqn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Plot loss and accuracy**"
      ],
      "metadata": {
        "id": "6yYB47Gdjo0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_accuracy(history, col_list):\n",
        "    fig, ax = plt.subplots(2, 6, figsize=(16, 6), sharex='col', sharey='row')\n",
        "    fig.tight_layout(pad=5.0)\n",
        "    for idx, col in enumerate(col_list):\n",
        "\n",
        "        ax[0, idx].plot(history[col + '_loss'], lw=2, color='darkgoldenrod')\n",
        "        ax[0, idx].plot(history['val_' + col + '_loss'], lw=2, color='indianred')\n",
        "        #ax[0, idx].legend(loc='center left')\n",
        "        ax[0, idx].legend(['Train', 'Validation'], fontsize=5)\n",
        "        ax[0, idx].set_xlabel('Epochs', size=10)\n",
        "        ax[0, idx].set_title('Loss: ' + col)\n",
        "\n",
        "        ax[1, idx].plot(history[col + '_accuracy'], lw=2, color='darkgoldenrod')\n",
        "        ax[1, idx].plot(history['val_' + col + '_accuracy'], lw=2, color='indianred')\n",
        "        #ax[0, idx].legend(loc='center left')\n",
        "        ax[1, idx].legend(['Train', 'Validation'], fontsize=5)\n",
        "        ax[1, idx].set_xlabel('Epochs', size=10)\n",
        "        ax[1, idx].set_title('Accuracy: ' + col)"
      ],
      "metadata": {
        "id": "BE9NqiNWjrPy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Custom metric**"
      ],
      "metadata": {
        "id": "uJw9YpJXjwSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MCRMSE(y_true, y_pred):\n",
        "    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n",
        "    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=-1, keepdims=True)"
      ],
      "metadata": {
        "id": "_VCdZF9ZjzN6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MCRMSE(y_preds, y_trues):\n",
        "    scores = []\n",
        "    idxes = y_trues.shape[1]\n",
        "    for i in range(idxes):\n",
        "        y_true = y_trues[:,i]\n",
        "        y_pred = y_preds[:,i]\n",
        "        score = mean_squared_error(y_true, y_pred, squared=False)\n",
        "        scores.append(score)\n",
        "    mcrmse_score = np.mean(scores)\n",
        "    return mcrmse_score, scores\n",
        "\n",
        "\n",
        "def get_score(y_trues, y_preds):\n",
        "    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n",
        "    return mcrmse_score, scores\n",
        "\n",
        "def loss_fn(outputs, targets):\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "    loss = torch.sqrt(loss_fn(outputs, targets))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "DzZ2iA2qjkOZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read input files**"
      ],
      "metadata": {
        "id": "zP0n_Dd8j26W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_train_df = pd.read_csv('train.csv')\n",
        "input_test_df = pd.read_csv('test.csv')\n",
        "label_cols = input_train_df.columns[2:]\n",
        "orig_train_df = copy.deepcopy(input_train_df)\n",
        "orig_train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Le5HU_86j4-9",
        "outputId": "291b06b4-f5c1-4c43-df69-4f04227fbbdf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        text_id                                          full_text  cohesion  \\\n",
              "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
              "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
              "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n",
              "3  003885A45F42  The best time in life is when you become yours...       4.5   \n",
              "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n",
              "\n",
              "   syntax  vocabulary  phraseology  grammar  conventions  \n",
              "0     3.5         3.0          3.0      4.0          3.0  \n",
              "1     2.5         3.0          2.0      2.0          2.5  \n",
              "2     3.5         3.0          3.0      3.0          2.5  \n",
              "3     4.5         4.5          4.5      4.0          5.0  \n",
              "4     3.0         3.0          3.0      2.5          2.5  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b3460a55-ac47-455c-9115-e2e4cb98a6ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>full_text</th>\n",
              "      <th>cohesion</th>\n",
              "      <th>syntax</th>\n",
              "      <th>vocabulary</th>\n",
              "      <th>phraseology</th>\n",
              "      <th>grammar</th>\n",
              "      <th>conventions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0016926B079C</td>\n",
              "      <td>I think that students would benefit from learn...</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0022683E9EA5</td>\n",
              "      <td>When a problem is a change you have to let it ...</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00299B378633</td>\n",
              "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>003885A45F42</td>\n",
              "      <td>The best time in life is when you become yours...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0049B1DF5CCC</td>\n",
              "      <td>Small act of kindness can impact in other peop...</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b3460a55-ac47-455c-9115-e2e4cb98a6ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b3460a55-ac47-455c-9115-e2e4cb98a6ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b3460a55-ac47-455c-9115-e2e4cb98a6ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Spliting the data**\n",
        "Original test data is very limited, there are only 3 records and it does not have labels to test. So we decided to repurpose the given train data to split into train, test and validation sets."
      ],
      "metadata": {
        "id": "A4DOnrFpkBz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle = np.random.permutation(np.arange(orig_train_df.shape[0]))\n",
        "orig_train_df = orig_train_df.iloc[shuffle]\n",
        "split=(0.8,0.1,0.1)\n",
        "splits = np.multiply(len(orig_train_df), split).astype(int)\n",
        "df_train, df_val, df_test = np.split(orig_train_df, [splits[0], splits[0] + splits[1]])\n",
        "\n",
        "X_train, X_val, X_test = df_train['full_text'], df_val['full_text'], df_test['full_text']\n",
        "y_train, y_val, y_test = np.array(df_train[label_cols]), np.array(df_val[label_cols]), np.array(df_test[label_cols])"
      ],
      "metadata": {
        "id": "ZtWKcw09kKZt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model building**"
      ],
      "metadata": {
        "id": "6LVCmgcikTTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_config_param()\n",
        "MAX_LENGTH = 512\n",
        "epochs = 5\n",
        "batch_size = 8\n",
        "dropout = .1\n",
        "learning_rate = .00005\n",
        "number_of_hidden_layer = 1\n",
        "hidden_layer_node_count = 64\n",
        "trainable_flag = False\n",
        "retrain_layer_count = 0"
      ],
      "metadata": {
        "id": "Uqi7K_9ylr37"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bertweet_model_checkpoint = 'vinai/bertweet-base'   \n",
        "bertweet_model = TFAutoModel.from_pretrained(bertweet_model_checkpoint)\n",
        "bertweet_tokenizer = AutoTokenizer.from_pretrained(bertweet_model_checkpoint, use_fast=False, normalization = True)"
      ],
      "metadata": {
        "id": "gMKPylKdmMXZ"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_text(text, tokenizer):\n",
        "    \n",
        "    encoded = tokenizer.batch_encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors=\"tf\",\n",
        "    )\n",
        "\n",
        "    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_masks\": attention_masks\n",
        "    }"
      ],
      "metadata": {
        "id": "zdV8CU_tPhHd"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tf.keras.layers.Input(shape=(MAX_LENGTH,), dtype=tf.int32, name='input_ids_layer')\n",
        "attention_mask = tf.keras.layers.Input(shape=(MAX_LENGTH,), dtype=tf.int32, name='attention_mask_layer')\n",
        "#segment_ids = tf.keras.layers.Input(shape=(MAX_LENGTH,), dtype=tf.int32, name=\"segment_id_layer\")"
      ],
      "metadata": {
        "id": "fBhhdt6tQIpS"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = encode_text(df_train['full_text'].tolist(), bertweet_tokenizer)\n",
        "val_encodings = encode_text(df_val['full_text'].tolist(), bertweet_tokenizer)\n",
        "test_encodings = encode_text(df_test['full_text'].tolist(), bertweet_tokenizer)"
      ],
      "metadata": {
        "id": "XvirTQAhQeLK"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPVx-RaAUk2r",
        "outputId": "fd25efba-d563-4bcc-f9ee-11c278426243"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': array([[    0, 16017, 19819, ...,     1,     1,     1],\n",
              "        [    0,     8,   101, ...,     1,     1,     1],\n",
              "        [    0,     8,  5766, ...,     1,     1,     1],\n",
              "        ...,\n",
              "        [    0,   726,  2522, ...,     9,  6553,     2],\n",
              "        [    0,  1038,    14, ...,     1,     1,     1],\n",
              "        [    0,  2420,    83, ...,   153,    15,     2]], dtype=int32),\n",
              " 'attention_masks': array([[1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 1, 1, ..., 1, 1, 1],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 1, 1, 1]], dtype=int32)}"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgpr29jOUzvv",
        "outputId": "b3006aef-a56b-4d4a-ec05-47b3ff68d0f9"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_masks'])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bertweet_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R9-o9OiSINX",
        "outputId": "73dbcc1f-ba40-45c4-ba48-bac7bfa1d87d"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<transformers.models.roberta.modeling_tf_roberta.TFRobertaModel at 0x7f90a4a91520>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bertweet_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_DjDrvc0Cl8",
        "outputId": "6556df51-4c1f-447c-bcb8-806f434cfbcb"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_roberta_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " roberta (TFRobertaMainLayer  multiple                 134899968 \n",
            " )                                                               \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 134,899,968\n",
            "Trainable params: 134,899,968\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(bertweet_model, show_shapes=False, show_dtype=False, show_layer_names=True, dpi=90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "zXf_cGl10IdL",
        "outputId": "f7992231-fec0-44df-d227-97d12396ce61"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bertweet_model_v1 = bertweet_model(input_ids, attention_mask=attention_mask)\n",
        "cls_token = bertweet_model_v1[0][:, 0, :]"
      ],
      "metadata": {
        "id": "3ksetNxoPhXc"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bertweet_model_v1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCyLq7_RWivn",
        "outputId": "9bb893db-18d2-45ac-9c91-2d8f02c5a120"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<KerasTensor: shape=(None, 512, 768) dtype=float32 (created by layer 'tf_roberta_model')>, pooler_output=<KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_roberta_model')>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(bertweet_model_v1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oi3NwYl6Wx_t",
        "outputId": "c03029f1-c4af-410b-939d-0060731d54c4"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['last_hidden_state', 'pooler_output']"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cls_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXr5LmdHTlG5",
        "outputId": "de124625-5463-49c6-debd-49f959e22d4c"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf.__operators__.getitem')>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer_list = []\n",
        "for hidden_layer_number in range(number_of_hidden_layer):\n",
        "    if hidden_layer_number == 0:\n",
        "        hidden_layer = tf.keras.layers.Dense(units = hidden_layer_node_count\n",
        "                                    , activation = 'relu'\n",
        "                                    , name = 'hidden_layer_' + str(hidden_layer_number + 1)\n",
        "                                     )(cls_token)\n",
        "    else:\n",
        "        hidden_layer = tf.keras.layers.Dense(units = hidden_layer_node_count\n",
        "                                    , activation = 'relu'\n",
        "                                    , name = 'hidden_layer_' + str(hidden_layer_number + 1)\n",
        "                                     )(layer_list[-1])\n",
        "    layer_list.append(hidden_layer)\n",
        "    dropout_layer = tf.keras.layers.Dropout(dropout, name = 'dropout_layer_' + str(hidden_layer_number + 1))(hidden_layer) \n",
        "    layer_list.append(dropout_layer)\n",
        "#layer_list.append(tf.keras.layers.Flatten()) "
      ],
      "metadata": {
        "id": "cqJaBHwTPhaa"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = tf.keras.layers.Dense(6,)(layer_list[-1])\n",
        "bertweet_v1_regression_model = tf.keras.Model(inputs = [input_ids, \n",
        "                                                        attention_mask\n",
        "                                                       ], \n",
        "                                                       outputs = output\n",
        "                                             )\n",
        "\n",
        "bertweet_v1_regression_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
        "                                     loss = MCRMSE,\n",
        "                                     metrics=MCRMSE\n",
        "                                    ) \n",
        "print(bertweet_v1_regression_model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRD1yhqxTSGm",
        "outputId": "8416b374-f0cf-434f-a13b-f1846485ab74"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids_layer (InputLayer)   [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask_layer (InputLay  [(None, 512)]       0           []                               \n",
            " er)                                                                                              \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  134899968  ['input_ids_layer[0][0]',        \n",
            " el)                            thPoolingAndCrossAt               'attention_mask_layer[0][0]']   \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][0]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " hidden_layer_1 (Dense)         (None, 64)           49216       ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_layer_1 (Dropout)      (None, 64)           0           ['hidden_layer_1[0][0]']         \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 6)            390         ['dropout_layer_1[0][0]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 134,949,574\n",
            "Trainable params: 134,949,574\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(bertweet_v1_regression_model, show_shapes=False, show_dtype=False, show_layer_names=True, dpi=90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "MJDDW9AbTSKQ",
        "outputId": "1fee3e25-c883-4480-85a1-d1199a08f331"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bertweet_v1_regression_model_history = bertweet_v1_regression_model.fit([train_encodings['input_ids'], \n",
        "                                                                         train_encodings['attention_masks']\n",
        "                                                                        ], \n",
        "                                                                        y_train,   \n",
        "                                                                        validation_data =([val_encodings['input_ids'], \n",
        "                                                                                           val_encodings['attention_masks']\n",
        "                                                                                          ], \n",
        "                                                                                          y_val\n",
        "                                                                                         ),    \n",
        "                                                                        batch_size = batch_size, \n",
        "                                                                        epochs = epochs\n",
        "                                                                       )                                                  \n",
        "bertweet_v1_regression_model_history_df = pd.DataFrame(bertweet_v1_regression_model_history.history)\n",
        "bertweet_v1_regression_model_history_df.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MuKGjALlTSN5",
        "outputId": "de11d932-51d9-45db-f0fe-9959010b3dd0"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-a060fbaf794c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m bertweet_v1_regression_model_history = bertweet_v1_regression_model.fit([train_encodings['input_ids'], \n\u001b[0m\u001b[1;32m      2\u001b[0m                                                                          \u001b[0mtrain_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_masks'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                                         ], \n\u001b[1;32m      4\u001b[0m                                                                         \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                         validation_data =([val_encodings['input_ids'], \n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'model_1/tf_roberta_model/roberta/embeddings/Gather_1' defined at (most recent call last):\n    File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 543, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2854, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3057, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-102-a060fbaf794c>\", line 1, in <module>\n      bertweet_v1_regression_model_history = bertweet_v1_regression_model.fit([train_encodings['input_ids'],\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1023, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/transformers/modeling_tf_utils.py\", line 943, in run_call_with_unpacked_inputs\n      symbolic_weight.name.split(\"/\")[:delimeter]\n    File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_tf_roberta.py\", line 970, in call\n      outputs = self.roberta(\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/transformers/modeling_tf_utils.py\", line 943, in run_call_with_unpacked_inputs\n      symbolic_weight.name.split(\"/\")[:delimeter]\n    File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_tf_roberta.py\", line 662, in call\n      embedding_output = self.embeddings(\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_tf_roberta.py\", line 175, in call\n      position_embeds = tf.gather(params=self.position_embeddings, indices=position_ids)\nNode: 'model_1/tf_roberta_model/roberta/embeddings/Gather_1'\nindices[7,128] = 130 is not in [0, 130)\n\t [[{{node model_1/tf_roberta_model/roberta/embeddings/Gather_1}}]] [Op:__inference_train_function_171862]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score_bertweet_v1_regression_model = bertweet_v1_regression_model.evaluate([test_encodings.input_ids, \n",
        "                                                                            test_encodings.attention_mask\n",
        "                                                                           ], \n",
        "                                                                           y_test\n",
        "                                                                          ) \n",
        "\n",
        "print(score_bertweet_v1_regression_model)"
      ],
      "metadata": {
        "id": "XY2XkyEWUNfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_bertweet_v1_regression_model = bertweet_v1_regression_model.predict([test_encodings.input_ids, \n",
        "                                                                                 test_encodings.attention_mask\n",
        "                                                                                ]\n",
        "                                                                               )\n",
        "print(predictions_bertweet_v1_regression_model)"
      ],
      "metadata": {
        "id": "XvkSN8mbUNij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test[label_cols]"
      ],
      "metadata": {
        "id": "jxP7OEQmPhdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_bertweet_v1_regression_model"
      ],
      "metadata": {
        "id": "HL557L53PhgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_arr = np.arange(len(bertweet_v1_regression_model['loss'])) + 1\n",
        "\n",
        "fig = plt.figure(figsize=(12, 4))\n",
        "ax = fig.add_subplot(1, 2, 1)\n",
        "ax.plot(x_arr, bertweet_v1_regression_model_history_df['loss'], '-o', label='Train loss')\n",
        "ax.plot(x_arr, bertweet_v1_regression_model_history_df['val_loss'], '--<', label='Validation loss')\n",
        "ax.legend(fontsize=15)\n",
        "ax.set_xlabel('Epoch', size=15)\n",
        "ax.set_ylabel('Loss', size=15)\n",
        "\n",
        "ax = fig.add_subplot(1, 2, 2)\n",
        "ax.plot(x_arr, bertweet_v1_regression_model_history_df['MCRMSE'], '-o', label='Train MCRMSE')\n",
        "ax.plot(x_arr, bertweet_v1_regression_model_history_df['val_MCRMSE'], '--<', label='Validation MCRMSE')\n",
        "ax.legend(fontsize=15)\n",
        "ax.set_xlabel('Epoch', size=15)\n",
        "ax.set_ylabel('Accuracy', size=15)\n",
        "ax.set_ylim(0,1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yP2B5aiVWN4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-xMHHLu7WN8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s7D-0KL4WOAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E3x3FiFFWOEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jDu3E9AJWOH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from emoji import demojize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "\n",
        "tokenizer = TweetTokenizer()\n",
        "\n",
        "\n",
        "def normalizeToken(token):\n",
        "    lowercased_token = token.lower()\n",
        "    if token.startswith(\"@\"):\n",
        "        return \"@USER\"\n",
        "    elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
        "        return \"HTTPURL\"\n",
        "    elif len(token) == 1:\n",
        "        return demojize(token)\n",
        "    else:\n",
        "        if token == \"’\":\n",
        "            return \"'\"\n",
        "        elif token == \"…\":\n",
        "            return \"...\"\n",
        "        else:\n",
        "            return token\n",
        "\n",
        "'''\n",
        "https://preply.com/en/blog/the-most-used-internet-abbreviations-for-texting-and-tweeting/\n",
        "'''\n",
        "def normalizeTweet(tweet):\n",
        "    tokens = tokenizer.tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
        "    normTweet = \" \".join([normalizeToken(token) for token in tokens])\n",
        "\n",
        "    normTweet = (\n",
        "        normTweet.replace(\"cannot \", \"can not \")\n",
        "        .replace(\"n't \", \" n't \")\n",
        "        .replace(\"n 't \", \" n't \")\n",
        "        .replace(\"ca n't\", \"can't\")\n",
        "        .replace(\"ai n't\", \"ain't\")\n",
        "        .replace(\"bc\", \"because\")\n",
        "        .replace(\"btw\", \"by the way\")\n",
        "        .replace(\"cya\", \"see ya\")\n",
        "        .replace(\" u \", \" you \")\n",
        "        .replace(\"dm\", \"direct message\")\n",
        "        .replace(\"gm\", \"good morning\")\n",
        "        .replace(\"ftw\", \"for the win\")\n",
        "        .replace(\"tbh\", \"to be honest\")\n",
        "        .replace(\"fwiw\", \"for what it’s worth\")\n",
        "        .replace(\"idk\", \"I don't know\")\n",
        "        .replace(\"ily\", \"I love you\")\n",
        "        .replace(\"brb\", \"be right back\")\n",
        "        .replace(\"imo\", \"in my opinion\")\n",
        "        .replace(\"irl\", \"in real life\")\n",
        "        .replace(\"jk\", \"just kidding\")\n",
        "        .replace(\"lmk\", \"let me know\")\n",
        "        .replace(\"lmc\", \"let me check\")\n",
        "        .replace(\"lol\", \"laughing out loud\")\n",
        "        .replace(\"nbd\", \"no big deal\")\n",
        "        .replace(\"np\", \"no problem\")\n",
        "        .replace(\"wc\", \"welcome\")\n",
        "        .replace(\"nsfw\", \"not safe for work\")\n",
        "        .replace(\"nvm\", \"never mind\")\n",
        "        .replace(\"omg\", \"oh my god\")\n",
        "        .replace(\"otoh\", \"on the other hand\")\n",
        "        .replace(\"omw\", \"on my way\")\n",
        "        .replace(\"rofl\", \"rolling on floor laughing\")\n",
        "        .replace(\"SO\", \"significant other\")\n",
        "        .replace(\"thx\", \"thanks\")\n",
        "        .replace(\"tmi\", \"too much information\")\n",
        "        .replace(\"ttyl\", \"talk to you later\")\n",
        "        .replace(\"fwiw\", \"for what it’s worth\")\n",
        "        .replace(\"yolo\", \"you only live once\")\n",
        "        .replace(\"tldr\", \"too long, didn't read\")\n",
        "        .replace(\"asap\", \"as soon as possible\")\n",
        "        .replace(\"bau\", \"business as usual\")\n",
        "        .replace(\"fyi\", \"for your information\")\n",
        "        .replace(\"fyip\", \"for your information please\")\n",
        "        .replace(\"fya\", \"for your action\")\n",
        "        .replace(\"fyap\", \"for your action please\")\n",
        "        .replace(\"msg\", \"message\")\n",
        "        .replace(\"fb\", \"facebook\")\n",
        "        .replace(\"txt\", \"text\")\n",
        "        .replace(\"gtg\", \"got to go\")\n",
        "    )\n",
        "    normTweet = (\n",
        "        normTweet.replace(\"'m \", \" 'm \")\n",
        "        .replace(\"'re \", \" 're \")\n",
        "        .replace(\"'s \", \" 's \")\n",
        "        .replace(\"'ll \", \" 'll \")\n",
        "        .replace(\"'d \", \" 'd \")\n",
        "        .replace(\"'ve \", \" 've \")\n",
        "    )\n",
        "    normTweet = (\n",
        "        normTweet.replace(\" p . m .\", \"  p.m.\")\n",
        "        .replace(\" p . m \", \" p.m \")\n",
        "        .replace(\" a . m .\", \" a.m.\")\n",
        "        .replace(\" a . m \", \" a.m \")\n",
        "    )\n",
        "\n",
        "    return \" \".join(normTweet.split())"
      ],
      "metadata": {
        "id": "Bs2mxbaP6YPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "#from TweetNormalizer import normalizeTweet\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-large\")\n",
        "\n",
        "line = normalizeTweet(\"DHEC confirms https://postandcourier.com/health/covid19/sc-has-first-two-presumptive-cases-of-coronavirus-dhec-confirms/article_bddfe4ae-5fd3-11ea-9ce4-5f495366cee6.html?utm_medium=social&utm_source=twitter&utm_campaign=user-share… via @postandcourier 😢\")\n",
        "\n",
        "input_ids = torch.tensor([tokenizer.encode(line)])\n",
        "input_ids\n",
        "'''"
      ],
      "metadata": {
        "id": "GvlvYpUR5Yxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/github/devhemza/BERTweet_sentiment_analysis/blob/main/BERTweet.ipynb#scrollTo=KrZkll3WI1jF"
      ],
      "metadata": {
        "id": "zLhk-SgxvNJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.keras.losses.mse\n",
        "metrics=tf.keras.metrics.mse\n",
        "x = bertweet_model_v1[1]\n",
        "x = tf.keras.layers.Dense(6,activation='sigmoid')(x)\n",
        "output = tf.keras.layers.Rescaling(scale=4.0, offset=1.0)(x)\n",
        "model = tf.keras.models.Model(inputs = [input_ids,attention_mask],outputs = output)\n",
        "model.compile(tf.keras.optimizers.Adam(learning_rate), loss=loss,metrics=metrics)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "V6lZKVR0uhgV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a27f9fd-22d3-42af-bce5-3f723a0ec312"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids_layer (InputLayer)   [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask_layer (InputLay  [(None, 512)]       0           []                               \n",
            " er)                                                                                              \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  134899968  ['input_ids_layer[0][0]',        \n",
            " el)                            thPoolingAndCrossAt               'attention_mask_layer[0][0]']   \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 6)            4614        ['tf_roberta_model[0][1]']       \n",
            "                                                                                                  \n",
            " rescaling_2 (Rescaling)        (None, 6)            0           ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 134,904,582\n",
            "Trainable params: 134,904,582\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit([train_encodings['input_ids'], \n",
        "                                                                         train_encodings['attention_masks']\n",
        "                                                                        ], \n",
        "                                                                        y_train,   \n",
        "                                                                        validation_data =([val_encodings['input_ids'], \n",
        "                                                                                           val_encodings['attention_masks']\n",
        "                                                                                          ], \n",
        "                                                                                          y_val\n",
        "                                                                                         ),    \n",
        "                                                                        batch_size = batch_size, \n",
        "                                                                        epochs = epochs\n",
        "                                                                       )"
      ],
      "metadata": {
        "id": "Y8ZxFpcbuhdQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e0c6a85-7ecb-4ffc-babd-a868a1f52810"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-113-09be57f4cc60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit([train_encodings['input_ids'], \n\u001b[0m\u001b[1;32m      2\u001b[0m                                                                          \u001b[0mtrain_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_masks'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                                         ], \n\u001b[1;32m      4\u001b[0m                                                                         \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                         validation_data =([val_encodings['input_ids'], \n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'model_4/tf_roberta_model/roberta/embeddings/Gather_1' defined at (most recent call last):\n    File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 543, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2854, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3057, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-113-09be57f4cc60>\", line 1, in <module>\n      history = model.fit([train_encodings['input_ids'],\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1023, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/transformers/modeling_tf_utils.py\", line 943, in run_call_with_unpacked_inputs\n      symbolic_weight.name.split(\"/\")[:delimeter]\n    File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_tf_roberta.py\", line 970, in call\n      outputs = self.roberta(\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/transformers/modeling_tf_utils.py\", line 943, in run_call_with_unpacked_inputs\n      symbolic_weight.name.split(\"/\")[:delimeter]\n    File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_tf_roberta.py\", line 662, in call\n      embedding_output = self.embeddings(\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_tf_roberta.py\", line 175, in call\n      position_embeds = tf.gather(params=self.position_embeddings, indices=position_ids)\nNode: 'model_4/tf_roberta_model/roberta/embeddings/Gather_1'\nindices[7,128] = 130 is not in [0, 130)\n\t [[{{node model_4/tf_roberta_model/roberta/embeddings/Gather_1}}]] [Op:__inference_train_function_199926]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Iw0Yg4huhaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IRTfK41muhVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XkRJU9t5uhPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uqEmblJpuhE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qt-cO03dj8K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1AjohReAltEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TRiCbPi1ltIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ghggAzrltLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q3ay_8IUltOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ebpYI17NltRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0h55oFq9ltVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c4kIIZlrlMto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1yBNNaeEj8a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
        "targets = np.array(df_train[col], dtype=\"float32\")\n",
        "\n",
        "MAX_LEN = 512\n",
        "\n",
        "def bert_encode(texts, tokenizer, max_len):\n",
        "    input_ids = []\n",
        "    # token_type_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for text in texts:\n",
        "        token = tokenizer(text, max_length=max_len, truncation=True, padding='max_length',\n",
        "                         add_special_tokens=True)\n",
        "        input_ids.append(token['input_ids'])\n",
        "        # token_type_ids.append(token['token_type_ids'])\n",
        "        attention_mask.append(token['attention_mask'])\n",
        "    \n",
        "    return np.array(input_ids), np.array(attention_mask)\n",
        "\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "\n",
        "ROBERTA_MODEL = \"roberta-base\"\n",
        "\n",
        "bert_tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_MODEL)\n",
        "bert_model = TFRobertaModel.from_pretrained(ROBERTA_MODEL)\n",
        "\n",
        "train_input_ids,train_attention_masks = bert_encode(df_train['full_text'], bert_tokenizer, MAX_LEN)\n",
        "loss = tf.keras.losses.mae\n",
        "metrics=tf.keras.metrics.mae\n",
        "\n",
        "\n",
        "def build_model(model_layer, learning_rate, dense_dim = 6):\n",
        "    \n",
        "    #define inputs\n",
        "    input_ids = tf.keras.Input(shape=(MAX_LEN ,),dtype='int64')\n",
        "    attention_masks = tf.keras.Input(shape=(MAX_LEN ,),dtype='int64')\n",
        " \n",
        "    \n",
        "    #insert BERT layer\n",
        "    transformer_layer = model_layer([input_ids,attention_masks])\n",
        "    \n",
        "    #choose only last hidden-state\n",
        "    x = transformer_layer[1]\n",
        "    x = tf.keras.layers.Dense(dense_dim,activation='sigmoid')(x)\n",
        "    output = tf.keras.layers.Rescaling(scale=4.0, offset=1.0)(x)\n",
        "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n",
        "\n",
        "    model.compile(tf.keras.optimizers.Adam(learning_rate), loss=loss,metrics=metrics)\n",
        "    \n",
        "    return model\n",
        "\n",
        "BERT= build_model(bert_model,learning_rate=1e-5)\n",
        "BERT.summary()\n",
        "\n",
        "history = BERT.fit((train_input_ids,train_attention_masks),targets,batch_size =4,        \n",
        "                epochs=2,\n",
        "                validation_split = 0.2\n",
        "                )\n",
        "\n"
      ],
      "metadata": {
        "id": "vY1Fd4w-j8cw",
        "outputId": "a36dd45f-06b7-4f6f-db20-646cde4e703d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model_3 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " odel)                          thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 6)            4614        ['tf_roberta_model_3[0][1]']     \n",
            "                                                                                                  \n",
            " rescaling_1 (Rescaling)        (None, 6)            0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 124,650,246\n",
            "Trainable params: 124,650,246\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/2\n",
            "626/626 [==============================] - 389s 551ms/step - loss: 0.4274 - mean_absolute_error: 0.4274 - val_loss: 0.3861 - val_mean_absolute_error: 0.3861\n",
            "Epoch 2/2\n",
            "626/626 [==============================] - 358s 572ms/step - loss: 0.3784 - mean_absolute_error: 0.3784 - val_loss: 0.3870 - val_mean_absolute_error: 0.3870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MQvcdlEarMy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tPw11bcvrNMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BtcrS9n7rNPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
        "targets = np.array(df_train[col], dtype=\"float32\")\n",
        "\n",
        "MAX_LEN = 512\n",
        "\n",
        "def bert_encode(texts, tokenizer, max_len):\n",
        "    input_ids = []\n",
        "    # token_type_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for text in texts:\n",
        "        token = tokenizer(text, max_length=max_len, truncation=True, padding='max_length',\n",
        "                         add_special_tokens=True)\n",
        "        input_ids.append(token['input_ids'])\n",
        "        # token_type_ids.append(token['token_type_ids'])\n",
        "        attention_mask.append(token['attention_mask'])\n",
        "    \n",
        "    return np.array(input_ids), np.array(attention_mask)\n",
        "\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "\n",
        "ROBERTA_MODEL = \"roberta-base\"\n",
        "\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_MODEL)\n",
        "roberta_model = TFRobertaModel.from_pretrained(ROBERTA_MODEL)\n",
        "\n",
        "train_input_ids,train_attention_masks = bert_encode(df_train['full_text'], bert_tokenizer, MAX_LEN)\n",
        "loss = tf.keras.losses.mse\n",
        "metrics=tf.keras.metrics.mse\n",
        "\n",
        "\n",
        "def build_model(model_layer, learning_rate, dense_dim = 6):\n",
        "    \n",
        "    #define inputs\n",
        "    input_ids = tf.keras.Input(shape=(MAX_LEN ,),dtype='int64')\n",
        "    attention_masks = tf.keras.Input(shape=(MAX_LEN ,),dtype='int64')\n",
        " \n",
        "    \n",
        "    #insert BERT layer\n",
        "    transformer_layer = model_layer([input_ids,attention_masks])\n",
        "    \n",
        "    #choose only last hidden-state\n",
        "    x = transformer_layer[1]\n",
        "    x = tf.keras.layers.Dense(dense_dim,activation=None)(x)\n",
        "    output = tf.keras.layers.Rescaling(scale=4.0, offset=1.0)(x)\n",
        "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n",
        "\n",
        "    model.compile(tf.keras.optimizers.Adam(learning_rate), loss=loss,metrics=metrics)\n",
        "    \n",
        "    return model\n",
        "\n",
        "ROBERTA= build_model(roberta_model,learning_rate=1e-5)\n",
        "ROBERTA.summary()\n",
        "print(keras.utils.plot_model(ROBERTA, show_shapes=False, show_dtype=False, show_layer_names=True, dpi=90))\n",
        "history = ROBERTA.fit((train_input_ids,train_attention_masks),\n",
        "                       targets,\n",
        "                       batch_size =8,        \n",
        "                       epochs=5,\n",
        "                       validation_split = 0.2\n",
        "                      )\n",
        "\n"
      ],
      "metadata": {
        "id": "5JYGg6iCrNSh",
        "outputId": "44aa8dc2-47d2-4e09-c6e9-56d4012fa84c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model_4 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
            " odel)                          thPoolingAndCrossAt               'input_4[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 512,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 6)            4614        ['tf_roberta_model_4[0][1]']     \n",
            "                                                                                                  \n",
            " rescaling_2 (Rescaling)        (None, 6)            0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 124,650,246\n",
            "Trainable params: 124,650,246\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "<IPython.core.display.Image object>\n",
            "Epoch 1/5\n",
            "313/313 [==============================] - 370s 1s/step - loss: 0.4484 - mean_squared_error: 0.4484 - val_loss: 0.3470 - val_mean_squared_error: 0.3470\n",
            "Epoch 2/5\n",
            "278/313 [=========================>....] - ETA: 33s - loss: 0.2663 - mean_squared_error: 0.2663"
          ]
        }
      ]
    }
  ]
}