{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilamaiti/spring_2023_w266_final_project_heesuk_iris_srila/blob/main/srila/roberta_ell.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing new libraries**"
      ],
      "metadata": {
        "id": "Lhf_T8cMjGsp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD4BChywisTm",
        "outputId": "5bbc1f19-f029-4fae-f00a-dbec227c13ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.1.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.9/dist-packages (1.8.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from wordcloud) (8.4.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.9/dist-packages (from wordcloud) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (23.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (5.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (4.39.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->wordcloud) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install wordcloud\n",
        "!pip install transformers\n",
        "!pip install emoji==0.6.0\n",
        "!pip3 install tokenizers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing libraries**"
      ],
      "metadata": {
        "id": "bC3s_6EZjMBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(f'transformers version: {transformers.__version__}')\n",
        "from transformers import logging as hf_logging\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "hf_logging.set_verbosity_error()\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import spacy      \n",
        "from spacy import displacy\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS\n",
        "from wordcloud import ImageColorGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "nltk.download('punkt')\n",
        "ROBERTA_MODEL = \"roberta-base\"\n",
        "\n",
        "# Other required libraries\n",
        "import math\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import copy\n",
        "import sys\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Data visualization libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tensorflow libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils.layer_utils import count_params\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.losses import mae\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from keras.models import Model\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.models import load_model\n",
        "\n",
        "import torch\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "MaPH5XqxjU_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General functions**"
      ],
      "metadata": {
        "id": "5fjpBUG5jbmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Rounding Off to Custom Decimal Places**"
      ],
      "metadata": {
        "id": "ahNGWcMIFD8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def roundPartial(value, resolution):\n",
        "    return round (value / resolution) * resolution"
      ],
      "metadata": {
        "id": "5opWTwwnFLLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Set parameters**"
      ],
      "metadata": {
        "id": "FYfihW3Mjgkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_config_param(seed = 99):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    tf.keras.backend.clear_session()\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/Kaggle\"\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    \n",
        "    \n",
        "set_config_param(20230214)"
      ],
      "metadata": {
        "id": "pr1-rS8tjiqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Plot loss and accuracy**"
      ],
      "metadata": {
        "id": "6yYB47Gdjo0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_accuracy(history, col_list):\n",
        "    fig, ax = plt.subplots(2, 6, figsize=(16, 6), sharex='col', sharey='row')\n",
        "    fig.tight_layout(pad=5.0)\n",
        "    for idx, col in enumerate(col_list):\n",
        "\n",
        "        ax[0, idx].plot(history[col + '_loss'], lw=2, color='darkgoldenrod')\n",
        "        ax[0, idx].plot(history['val_' + col + '_loss'], lw=2, color='indianred')\n",
        "        #ax[0, idx].legend(loc='center left')\n",
        "        ax[0, idx].legend(['Train', 'Validation'], fontsize=5)\n",
        "        ax[0, idx].set_xlabel('Epochs', size=10)\n",
        "        ax[0, idx].set_title('Loss: ' + col)\n",
        "\n",
        "        ax[1, idx].plot(history[col + '_accuracy'], lw=2, color='darkgoldenrod')\n",
        "        ax[1, idx].plot(history['val_' + col + '_accuracy'], lw=2, color='indianred')\n",
        "        #ax[0, idx].legend(loc='center left')\n",
        "        ax[1, idx].legend(['Train', 'Validation'], fontsize=5)\n",
        "        ax[1, idx].set_xlabel('Epochs', size=10)\n",
        "        ax[1, idx].set_title('Accuracy: ' + col)"
      ],
      "metadata": {
        "id": "BE9NqiNWjrPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Encode**"
      ],
      "metadata": {
        "id": "k3y3byq0Fk9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_encode(texts, tokenizer, max_len):\n",
        "    input_ids = []\n",
        "    # token_type_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for text in texts:\n",
        "        token = tokenizer(text, max_length=max_len, truncation=True, padding='max_length',\n",
        "                         add_special_tokens=True)\n",
        "        input_ids.append(token['input_ids'])\n",
        "        # token_type_ids.append(token['token_type_ids'])\n",
        "        attention_mask.append(token['attention_mask'])\n",
        "    \n",
        "    return np.array(input_ids), np.array(attention_mask)"
      ],
      "metadata": {
        "id": "cSxlBmVZFm2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Busild Base Model**"
      ],
      "metadata": {
        "id": "EdYaqDeEGJE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_base_model(model_layer, learning_rate, dense_dim = 6):\n",
        "    \n",
        "    #define inputs\n",
        "    input_ids = tf.keras.Input(shape=(MAX_LEN ,),dtype='int64', name = 'input_ids')\n",
        "    attention_masks = tf.keras.Input(shape=(MAX_LEN ,),dtype='int64', name = 'attention_masks')\n",
        "    \n",
        "    #insert BERT layer\n",
        "    transformer_layer = model_layer([input_ids,attention_masks])\n",
        "    \n",
        "    #choose only last hidden-state\n",
        "    x = transformer_layer[1]\n",
        "    output = tf.keras.layers.Dense(dense_dim)(x)\n",
        "    #output = tf.keras.layers.Rescaling(scale=4.0, offset=1.0)(x)\n",
        "    model = tf.keras.models.Model(inputs = [input_ids, attention_masks],outputs = output)\n",
        "\n",
        "    model.compile(tf.keras.optimizers.Adam(learning_rate), loss=loss,metrics=metrics)\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "202ecOu4GLFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Custom metric**"
      ],
      "metadata": {
        "id": "uJw9YpJXjwSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MCRMSE(y_true, y_pred):\n",
        "    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n",
        "    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=-1, keepdims=True).numpy()[0]"
      ],
      "metadata": {
        "id": "_VCdZF9ZjzN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read input files**"
      ],
      "metadata": {
        "id": "zP0n_Dd8j26W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_train_df = pd.read_csv('train.csv')\n",
        "input_test_df = pd.read_csv('test.csv')\n",
        "label_cols = input_train_df.columns[2:]\n",
        "orig_train_df = copy.deepcopy(input_train_df)\n",
        "orig_train_df.head()"
      ],
      "metadata": {
        "id": "Le5HU_86j4-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Spliting the data**\n",
        "Original test data is very limited, there are only 3 records and it does not have labels to test. So we decided to repurpose the given train data to split into train, test and validation sets."
      ],
      "metadata": {
        "id": "A4DOnrFpkBz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle = np.random.permutation(np.arange(orig_train_df.shape[0]))\n",
        "orig_train_df = orig_train_df.iloc[shuffle]\n",
        "split=(0.8,0.1,0.1)\n",
        "splits = np.multiply(len(orig_train_df), split).astype(int)\n",
        "df_train, df_val, df_test = np.split(orig_train_df, [splits[0], splits[0] + splits[1]])\n",
        "\n",
        "X_train, X_val, X_test = df_train['full_text'], df_val['full_text'], df_test['full_text']\n",
        "y_train, y_val, y_test = np.array(df_train[label_cols]), np.array(df_val[label_cols]), np.array(df_test[label_cols])"
      ],
      "metadata": {
        "id": "ZtWKcw09kKZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model building**"
      ],
      "metadata": {
        "id": "6LVCmgcikTTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_config_param()\n",
        "\n",
        "MAX_LEN = 512\n",
        "epochs = 5\n",
        "batch_size = 4\n",
        "learning_rate = 1e-5\n",
        "validation_split = .2\n",
        "dropout = .1\n",
        "number_of_hidden_layer = 1\n",
        "hidden_layer_node_count = 64\n",
        "trainable_flag = False\n",
        "retrain_layer_count = 0"
      ],
      "metadata": {
        "id": "Uqi7K_9ylr37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original test data is very limited, there are only 3 records and it does not have labels to test. So we decided to repurpose the given train data to split into train, test and validation sets."
      ],
      "metadata": {
        "id": "JZR9ezimKiaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle = np.random.permutation(np.arange(orig_train_df.shape[0]))\n",
        "orig_train_df = orig_train_df.iloc[shuffle]\n",
        "split=(0.8,0.2)\n",
        "splits = np.multiply(len(orig_train_df), split).astype(int)\n",
        "df_train, df_test = orig_train_df[ : splits[0]], orig_train_df[splits[0] : ]\n",
        "\n",
        "label_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
        "y_train = np.array(df_train[label_cols], dtype=\"float32\")\n",
        "y_test = np.array(df_test[label_cols], dtype=\"float32\")"
      ],
      "metadata": {
        "id": "dgqFkYcbKjUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_MODEL)\n",
        "roberta_model = TFRobertaModel.from_pretrained(ROBERTA_MODEL)\n",
        "\n",
        "train_input_ids,train_attention_masks = text_encode(df_train['full_text'], roberta_tokenizer, MAX_LEN)\n",
        "test_input_ids,test_attention_masks = text_encode(df_test['full_text'], roberta_tokenizer, MAX_LEN)\n",
        "\n",
        "loss = tf.keras.losses.mse\n",
        "metrics=tf.keras.metrics.mse\n",
        "\n",
        "ROBERTA_v1 = build_base_model(roberta_model,learning_rate=1e-5)\n",
        "ROBERTA_v1.summary()\n",
        "\n",
        "history_v1 = ROBERTA_v1.fit((train_input_ids,train_attention_masks),\n",
        "                            y_train,\n",
        "                            batch_size = batch_size,        \n",
        "                            epochs = epochs,\n",
        "                            validation_split = validation_split\n",
        "                            )\n",
        "\n",
        "history_v1_df = pd.DataFrame(history_v1.history)\n",
        "print(history_v1_df.T)\n",
        "\n",
        "score_v1 = ROBERTA_v1.evaluate([test_input_ids, test_attention_masks], \n",
        "                               y_test\n",
        "                              ) \n",
        "print(pd.DataFrame(score_v1))\n",
        "\n",
        "predictions_v1 = ROBERTA_v1.predict([test_input_ids, test_attention_masks])\n",
        "df_pred_v1 = pd.DataFrame(predictions_v1, columns=['pred_' + c for c in label_cols])\n",
        "#print(\"Before transform...\")\n",
        "#print(df_pred_v1.head())\n",
        "#print(df_pred_v1.tail())\n",
        "\n",
        "for col in label_cols:\n",
        "    df_pred_v1['transformed_pred_' + col] = df_pred_v1['pred_' + col].apply(lambda x : roundPartial(x, .5))\n",
        "#print(\"After transform...\")\n",
        "print(df_pred_v1.head())\n",
        "print(df_pred_v1.tail())\n",
        "\n",
        "df_compare_v1= pd.merge(df_test, df_pred_v1, left_index=True, right_index=True)\n",
        "\n",
        "x_arr = np.arange(len(history_v1_df['loss'])) + 1\n",
        "fig = plt.figure(figsize=(12, 4))\n",
        "ax = fig.add_subplot(1, 2, 1)\n",
        "ax.plot(x_arr, history_v1_df['loss'], '-o', label='Train loss')\n",
        "ax.plot(x_arr, history_v1_df['val_loss'], '--<', label='Validation loss')\n",
        "ax.legend(fontsize=15)\n",
        "ax.set_xlabel('Epoch', size=15)\n",
        "ax.set_ylabel('Loss', size=15)\n",
        "\n",
        "ax = fig.add_subplot(1, 2, 2)\n",
        "ax.plot(x_arr, history_v1_df['mean_squared_error'], '-o', label='Train MSE')\n",
        "ax.plot(x_arr, history_v1_df['val_mean_squared_error'], '--<', label='Validation MSE')\n",
        "ax.legend(fontsize=15)\n",
        "ax.set_xlabel('Epoch', size=15)\n",
        "ax.set_ylabel('Accuracy', size=15)\n",
        "ax.set_ylim(0,1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5JYGg6iCrNSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(ROBERTA_v1, show_shapes=False, show_dtype=False, show_layer_names=True, dpi=90)"
      ],
      "metadata": {
        "id": "MA6T_FXG0E-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_compare_v1"
      ],
      "metadata": {
        "id": "pAfdCoed0FKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_col_list = ['transformed_pred_' + col for col in label_cols]\n",
        "MCRMSE(np.array(df_test[label_cols]), np.array(df_pred_v1[pred_col_list]))"
      ],
      "metadata": {
        "id": "-gOf41GG-f0B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}