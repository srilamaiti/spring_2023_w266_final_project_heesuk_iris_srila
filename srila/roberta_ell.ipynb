{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilamaiti/spring_2023_w266_final_project_heesuk_iris_srila/blob/main/srila/roberta_ell.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing new libraries**"
      ],
      "metadata": {
        "id": "Lhf_T8cMjGsp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fD4BChywisTm",
        "outputId": "1d6a3ab6-ee72-4cef-e701-deeda7734251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji==0.6.0\n",
            "  Downloading emoji-0.6.0.tar.gz (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 KB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.6.0-py3-none-any.whl size=49732 sha256=0832e6eb522eb3dcb572187f9c7e5c04c12045bffdf81ddde5c1b1d2593a5ea3\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/2a/7f/1a0012c86b1061c6ee2ed9568b1f830f857a51e8e416452af2\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-multilearn\n",
            "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting iterative-stratification\n",
            "  Downloading iterative_stratification-0.1.7-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from iterative-stratification) (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from iterative-stratification) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from iterative-stratification) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->iterative-stratification) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->iterative-stratification) (1.1.1)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.11.0\n",
            "  Downloading tensorflow-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf<3.20,>=3.9.2\n",
            "  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
            "Collecting keras<2.12,>=2.11.0\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (23.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (0.32.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (23.3.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (4.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.4.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (0.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.53.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (3.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (2.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (3.3.0)\n",
            "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (67.6.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (16.0.0)\n",
            "Collecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.40.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.17.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.4.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.2.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (6.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard-data-server, protobuf, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.0\n",
            "    Uninstalling tensorboard-data-server-0.7.0:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.0\n",
            "    Uninstalling tensorboard-2.12.0:\n",
            "      Successfully uninstalled tensorboard-2.12.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed keras-2.11.0 protobuf-3.19.6 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: yellowbrick in /usr/local/lib/python3.9/dist-packages (1.5)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from yellowbrick) (3.7.1)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from yellowbrick) (0.11.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from yellowbrick) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from yellowbrick) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from yellowbrick) (1.22.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (8.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (23.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (4.39.3)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (5.12.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.0->yellowbrick) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.0->yellowbrick) (3.1.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "#!pip install nltk\n",
        "#!pip install wordcloud\n",
        "#!pip3 install tokenizers\n",
        "#!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install emoji==0.6.0\n",
        "!pip install scikit-multilearn\n",
        "!pip install iterative-stratification\n",
        "!pip install tensorflow==2.11.0\n",
        "!pip install yellowbrick"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing libraries**"
      ],
      "metadata": {
        "id": "bC3s_6EZjMBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(f'transformers version: {transformers.__version__}')\n",
        "from transformers import logging as hf_logging\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "hf_logging.set_verbosity_error()\n",
        "'''\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import spacy      \n",
        "from spacy import displacy\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS\n",
        "from wordcloud import ImageColorGenerator\n",
        "nltk.download('punkt')\n",
        "'''\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "ROBERTA_MODEL = \"roberta-base\"\n",
        "\n",
        "# Other required libraries\n",
        "import math\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import copy\n",
        "import sys\n",
        "import gc\n",
        "\n",
        "# data visualization\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "\n",
        "# others\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from scipy.cluster.hierarchy import set_link_color_palette\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "from skmultilearn.model_selection import iterative_train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# distances\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Data visualization libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tensorflow libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils.layer_utils import count_params\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.losses import mae\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from keras.models import Model\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.models import load_model\n",
        "\n",
        "import torch\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaPH5XqxjU_z",
        "outputId": "34f0323d-6086-4c30-bb14-d5e547b49911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers version: 4.27.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General functions**"
      ],
      "metadata": {
        "id": "5fjpBUG5jbmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Rounding Off to Custom Decimal Places**"
      ],
      "metadata": {
        "id": "ahNGWcMIFD8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def roundPartial(value, resolution):\n",
        "    return round (value / resolution) * resolution"
      ],
      "metadata": {
        "id": "5opWTwwnFLLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Set parameters**"
      ],
      "metadata": {
        "id": "FYfihW3Mjgkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_config_param(seed = 99):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    tf.keras.backend.clear_session()\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/Kaggle\"\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    \n",
        "    \n",
        "set_config_param(20230214)"
      ],
      "metadata": {
        "id": "pr1-rS8tjiqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Plot loss and accuracy**"
      ],
      "metadata": {
        "id": "6yYB47Gdjo0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_accuracy(history, col_list):\n",
        "    fig, ax = plt.subplots(2, 6, figsize=(16, 6), sharex='col', sharey='row')\n",
        "    fig.tight_layout(pad=5.0)\n",
        "    for idx, col in enumerate(col_list):\n",
        "\n",
        "        ax[0, idx].plot(history[col + '_loss'], lw=2, color='darkgoldenrod')\n",
        "        ax[0, idx].plot(history['val_' + col + '_loss'], lw=2, color='indianred')\n",
        "        #ax[0, idx].legend(loc='center left')\n",
        "        ax[0, idx].legend(['Train', 'Validation'], fontsize=5)\n",
        "        ax[0, idx].set_xlabel('Epochs', size=10)\n",
        "        ax[0, idx].set_title('Loss: ' + col)\n",
        "\n",
        "        ax[1, idx].plot(history[col + '_accuracy'], lw=2, color='darkgoldenrod')\n",
        "        ax[1, idx].plot(history['val_' + col + '_accuracy'], lw=2, color='indianred')\n",
        "        #ax[0, idx].legend(loc='center left')\n",
        "        ax[1, idx].legend(['Train', 'Validation'], fontsize=5)\n",
        "        ax[1, idx].set_xlabel('Epochs', size=10)\n",
        "        ax[1, idx].set_title('Accuracy: ' + col)"
      ],
      "metadata": {
        "id": "BE9NqiNWjrPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Plot Loss and other KPI specified**"
      ],
      "metadata": {
        "id": "dII35P-P_J_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_plot(df, kpi_name, kpi_string):\n",
        "    x_arr = np.arange(len(df['loss'])) + 1\n",
        "    fig = plt.figure(figsize=(12, 4))\n",
        "    ax = fig.add_subplot(1, 2, 1)\n",
        "    ax.plot(x_arr, df['loss'], '-o', label = 'Train loss')\n",
        "    ax.plot(x_arr, df['val_loss'], '--<', label = 'Validation loss')\n",
        "    ax.legend(fontsize = 15)\n",
        "    ax.set_xlabel('Epoch', size = 15)\n",
        "    ax.set_ylabel('Loss', size = 15)\n",
        "\n",
        "    ax = fig.add_subplot(1, 2, 2)\n",
        "    ax.plot(x_arr, df[kpi_name], '-o', label = 'Train ' + kpi_string)\n",
        "    ax.plot(x_arr, df['val_' + kpi_name], '--<', label = 'Validation ' + kpi_string)\n",
        "    ax.legend(fontsize = 15)\n",
        "    ax.set_xlabel('Epoch', size = 15)\n",
        "    ax.set_ylabel('Accuracy', size = 15)\n",
        "    #ax.set_ylim(0,1)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yQgOGDu6_PVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Encode**"
      ],
      "metadata": {
        "id": "k3y3byq0Fk9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_encode(texts, tokenizer, max_len):\n",
        "    input_ids = []\n",
        "    # token_type_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for text in texts:\n",
        "        token = tokenizer(text, \n",
        "                          max_length = max_len, \n",
        "                          truncation = True, \n",
        "                          padding = 'max_length',\n",
        "                          add_special_tokens = True)\n",
        "        input_ids.append(token['input_ids'])\n",
        "        # token_type_ids.append(token['token_type_ids'])\n",
        "        attention_mask.append(token['attention_mask'])\n",
        "    \n",
        "    return np.array(input_ids), np.array(attention_mask)"
      ],
      "metadata": {
        "id": "cSxlBmVZFm2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Custom metric**"
      ],
      "metadata": {
        "id": "uJw9YpJXjwSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MCRMSE(y_true, y_pred):\n",
        "    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis = 1)\n",
        "    return tf.reduce_mean(tf.sqrt(colwise_mse), axis = -1, keepdims = True)"
      ],
      "metadata": {
        "id": "_VCdZF9ZjzN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Build Base Model**"
      ],
      "metadata": {
        "id": "EdYaqDeEGJE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_regression_model(loss = 'MCRMSE',\n",
        "                           model_name = 'Roberta', \n",
        "                           dense_dim = 6, \n",
        "                           MAX_LEN = 512,\n",
        "                           learning_rate = 1e-5,\n",
        "                           dropout = .1,\n",
        "                           number_of_hidden_layers = 1,\n",
        "                           hidden_layer_node_count = 64,\n",
        "                           retrain_layer_count = 0):\n",
        "    \n",
        "    # Define inputs\n",
        "    input_ids = tf.keras.Input(shape = (MAX_LEN ,), dtype = 'int64', name = 'input_ids')\n",
        "    attention_masks = tf.keras.Input(shape = (MAX_LEN ,), dtype = 'int64', name = 'attention_masks')\n",
        "    \n",
        "    if model_name == 'Roberta':\n",
        "        model_tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_MODEL)\n",
        "        model = TFRobertaModel.from_pretrained(ROBERTA_MODEL)\n",
        "\n",
        "    # Adjust the trainable layer weights based on retrain_layer_count\n",
        "    # If retrain_layer_count is 0, then base model is frozen.\n",
        "    # If retrain_layer_count is 12, then the entire base model is trainable.\n",
        "    # And that implies that all the pretrained weights are lost and it relearns\n",
        "    # from the input data.\n",
        "    # If retrain_layer_count is between 1 and 11, then the last n layers of\n",
        "    # the pretrained model retrained.\n",
        "    if retrain_layer_count == 0:\n",
        "        # The pretained model is frozen\n",
        "        model.trainable = False           \n",
        "\n",
        "    elif retrain_layer_count == 12:  \n",
        "        # The pretrained model is retrained thru all layers.       \n",
        "        model.trainable = True     \n",
        "\n",
        "    else:    \n",
        "        # Restrict training to the num_train_layers outer transformer layers\n",
        "        retrain_layer_list = []\n",
        "        model.trainable = False  \n",
        "        for retrain_layer_number in range(retrain_layer_count):\n",
        "\n",
        "            layer_code = '_' + str(11 - retrain_layer_number)\n",
        "            retrain_layer_list.append(layer_code)\n",
        "        \n",
        "        print('Retrain layers: \\n', retrain_layer_list)\n",
        "        #model.compile()\n",
        "        print(f\"Number of trainable parameters : {count_params(model.trainable_weights)}\")\n",
        "        print(f\"Number of non-trainable parameters : {count_params(model.non_trainable_variables)}\")\n",
        "        for weight in model.weights:\n",
        "            weight._trainable = False\n",
        "            #print(\"***\", layer.name, layer._trainable)\n",
        "            if 'layer_' in weight.name and weight.name.split(\".\")[1].split(\"/\")[0] in retrain_layer_list:\n",
        "                weight._trainable = True\n",
        "                print(\"$$$\", weight.name, weight._trainable)\n",
        "            elif 'layer_' not in weight.name :\n",
        "                weight._trainable = True\n",
        "                print(\"###\", weight.name, weight._trainable)\n",
        "        model.compile()\n",
        "\n",
        "        for weight_details in model.weights:\n",
        "            print(weight_details.name, weight_details.trainable)\n",
        "    print(f\"Number of trainable parameters : {count_params(model.trainable_weights)}\")\n",
        "    print(f\"Number of non-trainable parameters : {count_params(model.non_trainable_variables)}\")\n",
        "                \n",
        "    # Insert pretrained model layer\n",
        "    pretrained_transformer = model([input_ids, attention_masks])\n",
        "\n",
        "    # Get the CLS output off the pretrained model\n",
        "    cls_token = pretrained_transformer[0][:, 0, :]\n",
        "\n",
        "    # Append the hidden layer and dropout layer\n",
        "    layer_list = []\n",
        "    for layer in range(number_of_hidden_layers):\n",
        "        if layer == 0:\n",
        "            hidden_layer = tf.keras.layers.Dense(units      = hidden_layer_node_count\n",
        "                                               , activation = 'relu'\n",
        "                                               , name       = 'hidden_layer_' + str(layer + 1)\n",
        "                                                )(cls_token)\n",
        "        else:\n",
        "            hidden_layer = tf.keras.layers.Dense(units      = hidden_layer_node_count\n",
        "                                               , activation = 'relu'\n",
        "                                               , name       = 'hidden_layer_' + str(layer + 1)\n",
        "                                            )(layer_list[-1])\n",
        "        layer_list.append(hidden_layer)\n",
        "        dropout_layer = tf.keras.layers.Dropout(dropout, \n",
        "                                                name = 'dropout_layer_' + str(layer + 1)\n",
        "                                               )(hidden_layer) \n",
        "        layer_list.append(dropout_layer)\n",
        "\n",
        "    # Add the output layer\n",
        "    output = tf.keras.layers.Dense(6,)(layer_list[-1])\n",
        "\n",
        "    # Build the final model\n",
        "    regression_model = tf.keras.Model(inputs = [input_ids, attention_masks], outputs = output)\n",
        "    \n",
        "    # Model compile\n",
        "    if loss == 'MCRMSE':\n",
        "        regression_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
        "                                 loss      = MCRMSE,\n",
        "                                 metrics   = MCRMSE\n",
        "                                )\n",
        "    \n",
        "    print(regression_model.summary())\n",
        "    keras.utils.plot_model(regression_model, \n",
        "                           show_shapes = False, \n",
        "                           show_dtype = False, \n",
        "                           show_layer_names = True, \n",
        "                           dpi = 90)\n",
        "    return regression_model\n",
        "\n",
        "'''\n",
        "regression_model = build_regression_model(\n",
        "                       loss = 'MCRMSE',\n",
        "                       model_name = 'Roberta', \n",
        "                       dense_dim = 6, \n",
        "                       MAX_LEN = 512,\n",
        "                       learning_rate = 1e-5,\n",
        "                       dropout = .1,\n",
        "                       number_of_hidden_layers = 1,\n",
        "                       hidden_layer_node_count = 64,\n",
        "                       retrain_layer_count = 6)\n",
        "'''"
      ],
      "metadata": {
        "id": "brWkfFsUk0wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_fit(model, \n",
        "              df_train, \n",
        "              train_indices,\n",
        "              val_indices,\n",
        "              model_name = 'Roberta', \n",
        "              MAX_LEN = 512,\n",
        "              epochs = 5,\n",
        "              batch_size = 4,\n",
        "              validation_split = .2):\n",
        "  \n",
        "    # Building the tokenizer for the given model\n",
        "    if model_name == 'Roberta':\n",
        "        tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_MODEL)\n",
        "\n",
        "    train_encoded_input_ids, train_encoded_attention_masks = text_encode(df_train.iloc[list(train_indices)]['full_text'], tokenizer, MAX_LEN)\n",
        "    val_encoded_input_ids, val_encoded_attention_masks = text_encode(df_train.iloc[list(val_indices)]['full_text'], tokenizer, MAX_LEN)\n",
        "\n",
        "    y_train = np.array(df_train.iloc[list(train_indices)][label_cols], dtype = \"float32\")\n",
        "    y_val = np.array(df_train.iloc[list(val_indices)][label_cols], dtype = \"float32\")\n",
        "    \n",
        "    hist = model.fit([train_encoded_input_ids, train_encoded_attention_masks],\n",
        "                     y_train,\n",
        "                     validation_data = ([val_encoded_input_ids, val_encoded_attention_masks], \n",
        "                                        y_val\n",
        "                                       ),\n",
        "                     batch_size = batch_size,        \n",
        "                     epochs = epochs\n",
        "                    )\n",
        "\n",
        "    df_history = pd.DataFrame(hist.history)\n",
        "    return df_history"
      ],
      "metadata": {
        "id": "SzxEshCMv3Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_base_model(model_layer, learning_rate, dense_dim = 6):\n",
        "    \n",
        "    #define inputs\n",
        "    input_ids = tf.keras.Input(shape = (MAX_LEN ,), dtype = 'int64', name = 'input_ids')\n",
        "    attention_masks = tf.keras.Input(shape = (MAX_LEN ,), dtype = 'int64', name = 'attention_masks')\n",
        "    \n",
        "    #insert BERT layer\n",
        "    transformer_layer = model_layer([input_ids, attention_masks])\n",
        "    \n",
        "    #choose only last hidden-state\n",
        "    x = transformer_layer[1]\n",
        "    output = tf.keras.layers.Dense(dense_dim)(x)\n",
        "    #output = tf.keras.layers.Rescaling(scale=4.0, offset=1.0)(x)\n",
        "    model = tf.keras.models.Model(inputs = [input_ids, attention_masks], outputs = output)\n",
        "\n",
        "    model.compile(tf.keras.optimizers.Adam(learning_rate), loss = mse_loss, metrics = mse_metrics)\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "202ecOu4GLFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Build a model with custom loss**"
      ],
      "metadata": {
        "id": "XmCPltuehyyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_base_model_with_custom_loss(model_layer, learning_rate, dense_dim = 6):\n",
        "    \n",
        "    #define inputs\n",
        "    input_ids = tf.keras.Input(shape = (MAX_LEN ,), dtype = 'int64', name = 'input_ids')\n",
        "    attention_masks = tf.keras.Input(shape = (MAX_LEN ,), dtype = 'int64', name = 'attention_masks')\n",
        "    \n",
        "    #insert BERT layer\n",
        "    transformer_layer = model_layer([input_ids, attention_masks])\n",
        "    \n",
        "    #choose only last hidden-state\n",
        "    x = transformer_layer[1]\n",
        "    output = tf.keras.layers.Dense(dense_dim)(x)\n",
        "    #output = tf.keras.layers.Rescaling(scale=4.0, offset=1.0)(x)\n",
        "    model = tf.keras.models.Model(inputs = [input_ids, attention_masks], outputs = output)\n",
        "\n",
        "    model.compile(tf.keras.optimizers.Adam(learning_rate), loss = MCRMSE, metrics = MCRMSE)\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "lnaCwE2Uhx-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Evaluate**"
      ],
      "metadata": {
        "id": "96pZ-IUrCD30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, y_test, test_encoded_input_ids, test_encoded_attention_masks):\n",
        "    score = model.evaluate([test_encoded_input_ids, test_encoded_attention_masks], \n",
        "                           y_test\n",
        "                          ) \n",
        "    print('\\nTest Loss : {:.2f}%'.format(score[0]))\n",
        "    print('\\nTest Accuracy :  {:.2f}%'.format(score[1]))\n",
        "    return score[0], score[1]"
      ],
      "metadata": {
        "id": "mQ1hLAW2CGm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Predict**"
      ],
      "metadata": {
        "id": "wJV4COxjCc9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_model(model, df_test, test_encoded_input_ids, test_encoded_attention_masks, label_cols):\n",
        "    predictions = model.predict([test_encoded_input_ids, test_encoded_attention_masks])\n",
        "    df_predictions = pd.DataFrame(predictions, columns=['pred_' + c for c in label_cols])\n",
        "    for col in label_cols:\n",
        "        df_predictions['transformed_pred_' + col] = df_predictions['pred_' + col].apply(lambda x : roundPartial(x, .5))\n",
        "    df_comparison = pd.merge(df_test, df_predictions, left_index = True, right_index = True)\n",
        "    return df_predictions, df_comparison"
      ],
      "metadata": {
        "id": "PJsH7HXpCiAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Plot Model Structure**"
      ],
      "metadata": {
        "id": "4CGFbdlnFEMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_model_structure(model):\n",
        "    keras.utils.plot_model(model, show_shapes = False, show_dtype = False, show_layer_names = True, dpi = 90)"
      ],
      "metadata": {
        "id": "tmSoWa7kFHCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read input files**"
      ],
      "metadata": {
        "id": "zP0n_Dd8j26W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_train_df = pd.read_csv('train.csv')\n",
        "input_test_df = pd.read_csv('test.csv')\n",
        "# Cleaning up full_text : Removing tabl and carriage return characters\n",
        "input_train_df['full_text'] = input_train_df[\"full_text\"].replace(re.compile(r'[\\n\\r\\t]'), ' ', regex = True)\n",
        "input_test_df['full_text'] = input_test_df[\"full_text\"].replace(re.compile(r'[\\n\\r\\t]'), ' ', regex = True)\n",
        "\n",
        "label_cols = input_train_df.columns[2:]\n",
        "input_train_df['score_sum'] = np.sum(input_train_df[label_cols], axis = 1)\n",
        "pred_col_list = ['transformed_pred_' + col for col in label_cols]\n",
        "\n",
        "orig_train_df = copy.deepcopy(input_train_df)\n",
        "orig_train_df.head()"
      ],
      "metadata": {
        "id": "Le5HU_86j4-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we do not have labels for our test data, we are repurposing our training data by splitting it into 80:20 ratio."
      ],
      "metadata": {
        "id": "KsToL7McZbu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffling them back again\n",
        "shuffle = np.random.permutation(np.arange(orig_train_df.shape[0]))\n",
        "orig_train_df = orig_train_df.iloc[shuffle]\n",
        "\n",
        "# Splitting the data in 80:20 split\n",
        "split = (0.8, 0.2)\n",
        "splits = np.multiply(len(orig_train_df), split).astype(int)\n",
        "df_train, df_test = orig_train_df[ : splits[0]], orig_train_df[splits[0] : ]\n",
        "y_test = np.array(df_test[label_cols], dtype = \"float32\")\n",
        "\n",
        "print(f\"Length of train data : {len(df_train)}\")\n",
        "print(f\"Length of test data : {len(df_test)}\")"
      ],
      "metadata": {
        "id": "Q9c7XpOGZi46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Clustering**"
      ],
      "metadata": {
        "id": "Tuz4PZrbh1vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_rating = copy.deepcopy(df_train[label_cols])\n",
        "rating_values_array = np.array(df_rating[label_cols])\n",
        "\n",
        "# standardize\n",
        "sc = StandardScaler()\n",
        "rating_values_array_std = sc.fit(rating_values_array).transform(rating_values_array)"
      ],
      "metadata": {
        "id": "YTMJpfp8YzNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km = KMeans(random_state=42)\n",
        "visualizer = KElbowVisualizer(km, k=(2,10))\n",
        " \n",
        "visualizer.fit(rating_values_array_std)        # Fit the data to the visualizer\n",
        "visualizer.show()        # Finalize and render the figure"
      ],
      "metadata": {
        "id": "gHFkIEq5Cgul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is how the Elbow / SSE Plot would look like. As per the plot given below, for n_clusters = 3 that represents the elbow you start seeing diminishing returns by increasing k. The line starts looking linear."
      ],
      "metadata": {
        "id": "uLpAtecfC72h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 3, figsize=(20,5))\n",
        "for idx, i in enumerate([2, 3, 4]):\n",
        "    '''\n",
        "    Create KMeans instance for different number of clusters\n",
        "    '''\n",
        "    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)\n",
        "    #q, mod = divmod(i, 2)\n",
        "    '''\n",
        "    Create SilhouetteVisualizer instance with KMeans instance\n",
        "    Fit the visualizer\n",
        "    '''\n",
        "    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[idx])\n",
        "    visualizer.fit(rating_values_array_std) "
      ],
      "metadata": {
        "id": "u2cZ4AyoDOoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given above, the Silhouette plot for n_clusters = 3 looks to be most appropriate than others as it stands well against all the three measuring criteria (scores below average Silhouette score, Wide fluctuations in the size of the plot, and non-uniform thickness)."
      ],
      "metadata": {
        "id": "B-e9bqdIGWnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "km_base = KMeans(n_clusters=3,\n",
        "           #init='random',\n",
        "           init='k-means++',\n",
        "           n_init=10,\n",
        "           max_iter=300,\n",
        "           tol=1e-04,\n",
        "           random_state=1234)\n",
        "\n",
        "# predict k-means classes\n",
        "y_km_base = km_base.fit_predict(rating_values_array_std)\n",
        "\n",
        "# Assigning cluster value to the datafarme\n",
        "df_train['cluster_id'] = y_km_base"
      ],
      "metadata": {
        "id": "Spyt5ysCGSEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "i8Jlal-4bqri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that an increase in *k* is associated with a decrease in the within-cluster SSE. \n",
        "\n",
        "This is because the examples are closer to the centroid they assigned to.\n",
        "\n",
        "**The elbow solution**: the optimal *k* is where the within-cluster SSE begings to increase most rapidly.\n",
        "\n",
        "For this particular example the elbow is at *k=2* so we started with a good number of clusters.\n"
      ],
      "metadata": {
        "id": "kgY72Q0Qj5rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_cluster0 = df_train[df_train.cluster_id == 0]\n",
        "df_train_cluster1 = df_train[df_train.cluster_id == 1]\n",
        "df_train_cluster2 = df_train[df_train.cluster_id == 2]\n",
        "\n",
        "print(f\"Length of cluster 0 : {len(df_train_cluster0)}\")\n",
        "print(f\"Length of cluster 1 : {len(df_train_cluster1)}\")\n",
        "print(f\"Length of cluster 2 : {len(df_train_cluster2)}\")"
      ],
      "metadata": {
        "id": "O2RZdPo215xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Min and max score in cluster 0 are : {np.min(df_train_cluster0['score_sum'])} and {np.max(df_train_cluster0['score_sum'])}\")\n",
        "print(f\"Min and Max score in cluster 1 are : {np.min(df_train_cluster1['score_sum'])} and {np.max(df_train_cluster1['score_sum'])}\")\n",
        "print(f\"Min and Max score in cluster 2 are : {np.min(df_train_cluster2['score_sum'])} and {np.max(df_train_cluster2['score_sum'])}\")"
      ],
      "metadata": {
        "id": "t1Hd6zkQeqvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cluster is divided based on the distribution of the data. Low-scores are in one bucket, medium scores are placed in another and top scrores are placed in the higher bucket."
      ],
      "metadata": {
        "id": "61PU78V5HbuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model parameter setup**"
      ],
      "metadata": {
        "id": "1kbjNbnOZGJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_config_param()\n",
        "\n",
        "# Fixed parameters\n",
        "dense_dim = 6\n",
        "number_of_splits = 5\n",
        "random_state = 2023\n",
        "MAX_LEN = 512\n",
        "mse_loss = tf.keras.losses.mse\n",
        "mse_metrics = tf.keras.metrics.mse\n",
        "\n",
        "# Variable parameters\n",
        "epochs = 5\n",
        "batch_size = 4\n",
        "learning_rate = 1e-5\n",
        "validation_split = .2\n",
        "dropout = .1\n",
        "number_of_hidden_layers = 1\n",
        "hidden_layer_node_count = 64\n",
        "retrain_layer_count = 0\n",
        "\n",
        "# Variable parameter dictionary\n",
        "param_list = [\n",
        "                 # Completely frozen base layer\n",
        "                 {'epochs'                  : 5,\n",
        "                  'batch_size'              : 4,\n",
        "                  'learning_rate'           : 1e-5,\n",
        "                  'validation_split'        : .2,\n",
        "                  'dropout'                 : .1,\n",
        "                  'number_of_hidden_layers' : 1,\n",
        "                  'hidden_layer_node_count' : 64,\n",
        "                  'retrain_layer_count'     : 0\n",
        "                 },\n",
        "                 # Partially frozen base layer\n",
        "                 {'epochs'                  : 5,\n",
        "                  'batch_size'              : 4,\n",
        "                  'learning_rate'           : 1e-5,\n",
        "                  'validation_split'        : .2,\n",
        "                  'dropout'                 : .1,\n",
        "                  'number_of_hidden_layers' : 1,\n",
        "                  'hidden_layer_node_count' : 64,\n",
        "                  'retrain_layer_count'     : 6\n",
        "                 }\n",
        "                 # Completely unfrozen base layer\n",
        "                 {'epochs'                  : 5,\n",
        "                  'batch_size'              : 4,\n",
        "                  'learning_rate'           : 1e-5,\n",
        "                  'validation_split'        : .2,\n",
        "                  'dropout'                 : .1,\n",
        "                  'number_of_hidden_layers' : 1,\n",
        "                  'hidden_layer_node_count' : 64,\n",
        "                  'retrain_layer_count'     : 12\n",
        "                 },\n",
        "             ]"
      ],
      "metadata": {
        "id": "viDJ9yPuZM7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model building**\n",
        "\n",
        "Original test data is very limited, there are only 3 records and it does not have labels to test. So we decided to repurpose the given train data to split into train, test and validation sets.\n",
        "\n",
        "We take the initial input train set and split into train and test set with 8:2 ratio.\n",
        "\n",
        "The train part is then going thru k fold cross validation and get tested on validation set and final test is done on the test set. Final test accuracy will be the average MCRMSE score across k-folds.\n",
        "\n"
      ],
      "metadata": {
        "id": "_tFtDMGBZVA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining the two clusters' data\n",
        "df_train = pd.concat([df_train_cluster0, df_train_cluster1, df_train_cluster2])\n",
        "\n",
        "# shuffling them back again\n",
        "shuffle = np.random.permutation(np.arange(df_train.shape[0]))\n",
        "df_train = df_train.iloc[shuffle]\n",
        "\n",
        "\n",
        "MCRMSE_list = []\n",
        "\n",
        "'''\n",
        "rating_cluster has two values 0 and 1.\n",
        "We are doing k fold with stratification using rating_cluster.\n",
        "We introduced this new column to split on as as our data ouput is multi class\n",
        "and multi label with continuous values and traditional k fold split does not\n",
        "support that.\n",
        "This new column will help us to see if our model is performing better for which \n",
        "group : above or below average.\n",
        "'''\n",
        "for idx, param_entry in enumerate(param_list):\n",
        "    \n",
        "    epoch_val = param_entry['epochs']\n",
        "    batch_size_val = param_entry['batch_size']\n",
        "    learning_rate_val = param_entry['learning_rate']\n",
        "    validation_split_val = param_entry['validation_split']\n",
        "    dropout_val = param_entry['dropout']\n",
        "    number_of_hidden_layers_val = param_entry['number_of_hidden_layers']\n",
        "    hidden_layer_node_count_val = param_entry['hidden_layer_node_count']\n",
        "    retrain_layer_count_val = param_entry['retrain_layer_count']\n",
        "\n",
        "    for kfold, (train_indices, val_indices) in enumerate(StratifiedKFold(n_splits     = number_of_splits, \n",
        "                                                                         shuffle      = True, \n",
        "                                                                         random_state = random_state\n",
        "                                                                         ).split(df_train['cluster_id'].values.tolist(), \n",
        "                                                                                 df_train['cluster_id'].values.tolist()\n",
        "                                                                                )\n",
        "                                                         ):\n",
        "        print(\"************************\")\n",
        "        print(f\"Iteration : {idx + 1}\")\n",
        "        print(\"Parameters...\")\n",
        "        print(f\"Epochs : {epoch_val}\")\n",
        "        print(f\"Batch size : {batch_size_val}\")\n",
        "        print(f\"Learning rate : {learning_rate_val}\")\n",
        "        print(f\"Validation split : {validation_split_val}\")\n",
        "        print(f\"Dropout : {dropout_val}\")\n",
        "        print(f\"Number of hidden layers : {number_of_hidden_layers_val}\")\n",
        "        print(f\"Hidden layer node count : {hidden_layer_node_count_val}\")\n",
        "        print(f\"Retrain layer count : {retrain_layer_count_val}\")\n",
        "        print(\"************************\")\n",
        "        print(f\"k-fold : {kfold + 1}\")\n",
        "        print(f\"length of train data : {len(train_indices)}\")\n",
        "        print(f\"length of validation data : {len(val_indices)}\")\n",
        "        print(\"************************\")\n",
        "        \n",
        "        # Model building\n",
        "        print(\"Building model...\")\n",
        "        regression_model = build_regression_model(loss                    = 'MCRMSE',\n",
        "                                                  model_name              = 'Roberta', \n",
        "                                                  dense_dim               = dense_dim, \n",
        "                                                  MAX_LEN                 = MAX_LEN,\n",
        "                                                  learning_rate           = learning_rate_val,\n",
        "                                                  dropout                 = dropout_val,\n",
        "                                                  number_of_hidden_layers = number_of_hidden_layers_val,\n",
        "                                                  hidden_layer_node_count = hidden_layer_node_count_val,\n",
        "                                                  retrain_layer_count     = retrain_layer_count_val\n",
        "                                                 )\n",
        "        \n",
        "        # Model fitting\n",
        "        print(\"Fitting model...\")\n",
        "        df_history = model_fit(regression_model, \n",
        "                               df_train, \n",
        "                               train_indices,\n",
        "                               val_indices,\n",
        "                               model_name = 'Roberta', \n",
        "                               MAX_LEN = MAX_LEN,\n",
        "                               epochs = epoch_val,\n",
        "                               batch_size = batch_size_val,\n",
        "                               validation_split = validation_split_val\n",
        "                              )\n",
        "        print(df_history.T)\n",
        "\n",
        "        print(\"Plotting loss and MCRMSE...\")\n",
        "        custom_plot(df_history, 'MCRMSE', 'MCRMSE')\n",
        "\n",
        "        # Prep for model evaluation with test data\n",
        "        print(\"Evaluating mode...\")\n",
        "        roberta_tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_MODEL)\n",
        "        test_encoded_input_ids, test_encoded_attention_masks = text_encode(df_test['full_text'], \n",
        "                                                                           roberta_tokenizer, \n",
        "                                                                           MAX_LEN\n",
        "                                                                          )\n",
        "        # Model evaluation\n",
        "        test_loss, test_accuracy = evaluate_model(regression_model, \n",
        "                                                  y_test, \n",
        "                                                  test_encoded_input_ids, \n",
        "                                                  test_encoded_attention_masks\n",
        "                                                 )\n",
        "\n",
        "        # Model prediction\n",
        "        print(\"Model prediction...\")\n",
        "        df_prediction, df_comparison = predict_model(regression_model, \n",
        "                                                     df_test, \n",
        "                                                     test_encoded_input_ids, \n",
        "                                                     test_encoded_attention_masks, \n",
        "                                                     label_cols)\n",
        "\n",
        "        print(\"Plotting model structure...\")\n",
        "        keras.utils.plot_model(regression_model, \n",
        "                               show_shapes      = False, \n",
        "                               show_dtype       = False, \n",
        "                               show_layer_names = True, \n",
        "                               dpi              = 90\n",
        "                              )\n",
        "\n",
        "        print(\"Appending to kpi list...\")\n",
        "        MCRMSE_list.append({'model_name'                  : 'roberta-base',\n",
        "                            'iteration'                   : idx + 1,\n",
        "                            'epoch_val'                   : epoch_val,\n",
        "                            'batch_size_val'              : batch_size_val,\n",
        "                            'learning_rate_val'           : learning_rate_val,\n",
        "                            'validation_split_val'        : validation_split_val,\n",
        "                            'dropout_val'                 : dropout_val,\n",
        "                            'number_of_hidden_layers_val' : number_of_hidden_layers_val,\n",
        "                            'hidden_layer_node_count_val' : hidden_layer_node_count_val,\n",
        "                            'retrain_layer_count_val'     : retrain_layer_count_val,\n",
        "                            'fold'                        : kfold + 1, \n",
        "                            'train_loss'                  : df_history.iloc[-1][0],\n",
        "                            'train_accuracy'              : df_history.iloc[-1][1],\n",
        "                            'val_loss'                    : df_history.iloc[-1][2],\n",
        "                            'val_accuracy'                : df_history.iloc[-1][2],\n",
        "                            'test_loss'                   : test_loss,\n",
        "                            'test_accuracy'               : test_accuracy\n",
        "                           }\n",
        "                          )\n",
        "        # Saving the model\n",
        "        print(\"Saving the model...\")\n",
        "        model_file_name = 'regression_model_iter_' + str(idx + 1) + '_kfold_' + str(kfold + 1) + \".h5\"\n",
        "        regression_model.save(model_file_name)"
      ],
      "metadata": {
        "id": "sHeyCiPxENqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "kpi_col_list = ['model_name',\n",
        "                'iteration',\n",
        "                'epoch_val',\n",
        "                'batch_size_val',\n",
        "                'learning_rate_val',\n",
        "                'validation_split_val',\n",
        "                'dropout_val',\n",
        "                'number_of_hidden_layers_val',\n",
        "                'hidden_layer_node_count_val',\n",
        "                'retrain_layer_count_val',\n",
        "                'fold', \n",
        "                'train_loss', \n",
        "                'train_accuracy', \n",
        "                'val_loss', \n",
        "                'val_accuracy', \n",
        "                'test_loss', \n",
        "                'test_accuracy'\n",
        "               ]\n",
        "df_MCRMSE = pd.DataFrame(MCRMSE_list, columns = kpi_col_list)    \n",
        "df_MCRMSE.to_csv(\"kpi_stats.csv\", index = False)\n",
        "df_MCRMSE    "
      ],
      "metadata": {
        "id": "pk6ZkXlSjt5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Average test accuracy and loss...\")\n",
        "df_MCRMSE.groupby(['model_name', 'iteration']).agg({'test_loss'      : [np.mean, np.min, np.max],  \n",
        "                                                    'test_accuracy'  : [np.mean, np.min, np.max] \n",
        "                                                   }\n",
        "                                                  )"
      ],
      "metadata": {
        "id": "zDwRZh-7ENeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pwOvytm2kqvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QKXM8qrCkqyI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}